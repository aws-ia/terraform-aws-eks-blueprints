{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Amazon EKS Blueprints for Terraform \u00b6 Welcome to Amazon EKS Blueprints for Terraform! This repository contains a collection of Terraform modules that aim to make it easier and faster for customers to adopt Amazon EKS . What is EKS Blueprints \u00b6 EKS Blueprints helps you compose complete EKS clusters that are fully bootstrapped with the operational software that is needed to deploy and operate workloads. With EKS Blueprints, you describe the configuration for the desired state of your EKS environment, such as the control plane, worker nodes, and Kubernetes add-ons, as an IaC blueprint. Once a blueprint is configured, you can use it to stamp out consistent environments across multiple AWS accounts and Regions using continuous deployment automation. You can use EKS Blueprints to easily bootstrap an EKS cluster with Amazon EKS add-ons as well as a wide range of popular open-source add-ons, including Prometheus, Karpenter, Nginx, Traefik, AWS Load Balancer Controller, Fluent Bit, Keda, ArgoCD, and more. EKS Blueprints also helps you implement relevant security controls needed to operate workloads from multiple teams in the same cluster. Examples \u00b6 To view a library of examples for how you can leverage terraform-aws-eks-blueprints , please see our examples . Workshop \u00b6 We maintain a hands-on self-paced workshop, the EKS Blueprints for Terraform workshop helps you with foundational setup of your EKS cluster, and it gradually adds complexity via existing and new modules. Motivation \u00b6 Kubernetes is a powerful and extensible container orchestration technology that allows you to deploy and manage containerized applications at scale. The extensible nature of Kubernetes also allows you to use a wide range of popular open-source tools, commonly referred to as add-ons, in Kubernetes clusters. With such a large number of tooling and design choices available however, building a tailored EKS cluster that meets your application\u2019s specific needs can take a significant amount of time. It involves integrating a wide range of open-source tools and AWS services and requires deep expertise in AWS and Kubernetes. AWS customers have asked for examples that demonstrate how to integrate the landscape of Kubernetes tools and make it easy for them to provision complete, batteries-included EKS clusters that meet specific application requirements. EKS Blueprints was built to address this customer need. You can use EKS Blueprints to configure and deploy purpose built EKS clusters, and start onboarding workloads in days, rather than months. What can I do with this Solution? \u00b6 Customers can use this solution to easily architect and deploy complete, opinionated EKS clusters. Specifically, customers can leverage the eks-blueprints module to: Deploy Well-Architected EKS clusters across any number of accounts and regions. Manage cluster configuration, including add-ons that run in each cluster, from a single Git repository. Define teams, namespaces, and their associated access permissions for your clusters. Leverage GitOps-based workflows for onboarding and managing workloads for your teams. Create Continuous Delivery (CD) pipelines that are responsible for deploying your infrastructure.","title":"Overview"},{"location":"#amazon-eks-blueprints-for-terraform","text":"Welcome to Amazon EKS Blueprints for Terraform! This repository contains a collection of Terraform modules that aim to make it easier and faster for customers to adopt Amazon EKS .","title":"Amazon EKS Blueprints for Terraform"},{"location":"#what-is-eks-blueprints","text":"EKS Blueprints helps you compose complete EKS clusters that are fully bootstrapped with the operational software that is needed to deploy and operate workloads. With EKS Blueprints, you describe the configuration for the desired state of your EKS environment, such as the control plane, worker nodes, and Kubernetes add-ons, as an IaC blueprint. Once a blueprint is configured, you can use it to stamp out consistent environments across multiple AWS accounts and Regions using continuous deployment automation. You can use EKS Blueprints to easily bootstrap an EKS cluster with Amazon EKS add-ons as well as a wide range of popular open-source add-ons, including Prometheus, Karpenter, Nginx, Traefik, AWS Load Balancer Controller, Fluent Bit, Keda, ArgoCD, and more. EKS Blueprints also helps you implement relevant security controls needed to operate workloads from multiple teams in the same cluster.","title":"What is EKS Blueprints"},{"location":"#examples","text":"To view a library of examples for how you can leverage terraform-aws-eks-blueprints , please see our examples .","title":"Examples"},{"location":"#workshop","text":"We maintain a hands-on self-paced workshop, the EKS Blueprints for Terraform workshop helps you with foundational setup of your EKS cluster, and it gradually adds complexity via existing and new modules.","title":"Workshop"},{"location":"#motivation","text":"Kubernetes is a powerful and extensible container orchestration technology that allows you to deploy and manage containerized applications at scale. The extensible nature of Kubernetes also allows you to use a wide range of popular open-source tools, commonly referred to as add-ons, in Kubernetes clusters. With such a large number of tooling and design choices available however, building a tailored EKS cluster that meets your application\u2019s specific needs can take a significant amount of time. It involves integrating a wide range of open-source tools and AWS services and requires deep expertise in AWS and Kubernetes. AWS customers have asked for examples that demonstrate how to integrate the landscape of Kubernetes tools and make it easy for them to provision complete, batteries-included EKS clusters that meet specific application requirements. EKS Blueprints was built to address this customer need. You can use EKS Blueprints to configure and deploy purpose built EKS clusters, and start onboarding workloads in days, rather than months.","title":"Motivation"},{"location":"#what-can-i-do-with-this-solution","text":"Customers can use this solution to easily architect and deploy complete, opinionated EKS clusters. Specifically, customers can leverage the eks-blueprints module to: Deploy Well-Architected EKS clusters across any number of accounts and regions. Manage cluster configuration, including add-ons that run in each cluster, from a single Git repository. Define teams, namespaces, and their associated access permissions for your clusters. Leverage GitOps-based workflows for onboarding and managing workloads for your teams. Create Continuous Delivery (CD) pipelines that are responsible for deploying your infrastructure.","title":"What can I do with this Solution?"},{"location":"core-concepts/","text":"Core Concepts \u00b6 This document provides a high level overview of the Core Concepts that are embedded in EKS Blueprints. For the purposes of this document, we will assume the reader is familiar with Git, Docker, Kubernetes and AWS. Concept Description Cluster An Amazon EKS Cluster and associated worker groups. Add-on Operational software that provides key functionality to support your Kubernetes applications. Team A logical grouping of IAM identities that have access to Kubernetes resources. Pipeline Continuous Delivery pipelines for deploying clusters and add-ons . Application An application that runs within an EKS Cluster. Cluster \u00b6 A cluster is simply an EKS cluster. EKS Blueprints provides for customizing the compute options you leverage with your clusters . The framework currently supports EC2 , Fargate and BottleRocket instances. It also supports managed and self-managed node groups. To specify the type of compute you want to use for your cluster , you use the managed_node_groups , self_managed_nodegroups , or fargate_profiles variables. See our Node Groups documentation and our Node Group example directory for detailed information. Add-on \u00b6 Add-ons allow you to configure the operational tools that you would like to deploy into your EKS cluster. When you configure add-ons for a cluster , the add-ons will be provisioned at deploy time by leveraging the Terraform Helm provider. Add-ons can deploy both Kubernetes specific resources and AWS resources needed to support add-on functionality. For example, the metrics-server add-on only deploys the Kubernetes manifests that are needed to run the Kubernetes Metrics Server. By contrast, the aws-load-balancer-controller add-on deploys both Kubernetes YAML, in addition to creating resources via AWS APIs that are needed to support the AWS Load Balancer Controller functionality. EKS Blueprints allows you to manage your add-ons directly via Terraform (by leveraging the Terraform Helm provider) or via GitOps with ArgoCD. See our Add-ons documentation page for detailed information. Team \u00b6 Teams allow you to configure the logical grouping of users that have access to your EKS clusters, in addition to the access permissions they are granted. EKS Blueprints currently supports two types of teams : application-team and platform-team . application-team members are granted access to specific namespaces. platform-team members are granted administrative access to your clusters. See our Teams documentation page for detailed information. Application \u00b6 Applications represent the actual workloads that run within a Kubernetes cluster. The framework leverages a GitOps approach for deploying applications onto clusters. See our Applications documentation for detailed information.","title":"Core Concepts"},{"location":"core-concepts/#core-concepts","text":"This document provides a high level overview of the Core Concepts that are embedded in EKS Blueprints. For the purposes of this document, we will assume the reader is familiar with Git, Docker, Kubernetes and AWS. Concept Description Cluster An Amazon EKS Cluster and associated worker groups. Add-on Operational software that provides key functionality to support your Kubernetes applications. Team A logical grouping of IAM identities that have access to Kubernetes resources. Pipeline Continuous Delivery pipelines for deploying clusters and add-ons . Application An application that runs within an EKS Cluster.","title":"Core Concepts"},{"location":"core-concepts/#cluster","text":"A cluster is simply an EKS cluster. EKS Blueprints provides for customizing the compute options you leverage with your clusters . The framework currently supports EC2 , Fargate and BottleRocket instances. It also supports managed and self-managed node groups. To specify the type of compute you want to use for your cluster , you use the managed_node_groups , self_managed_nodegroups , or fargate_profiles variables. See our Node Groups documentation and our Node Group example directory for detailed information.","title":"Cluster"},{"location":"core-concepts/#add-on","text":"Add-ons allow you to configure the operational tools that you would like to deploy into your EKS cluster. When you configure add-ons for a cluster , the add-ons will be provisioned at deploy time by leveraging the Terraform Helm provider. Add-ons can deploy both Kubernetes specific resources and AWS resources needed to support add-on functionality. For example, the metrics-server add-on only deploys the Kubernetes manifests that are needed to run the Kubernetes Metrics Server. By contrast, the aws-load-balancer-controller add-on deploys both Kubernetes YAML, in addition to creating resources via AWS APIs that are needed to support the AWS Load Balancer Controller functionality. EKS Blueprints allows you to manage your add-ons directly via Terraform (by leveraging the Terraform Helm provider) or via GitOps with ArgoCD. See our Add-ons documentation page for detailed information.","title":"Add-on"},{"location":"core-concepts/#team","text":"Teams allow you to configure the logical grouping of users that have access to your EKS clusters, in addition to the access permissions they are granted. EKS Blueprints currently supports two types of teams : application-team and platform-team . application-team members are granted access to specific namespaces. platform-team members are granted administrative access to your clusters. See our Teams documentation page for detailed information.","title":"Team"},{"location":"core-concepts/#application","text":"Applications represent the actual workloads that run within a Kubernetes cluster. The framework leverages a GitOps approach for deploying applications onto clusters. See our Applications documentation for detailed information.","title":"Application"},{"location":"extensibility/","text":"Extensibility \u00b6 This guide provides an overview of extensibility options focusing on add-on extensions as the primary mechanism for the partners and customers. Overview \u00b6 EKS Blueprints framework is designed to be extensible. In the context of this guide, extensibility refers to the ability of customers and partners to both add new capabilities to the framework or platforms as well as customize existing behavior, including the ability to modify or override existing behavior. As of this writing, the primary means by which customers and partners can extend the EKS Blueprints for Terraform framework is by implementing new add-ons which could be leveraged exactly the same way as the core add-ons (supplied by the framework). Add-on Extensions \u00b6 Helm Add-ons \u00b6 Helm add-ons are the most common case that generally combines provisioning of a helm chart as well as supporting infrastructure such as wiring of proper IAM policies for the Kubernetes service account, provisioning or configuring other AWS resources (VPC, subnets, node groups). In order to simplify the add-on creation, we have provided a helper module called helm-addon for convenience. Non-helm Add-ons \u00b6 Add-ons that don't leverage helm but require to install arbitrary Kubernetes manifests will not be able to leverage the benefits provided by the helm-addon however, they are still relatively easy to implement and would follow a similar pattern. Such addons should leverage the kubectl provider . Public Add-ons \u00b6 The life-cycle of a public add-on should be decoupled from the life-cycle of the core framework repository. When decoupled, extensions can be released at any arbitrary cadence specific to the extension, enabling better agility when it comes to new features or bug fixes. The owner of such public add-on is ultimately responsible for the quality and maintenance of the add-on. In order to enable this model the following workflow outline steps required to create and release a public add-on: Public add-on are created in a separate repository. Public GitHub repository is preferred as it aligns with the open-source spirit of the framework and enables external reviews/feedback. Add-ons are released and consumed as distinct public Terraform modules. Public add-ons are expected to have sufficient documentation to allow customers to consume them independently. Documentation can reside in GitHub or external resources referenced in the documentation bundled with the extension. Public add-ons are expected to be tested and validated against released EKS Blueprints versions, e.g. with a CI/CD pipeline or GitHub Actions. Partner Add-ons \u00b6 Partner extensions (APN Partner) are expected to comply with the public extension workflow and additional items required to ensure proper validation and documentation support for a partner extension. We expect 2 PRs to be created for every Partner Add-On. A PR against the main EKS Blueprints repository that contains the following: Update kubernetes-addons/main.tf to add a module invocation of the remote terraform module for the add-on. Documentation to update the Add-Ons section. Example of add-on documentation can be found here along with the list of other add-ons. A second PR against the EKS Blueprints Add-Ons repository to create an ArgoCD application for your add-on. See example of other add-ons that shows what should be added. Add-ons that do not provide GitOps support are not expected to create this PR. Private Add-ons \u00b6 There are two ways in which a customer can implement fully private add-ons: Add-ons specific to a customer instance of EKS Blueprints can be implemented inline with the blueprint in the same codebase. Such extensions are scoped to the customer base. Forking the repo however has disadvantages when it comes to ongoing feature releases and bug fixes which will have to be manually ported to your fork. We recommend, you implement a separate repository for your private add-on while still using the upstream framework. This gives you the advantage of keeping up with ongoing feature releases and bug fixes while keeping your add-on private. The following example shows you can leverage EKS Blueprints to provide your own helm add-on. #--------------------------------------------------------------- # AWS VPC CNI Metrics Helper # This is using local helm chart #--------------------------------------------------------------- data \"aws_partition\" \"current\" {} data \"aws_caller_identity\" \"current\" {} locals { cni_metrics_name = \"cni-metrics-helper\" default_helm_values = [templatefile(\"${path.module}/helm-values/cni-metrics-helper-values.yaml\", { eks_cluster_id = var.eks_cluster_id, image = \"602401143452.dkr.ecr.${var.region}.amazonaws.com/cni-metrics-helper:v1.10.3\", sa-name = local.cni_metrics_name oidc_url = \"oidc.eks.eu-west-1.amazonaws.com/id/E6CASOMETHING55B9D01F7\" })] addon_context = { aws_caller_identity_account_id = data.aws_caller_identity.current.account_id aws_caller_identity_arn = data.aws_caller_identity.current.arn aws_eks_cluster_endpoint = data.aws_eks_cluster.cluster.endpoint aws_partition_id = data.aws_partition.current.partition aws_region_name = var.region eks_cluster_id = var.eks_cluster_id eks_oidc_issuer_url = local.oidc_url eks_oidc_provider_arn = \"arn:${data.aws_partition.current.partition}:iam::${data.aws_caller_identity.current.account_id}:oidc-provider/${local.oidc_url}\" tags = {} } helm_config = { name = local.cni_metrics_name description = \"CNI Metrics Helper Helm Chart\" timeout = \"300\" chart = \"${path.module}/local-helm-charts/cni-metrics-helper\" version = \"0.1.7\" repository = null namespace = \"kube-system\" lint = false values = local.default_helm_values } irsa_config = { kubernetes_namespace = \"kube-system\" kubernetes_service_account = local.cni_metrics_name create_kubernetes_namespace = false create_kubernetes_service_account = true irsa_iam_policies = [aws_iam_policy.cni_metrics.arn] } } module \"helm_addon\" { source = \"github.com/aws-ia/terraform-aws-eks-blueprints//modules/kubernetes-addons/helm-addon\" helm_config = local.helm_config irsa_config = local.irsa_config addon_context = local.addon_context } resource \"aws_iam_policy\" \"cni_metrics\" { name = \"${var.eks_cluster_id}-cni-metrics\" description = \"IAM policy for EKS CNI Metrics helper\" path = \"/\" policy = data.aws_iam_policy_document.cni_metrics.json tags = var.tags } data \"aws_iam_policy_document\" \"cni_metrics\" { statement { sid = \"CNIMetrics\" actions = [ \"cloudwatch:PutMetricData\" ] resources = [\"*\"] } } Secrets Handling \u00b6 We expect that certain add-ons will need to provide access to sensitive values to their helm chart configuration such as password, license keys, API keys, etc. We recommend that you ask customers to store such secrets in an external secret store such as AWS Secrets Manager or AWS Systems Manager Parameter Store and use the AWS Secrets and Configuration Provider (ASCP) to mount the secrets as files or environment variables in the pods of your add-on. We are actively working on providing a native add-on for ASCP as of this writing which you will be able to levarage for your add-on. Example Public Add-On \u00b6 Kube-state-metrics-addon extension contains a sample implementation of the kube-state-metrics that demonstrates how to write a public add-on that lives outside of the core repo. Add-On Repo \u00b6 We recommend the use of pattern terraform-eksblueprints-<addon_name> as the name of the repo so that you are able to easily publish the module to Terraform registry . See kube-state-metrics for an example. Add-On Code \u00b6 We recommend your add-on code follow Terraform standards for best practices for organizing your code, such as.. . \u251c\u2500\u2500 CODE_OF_CONDUCT.md \u251c\u2500\u2500 CONTRIBUTING.md \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 blueprints \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 addons \u2502 \u2502 \u251c\u2500\u2500 README.md \u2502 \u2502 \u251c\u2500\u2500 addons.tfbackend \u2502 \u2502 \u251c\u2500\u2500 backend.tf \u2502 \u2502 \u251c\u2500\u2500 data.tf \u2502 \u2502 \u251c\u2500\u2500 main.tf \u2502 \u2502 \u251c\u2500\u2500 providers.tf \u2502 \u2502 \u2514\u2500\u2500 variables.tf \u2502 \u251c\u2500\u2500 eks \u2502 \u2502 \u251c\u2500\u2500 README.md \u2502 \u2502 \u251c\u2500\u2500 backend.tf \u2502 \u2502 \u251c\u2500\u2500 data.tf \u2502 \u2502 \u251c\u2500\u2500 eks.tfbackend \u2502 \u2502 \u251c\u2500\u2500 main.tf \u2502 \u2502 \u251c\u2500\u2500 outputs.tf \u2502 \u2502 \u251c\u2500\u2500 providers.tf \u2502 \u2502 \u2514\u2500\u2500 variables.tf \u2502 \u251c\u2500\u2500 vars \u2502 \u2502 \u2514\u2500\u2500 config.tfvars \u2502 \u2514\u2500\u2500 vpc \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 backend.tf \u2502 \u251c\u2500\u2500 data.tf \u2502 \u251c\u2500\u2500 locals.tf \u2502 \u251c\u2500\u2500 main.tf \u2502 \u251c\u2500\u2500 outputs.tf \u2502 \u251c\u2500\u2500 providers.tf \u2502 \u251c\u2500\u2500 variables.tf \u2502 \u2514\u2500\u2500 vpc.tfbackend \u251c\u2500\u2500 locals.tf \u251c\u2500\u2500 main.tf \u251c\u2500\u2500 outputs.tf \u251c\u2500\u2500 values.yaml \u2514\u2500\u2500 variables.tf In the above code tree, The root directory contains your add-on code. The blueprints code contains the code that demonstrates how customers can use your add-on with the EKS Blueprints framework. Here, we highly recommend that you show the true value add of your add-on through the pattern. Customers will benefit the most where the example shows how they can integrate their workload with your add-on. If your add-on can be deployed via helm chart, we recommend the use of the helm-addon as shown below. Note : Use the latest published module in the source version. main.tf module \"helm_addon\" { source = \"github.com/aws-ia/terraform-aws-eks-blueprints//modules/kubernetes-addons/helm-addon?ref=v3.5.0\" manage_via_gitops = var.manage_via_gitops ### The following values are defined in locals.tf set_values = local.set_values set_sensitive_values = local.set_sensitive_values helm_config = local.helm_config addon_context = var.addon_context } Core Repo Changes \u00b6 Once you have tested your add-on locally against your fork of the core repo, please open a PR that contains the following: Update to kubernetes-addons/main.tf with a code block that invokes your add-on. E.g. module \"kube_state_metrics\" { count = var.enable_kube_state_metrics ? 1 : 0 source = \"askulkarni2/kube-state-metrics-addon/eksblueprints\" version = \"0.0.2\" helm_config = var.kube_state_metrics_helm_config addon_context = local.addon_context manage_via_gitops = var.argocd_manage_add_ons } Update to kubernetes-addons/variables.tf to accept parameters for your add-on. E.g. #-----------Kube State Metrics ADDON------------- variable \"enable_kube_state_metrics\" { type = bool default = false description = \"Enable Kube State Metrics add-on\" } variable \"kube_state_metrics_helm_config\" { type = any default = {} description = \"Kube State Metrics Helm Chart config\" } Add documentation under add-on docs that gives an overview of your add-on and points the customer to the actual documentation which would live in your add-on repo. GitOps \u00b6 If your add-on can be managed via ArgoCD GitOps, then Provide the argo_gitops_config as an output of your add-on module as shown here . outputs.tf output \"argocd_gitops_config\" { description = \"Configuration used for managing the add-on with ArgoCD\" value = var.manage_via_gitops ? local.argocd_gitops_config : null } In the PR against the core repo, update kubernetes-addons/locals.tf to provide the add-on module output argocd_gitops_config to the argocd_add_on_config as shown for others. Open a PR against the eks-blueprints-addons repo with the following changes: Create a wrapper Helm chart for your add-on similar to kube-state-metrics Create a Chart.yaml which points to the location of your actual helm chart. Create a values.yaml which contains a default best-practice configuration for your add-on. Create an ArgoCD application template which is applied if enable_<add_on> = true is used by the customer in the consumer module. This also used to parameterize your add-ons helm chart wrapper with values that will be passed over from Terraform to Helm using the GitOps bridge .","title":"Extensibility"},{"location":"extensibility/#extensibility","text":"This guide provides an overview of extensibility options focusing on add-on extensions as the primary mechanism for the partners and customers.","title":"Extensibility"},{"location":"extensibility/#overview","text":"EKS Blueprints framework is designed to be extensible. In the context of this guide, extensibility refers to the ability of customers and partners to both add new capabilities to the framework or platforms as well as customize existing behavior, including the ability to modify or override existing behavior. As of this writing, the primary means by which customers and partners can extend the EKS Blueprints for Terraform framework is by implementing new add-ons which could be leveraged exactly the same way as the core add-ons (supplied by the framework).","title":"Overview"},{"location":"extensibility/#add-on-extensions","text":"","title":"Add-on Extensions"},{"location":"extensibility/#helm-add-ons","text":"Helm add-ons are the most common case that generally combines provisioning of a helm chart as well as supporting infrastructure such as wiring of proper IAM policies for the Kubernetes service account, provisioning or configuring other AWS resources (VPC, subnets, node groups). In order to simplify the add-on creation, we have provided a helper module called helm-addon for convenience.","title":"Helm Add-ons"},{"location":"extensibility/#non-helm-add-ons","text":"Add-ons that don't leverage helm but require to install arbitrary Kubernetes manifests will not be able to leverage the benefits provided by the helm-addon however, they are still relatively easy to implement and would follow a similar pattern. Such addons should leverage the kubectl provider .","title":"Non-helm Add-ons"},{"location":"extensibility/#public-add-ons","text":"The life-cycle of a public add-on should be decoupled from the life-cycle of the core framework repository. When decoupled, extensions can be released at any arbitrary cadence specific to the extension, enabling better agility when it comes to new features or bug fixes. The owner of such public add-on is ultimately responsible for the quality and maintenance of the add-on. In order to enable this model the following workflow outline steps required to create and release a public add-on: Public add-on are created in a separate repository. Public GitHub repository is preferred as it aligns with the open-source spirit of the framework and enables external reviews/feedback. Add-ons are released and consumed as distinct public Terraform modules. Public add-ons are expected to have sufficient documentation to allow customers to consume them independently. Documentation can reside in GitHub or external resources referenced in the documentation bundled with the extension. Public add-ons are expected to be tested and validated against released EKS Blueprints versions, e.g. with a CI/CD pipeline or GitHub Actions.","title":"Public Add-ons"},{"location":"extensibility/#partner-add-ons","text":"Partner extensions (APN Partner) are expected to comply with the public extension workflow and additional items required to ensure proper validation and documentation support for a partner extension. We expect 2 PRs to be created for every Partner Add-On. A PR against the main EKS Blueprints repository that contains the following: Update kubernetes-addons/main.tf to add a module invocation of the remote terraform module for the add-on. Documentation to update the Add-Ons section. Example of add-on documentation can be found here along with the list of other add-ons. A second PR against the EKS Blueprints Add-Ons repository to create an ArgoCD application for your add-on. See example of other add-ons that shows what should be added. Add-ons that do not provide GitOps support are not expected to create this PR.","title":"Partner Add-ons"},{"location":"extensibility/#private-add-ons","text":"There are two ways in which a customer can implement fully private add-ons: Add-ons specific to a customer instance of EKS Blueprints can be implemented inline with the blueprint in the same codebase. Such extensions are scoped to the customer base. Forking the repo however has disadvantages when it comes to ongoing feature releases and bug fixes which will have to be manually ported to your fork. We recommend, you implement a separate repository for your private add-on while still using the upstream framework. This gives you the advantage of keeping up with ongoing feature releases and bug fixes while keeping your add-on private. The following example shows you can leverage EKS Blueprints to provide your own helm add-on. #--------------------------------------------------------------- # AWS VPC CNI Metrics Helper # This is using local helm chart #--------------------------------------------------------------- data \"aws_partition\" \"current\" {} data \"aws_caller_identity\" \"current\" {} locals { cni_metrics_name = \"cni-metrics-helper\" default_helm_values = [templatefile(\"${path.module}/helm-values/cni-metrics-helper-values.yaml\", { eks_cluster_id = var.eks_cluster_id, image = \"602401143452.dkr.ecr.${var.region}.amazonaws.com/cni-metrics-helper:v1.10.3\", sa-name = local.cni_metrics_name oidc_url = \"oidc.eks.eu-west-1.amazonaws.com/id/E6CASOMETHING55B9D01F7\" })] addon_context = { aws_caller_identity_account_id = data.aws_caller_identity.current.account_id aws_caller_identity_arn = data.aws_caller_identity.current.arn aws_eks_cluster_endpoint = data.aws_eks_cluster.cluster.endpoint aws_partition_id = data.aws_partition.current.partition aws_region_name = var.region eks_cluster_id = var.eks_cluster_id eks_oidc_issuer_url = local.oidc_url eks_oidc_provider_arn = \"arn:${data.aws_partition.current.partition}:iam::${data.aws_caller_identity.current.account_id}:oidc-provider/${local.oidc_url}\" tags = {} } helm_config = { name = local.cni_metrics_name description = \"CNI Metrics Helper Helm Chart\" timeout = \"300\" chart = \"${path.module}/local-helm-charts/cni-metrics-helper\" version = \"0.1.7\" repository = null namespace = \"kube-system\" lint = false values = local.default_helm_values } irsa_config = { kubernetes_namespace = \"kube-system\" kubernetes_service_account = local.cni_metrics_name create_kubernetes_namespace = false create_kubernetes_service_account = true irsa_iam_policies = [aws_iam_policy.cni_metrics.arn] } } module \"helm_addon\" { source = \"github.com/aws-ia/terraform-aws-eks-blueprints//modules/kubernetes-addons/helm-addon\" helm_config = local.helm_config irsa_config = local.irsa_config addon_context = local.addon_context } resource \"aws_iam_policy\" \"cni_metrics\" { name = \"${var.eks_cluster_id}-cni-metrics\" description = \"IAM policy for EKS CNI Metrics helper\" path = \"/\" policy = data.aws_iam_policy_document.cni_metrics.json tags = var.tags } data \"aws_iam_policy_document\" \"cni_metrics\" { statement { sid = \"CNIMetrics\" actions = [ \"cloudwatch:PutMetricData\" ] resources = [\"*\"] } }","title":"Private Add-ons"},{"location":"extensibility/#secrets-handling","text":"We expect that certain add-ons will need to provide access to sensitive values to their helm chart configuration such as password, license keys, API keys, etc. We recommend that you ask customers to store such secrets in an external secret store such as AWS Secrets Manager or AWS Systems Manager Parameter Store and use the AWS Secrets and Configuration Provider (ASCP) to mount the secrets as files or environment variables in the pods of your add-on. We are actively working on providing a native add-on for ASCP as of this writing which you will be able to levarage for your add-on.","title":"Secrets Handling"},{"location":"extensibility/#example-public-add-on","text":"Kube-state-metrics-addon extension contains a sample implementation of the kube-state-metrics that demonstrates how to write a public add-on that lives outside of the core repo.","title":"Example Public Add-On"},{"location":"extensibility/#add-on-repo","text":"We recommend the use of pattern terraform-eksblueprints-<addon_name> as the name of the repo so that you are able to easily publish the module to Terraform registry . See kube-state-metrics for an example.","title":"Add-On Repo"},{"location":"extensibility/#add-on-code","text":"We recommend your add-on code follow Terraform standards for best practices for organizing your code, such as.. . \u251c\u2500\u2500 CODE_OF_CONDUCT.md \u251c\u2500\u2500 CONTRIBUTING.md \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 blueprints \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 addons \u2502 \u2502 \u251c\u2500\u2500 README.md \u2502 \u2502 \u251c\u2500\u2500 addons.tfbackend \u2502 \u2502 \u251c\u2500\u2500 backend.tf \u2502 \u2502 \u251c\u2500\u2500 data.tf \u2502 \u2502 \u251c\u2500\u2500 main.tf \u2502 \u2502 \u251c\u2500\u2500 providers.tf \u2502 \u2502 \u2514\u2500\u2500 variables.tf \u2502 \u251c\u2500\u2500 eks \u2502 \u2502 \u251c\u2500\u2500 README.md \u2502 \u2502 \u251c\u2500\u2500 backend.tf \u2502 \u2502 \u251c\u2500\u2500 data.tf \u2502 \u2502 \u251c\u2500\u2500 eks.tfbackend \u2502 \u2502 \u251c\u2500\u2500 main.tf \u2502 \u2502 \u251c\u2500\u2500 outputs.tf \u2502 \u2502 \u251c\u2500\u2500 providers.tf \u2502 \u2502 \u2514\u2500\u2500 variables.tf \u2502 \u251c\u2500\u2500 vars \u2502 \u2502 \u2514\u2500\u2500 config.tfvars \u2502 \u2514\u2500\u2500 vpc \u2502 \u251c\u2500\u2500 README.md \u2502 \u251c\u2500\u2500 backend.tf \u2502 \u251c\u2500\u2500 data.tf \u2502 \u251c\u2500\u2500 locals.tf \u2502 \u251c\u2500\u2500 main.tf \u2502 \u251c\u2500\u2500 outputs.tf \u2502 \u251c\u2500\u2500 providers.tf \u2502 \u251c\u2500\u2500 variables.tf \u2502 \u2514\u2500\u2500 vpc.tfbackend \u251c\u2500\u2500 locals.tf \u251c\u2500\u2500 main.tf \u251c\u2500\u2500 outputs.tf \u251c\u2500\u2500 values.yaml \u2514\u2500\u2500 variables.tf In the above code tree, The root directory contains your add-on code. The blueprints code contains the code that demonstrates how customers can use your add-on with the EKS Blueprints framework. Here, we highly recommend that you show the true value add of your add-on through the pattern. Customers will benefit the most where the example shows how they can integrate their workload with your add-on. If your add-on can be deployed via helm chart, we recommend the use of the helm-addon as shown below. Note : Use the latest published module in the source version. main.tf module \"helm_addon\" { source = \"github.com/aws-ia/terraform-aws-eks-blueprints//modules/kubernetes-addons/helm-addon?ref=v3.5.0\" manage_via_gitops = var.manage_via_gitops ### The following values are defined in locals.tf set_values = local.set_values set_sensitive_values = local.set_sensitive_values helm_config = local.helm_config addon_context = var.addon_context }","title":"Add-On Code"},{"location":"extensibility/#core-repo-changes","text":"Once you have tested your add-on locally against your fork of the core repo, please open a PR that contains the following: Update to kubernetes-addons/main.tf with a code block that invokes your add-on. E.g. module \"kube_state_metrics\" { count = var.enable_kube_state_metrics ? 1 : 0 source = \"askulkarni2/kube-state-metrics-addon/eksblueprints\" version = \"0.0.2\" helm_config = var.kube_state_metrics_helm_config addon_context = local.addon_context manage_via_gitops = var.argocd_manage_add_ons } Update to kubernetes-addons/variables.tf to accept parameters for your add-on. E.g. #-----------Kube State Metrics ADDON------------- variable \"enable_kube_state_metrics\" { type = bool default = false description = \"Enable Kube State Metrics add-on\" } variable \"kube_state_metrics_helm_config\" { type = any default = {} description = \"Kube State Metrics Helm Chart config\" } Add documentation under add-on docs that gives an overview of your add-on and points the customer to the actual documentation which would live in your add-on repo.","title":"Core Repo Changes"},{"location":"extensibility/#gitops","text":"If your add-on can be managed via ArgoCD GitOps, then Provide the argo_gitops_config as an output of your add-on module as shown here . outputs.tf output \"argocd_gitops_config\" { description = \"Configuration used for managing the add-on with ArgoCD\" value = var.manage_via_gitops ? local.argocd_gitops_config : null } In the PR against the core repo, update kubernetes-addons/locals.tf to provide the add-on module output argocd_gitops_config to the argocd_add_on_config as shown for others. Open a PR against the eks-blueprints-addons repo with the following changes: Create a wrapper Helm chart for your add-on similar to kube-state-metrics Create a Chart.yaml which points to the location of your actual helm chart. Create a values.yaml which contains a default best-practice configuration for your add-on. Create an ArgoCD application template which is applied if enable_<add_on> = true is used by the customer in the consumer module. This also used to parameterize your add-ons helm chart wrapper with values that will be passed over from Terraform to Helm using the GitOps bridge .","title":"GitOps"},{"location":"getting-started/","text":"Getting Started \u00b6 This getting started guide will help you deploy your first EKS environment using EKS Blueprints. Prerequisites: \u00b6 First, ensure that you have installed the following tools locally. aws cli kubectl terraform Deployment Steps \u00b6 The following steps will walk you through the deployment of an example blueprint . This example will deploy a new VPC, a private EKS cluster with public and private subnets, and one managed node group that will be placed in the private subnets. The example will also deploy the following add-ons into the EKS cluster: AWS Load Balancer Controller Cluster Autoscaler CoreDNS kube-proxy Metrics Server vpc-cni Clone the repo \u00b6 git clone https://github.com/aws-ia/terraform-aws-eks-blueprints.git Terraform INIT \u00b6 CD into the example directory: cd examples/eks-cluster-with-new-vpc/ Initialize the working directory with the following: terraform init Terraform PLAN \u00b6 Verify the resources that will be created by this execution: terraform plan Terraform APPLY \u00b6 We will leverage Terraform's target functionality to deploy a VPC, an EKS Cluster, and Kubernetes add-ons in separate steps. Deploy the VPC . This step will take roughly 3 minutes to complete. terraform apply -target=\"module.vpc\" Deploy the EKS cluster . This step will take roughly 14 minutes to complete. terraform apply -target=\"module.eks_blueprints\" Deploy the add-ons . This step will take rough 5 minutes to complete. terraform apply -target=\"module.eks_blueprints_kubernetes_addons\" Configure kubectl \u00b6 Terraform output will display a command in your console that you can use to bootstrap your local kubeconfig . configure_kubectl = \"aws eks --region <region> update-kubeconfig --name <cluster-name>\" Run the command in your terminal. aws eks --region <region> update-kubeconfig --name <cluster-name> Validation \u00b6 List worker nodes \u00b6 kubectl get nodes You should see output similar to the following: NAME STATUS ROLES AGE VERSION ip-10-0-10-161.us-west-2.compute.internal Ready <none> 4h18m v1.21.5-eks-9017834 ip-10-0-11-171.us-west-2.compute.internal Ready <none> 4h18m v1.21.5-eks-9017834 ip-10-0-12-48.us-west-2.compute.internal Ready <none> 4h18m v1.21.5-eks-9017834 List pods \u00b6 kubectl get pods -n kube-system You should see output similar to the following: NAME READY STATUS RESTARTS AGE aws-load-balancer-controller-954746b57-k9lhc 1/1 Running 1 15m aws-load-balancer-controller-954746b57-q5gh4 1/1 Running 1 15m aws-node-jlnkd 1/1 Running 1 15m aws-node-k86pv 1/1 Running 0 12m aws-node-kjcdg 1/1 Running 1 14m cluster-autoscaler-aws-cluster-autoscaler-5d4446b58-d6frd 1/1 Running 1 15m coredns-85d5b4454c-jksbw 1/1 Running 1 24m coredns-85d5b4454c-x7wwd 1/1 Running 1 24m kube-proxy-92slm 1/1 Running 1 18m kube-proxy-bz5kb 1/1 Running 1 18m kube-proxy-zl7cj 1/1 Running 1 18m metrics-server-694d47d564-hzd8h 1/1 Running 1 15m Cleanup \u00b6 To clean up your environment, destroy the Terraform modules in reverse order. Destroy the add-ons. terraform destroy -target=\"module.eks_blueprints_kubernetes_addons\" Destroy the EKS cluster. terraform destroy -target=\"module.eks_blueprints\" Destroy the VPC. terraform destroy -target=\"module.vpc\"","title":"Getting Started"},{"location":"getting-started/#getting-started","text":"This getting started guide will help you deploy your first EKS environment using EKS Blueprints.","title":"Getting Started"},{"location":"getting-started/#prerequisites","text":"First, ensure that you have installed the following tools locally. aws cli kubectl terraform","title":"Prerequisites:"},{"location":"getting-started/#deployment-steps","text":"The following steps will walk you through the deployment of an example blueprint . This example will deploy a new VPC, a private EKS cluster with public and private subnets, and one managed node group that will be placed in the private subnets. The example will also deploy the following add-ons into the EKS cluster: AWS Load Balancer Controller Cluster Autoscaler CoreDNS kube-proxy Metrics Server vpc-cni","title":"Deployment Steps"},{"location":"getting-started/#clone-the-repo","text":"git clone https://github.com/aws-ia/terraform-aws-eks-blueprints.git","title":"Clone the repo"},{"location":"getting-started/#terraform-init","text":"CD into the example directory: cd examples/eks-cluster-with-new-vpc/ Initialize the working directory with the following: terraform init","title":"Terraform INIT"},{"location":"getting-started/#terraform-plan","text":"Verify the resources that will be created by this execution: terraform plan","title":"Terraform PLAN"},{"location":"getting-started/#terraform-apply","text":"We will leverage Terraform's target functionality to deploy a VPC, an EKS Cluster, and Kubernetes add-ons in separate steps. Deploy the VPC . This step will take roughly 3 minutes to complete. terraform apply -target=\"module.vpc\" Deploy the EKS cluster . This step will take roughly 14 minutes to complete. terraform apply -target=\"module.eks_blueprints\" Deploy the add-ons . This step will take rough 5 minutes to complete. terraform apply -target=\"module.eks_blueprints_kubernetes_addons\"","title":"Terraform APPLY"},{"location":"getting-started/#configure-kubectl","text":"Terraform output will display a command in your console that you can use to bootstrap your local kubeconfig . configure_kubectl = \"aws eks --region <region> update-kubeconfig --name <cluster-name>\" Run the command in your terminal. aws eks --region <region> update-kubeconfig --name <cluster-name>","title":"Configure kubectl"},{"location":"getting-started/#validation","text":"","title":"Validation"},{"location":"getting-started/#list-worker-nodes","text":"kubectl get nodes You should see output similar to the following: NAME STATUS ROLES AGE VERSION ip-10-0-10-161.us-west-2.compute.internal Ready <none> 4h18m v1.21.5-eks-9017834 ip-10-0-11-171.us-west-2.compute.internal Ready <none> 4h18m v1.21.5-eks-9017834 ip-10-0-12-48.us-west-2.compute.internal Ready <none> 4h18m v1.21.5-eks-9017834","title":"List worker nodes"},{"location":"getting-started/#list-pods","text":"kubectl get pods -n kube-system You should see output similar to the following: NAME READY STATUS RESTARTS AGE aws-load-balancer-controller-954746b57-k9lhc 1/1 Running 1 15m aws-load-balancer-controller-954746b57-q5gh4 1/1 Running 1 15m aws-node-jlnkd 1/1 Running 1 15m aws-node-k86pv 1/1 Running 0 12m aws-node-kjcdg 1/1 Running 1 14m cluster-autoscaler-aws-cluster-autoscaler-5d4446b58-d6frd 1/1 Running 1 15m coredns-85d5b4454c-jksbw 1/1 Running 1 24m coredns-85d5b4454c-x7wwd 1/1 Running 1 24m kube-proxy-92slm 1/1 Running 1 18m kube-proxy-bz5kb 1/1 Running 1 18m kube-proxy-zl7cj 1/1 Running 1 18m metrics-server-694d47d564-hzd8h 1/1 Running 1 15m","title":"List pods"},{"location":"getting-started/#cleanup","text":"To clean up your environment, destroy the Terraform modules in reverse order. Destroy the add-ons. terraform destroy -target=\"module.eks_blueprints_kubernetes_addons\" Destroy the EKS cluster. terraform destroy -target=\"module.eks_blueprints\" Destroy the VPC. terraform destroy -target=\"module.vpc\"","title":"Cleanup"},{"location":"node-groups/","text":"Node Groups \u00b6 The framework uses dedicated sub modules for creating AWS Managed Node Groups , Self-managed Node groups and Fargate profiles . These modules provide flexibility to add or remove managed/self-managed node groups/fargate profiles by simply adding/removing map of values to input config. See example . The aws-auth ConfigMap handled by this module allow your nodes to join your cluster, and you also use this ConfigMap to add RBAC access to IAM users and roles. Each Node Group can have dedicated IAM role, Launch template and Security Group to improve the security. Additional IAM Roles, Users and Accounts \u00b6 Access to EKS cluster using AWS IAM entities is enabled by the AWS IAM Authenticator for Kubernetes, which runs on the Amazon EKS control plane. The authenticator gets its configuration information from the aws-auth ConfigMap . The following config grants additional AWS IAM users or roles the ability to interact with your cluster. However, the best practice is to leverage soft-multitenancy with the help of Teams module. Teams feature helps to manage users with dedicated namespaces, RBAC, IAM roles and register users with aws-auth to provide access to the EKS Cluster. The below example demonstrates adding additional IAM Roles, IAM Users and Accounts using EKS Blueprints module module \"eks_blueprints\" { source = \"github.com/aws-ia/terraform-aws-eks-blueprints\" # EKS CLUSTER cluster_version = \"1.21\" # EKS Cluster Version vpc_id = \"<vpcid>\" # Enter VPC ID private_subnet_ids = [\"<subnet-a>\", \"<subnet-b>\", \"<subnet-c>\"] # Enter Private Subnet IDs # List of map_roles map_roles = [ { rolearn = \"arn:aws:iam::<aws-account-id>:role/<role-name>\" # The ARN of the IAM role username = \"ops-role\" # The user name within Kubernetes to map to the IAM role groups = [\"system:masters\"] # A list of groups within Kubernetes to which the role is mapped; Checkout K8s Role and Rolebindings } ] # List of map_users map_users = [ { userarn = \"arn:aws:iam::<aws-account-id>:user/<username>\" # The ARN of the IAM user to add. username = \"opsuser\" # The user name within Kubernetes to map to the IAM role groups = [\"system:masters\"] # A list of groups within Kubernetes to which the role is mapped; Checkout K8s Role and Rolebindings } ] map_accounts = [\"123456789\", \"9876543321\"] # List of AWS account ids } Managed Node Groups \u00b6 The below example demonstrates the minimum configuration required to deploy a managed node group. # EKS MANAGED NODE GROUPS managed_node_groups = { mng = { node_group_name = \"mng-ondemand\" instance_types = [\"m5.large\"] subnet_ids = [] # Mandatory Public or Private Subnet IDs disk_size = 100 # disk_size will be ignored when using Launch Templates } } The below example demonstrates advanced configuration options for a managed node group with launch templates. managed_node_groups = { # Managed Node groups with Launch templates using AMI TYPE mng_lt = { # Node Group configuration node_group_name = \"mng-lt\" create_launch_template = true # false will use the default launch template launch_template_os = \"amazonlinux2eks\" # amazonlinux2eks or windows or bottlerocket public_ip = false # Use this to enable public IP for EC2 instances; only for public subnets used in launch templates ; enable_monitoring = true create_iam_role = false # default is true; set to false to bring your own IAM Role with iam_role_arn option iam_role_arn = \"<ENTER-YOUR-IAM-ROLE>\" # Node groups creates a new IAM role if `iam_role_arn` is not specified pre_userdata = <<-EOT yum install -y amazon-ssm-agent systemctl enable amazon-ssm-agent && systemctl start amazon-ssm-agent EOT # Node Group scaling configuration desired_size = 3 max_size = 3 min_size = 3 # Node Group update configuration. Set the maximum number or percentage of unavailable nodes to be tolerated during the node group version update. update_config = [{ max_unavailable_percentage = 30 }] # Node Group compute configuration ami_type = \"AL2_x86_64\" # Amazon Linux 2(AL2_x86_64), AL2_x86_64_GPU, AL2_ARM_64, BOTTLEROCKET_x86_64, BOTTLEROCKET_ARM_64 release_version = \"\" # Enter AMI release version to deploy the latest AMI released by AWS. Used only when you specify ami_type capacity_type = \"ON_DEMAND\" # ON_DEMAND or SPOT instance_types = [\"m5.large\"] # List of instances to get capacity from multipe pools block_device_mappings = [ { device_name = \"/dev/xvda\" volume_type = \"gp3\" volume_size = 100 }, { device_name = \"/dev/xvdf\" # mount point to /local1 (it could be local2, depending upon the disks are attached during boot) volume_type = \"gp3\" # The volume type. Can be standard, gp2, gp3, io1, io2, sc1 or st1 (Default: gp3). volume_size = 100 delete_on_termination = true encrypted = true kms_key_id = \"\" # Custom KMS Key can be used to encrypt the disk iops = 3000 throughput = 125 } ] # Node Group network configuration subnet_ids = [] # Mandatory - # Define private/public subnets list with comma separated [\"subnet1\",\"subnet2\",\"subnet3\"] additional_iam_policies = [] # Attach additional IAM policies to the IAM role attached to this worker group # SSH ACCESS Optional - Recommended to use SSM Session manager remote_access = false ec2_ssh_key = \"\" ssh_security_group_id = \"\" # Taints can be applied through EKS API or through Bootstrap script using kubelet_extra_args # e.g., k8s_taints = [{key= \"spot\", value=\"true\", \"effect\"=\"NO_SCHEDULE\"}] k8s_taints = [{key= \"purpose\", value=\"execution\", effect=\"NO_SCHEDULE\"}] # Node Labels can be applied through EKS API or through Bootstrap script using kubelet_extra_args k8s_labels = { Environment = \"preprod\" Zone = \"dev\" WorkerType = \"ON_DEMAND\" } additional_tags = { ExtraTag = \"m4-on-demand\" Name = \"m4-on-demand\" subnet_type = \"private\" } launch_template_tags = { SomeAwsProviderDefaultTag1: \"TRUE\" SomeAwsProviderDefaultTag2: \"TRUE\" } } } The below example demonstrates advanced configuration options using GPU instances/ARM instances/Bottlerocket and custom AMIs managed node groups. #---------------------------------------------------------# # GPU instance type Worker Group #---------------------------------------------------------# gpu = { # 1> Node Group configuration - Part1 node_group_name = \"gpu-mg5\" # Max 40 characters for node group name create_launch_template = true # false will use the default launch template launch_template_os = \"amazonlinux2eks\" # amazonlinux2eks or bottlerocket public_ip = false # Use this to enable public IP for EC2 instances; only for public subnets used in launch templates ; pre_userdata = <<-EOT yum install -y amazon-ssm-agent systemctl enable amazon-ssm-agent && systemctl start amazon-ssm-agent EOT # 2> Node Group scaling configuration desired_size = 2 max_size = 2 min_size = 2 max_unavailable = 1 # or percentage = 20 # 3> Node Group compute configuration ami_type = \"AL2_x86_64_GPU\" # AL2_x86_64, AL2_x86_64_GPU, AL2_ARM_64, CUSTOM capacity_type = \"ON_DEMAND\" # ON_DEMAND or SPOT instance_types = [\"m5.large\"] # List of instances to get capacity from multipe pools block_device_mappings = [ { device_name = \"/dev/xvda\" volume_type = \"gp3\" volume_size = 100 } ] # 4> Node Group network configuration subnet_ids = [] # Defaults to private subnet-ids used by EKS Controle plane. Define your private/public subnets list with comma separated subnet_ids = ['subnet1','subnet2','subnet3'] k8s_taints = [] k8s_labels = { Environment = \"preprod\" Zone = \"dev\" WorkerType = \"ON_DEMAND\" } additional_tags = { ExtraTag = \"m5x-on-demand\" Name = \"m5x-on-demand\" subnet_type = \"private\" } launch_template_tags = { SomeAwsProviderDefaultTag1: \"TRUE\" SomeAwsProviderDefaultTag2: \"TRUE\" } } #---------------------------------------------------------# # ARM instance type Worker Group #---------------------------------------------------------# arm = { # 1> Node Group configuration - Part1 node_group_name = \"arm-m6g-2vcpu-8gb\" # Max 40 characters for node group name create_launch_template = true # false will use the default launch template launch_template_os = \"amazonlinux2eks\" # amazonlinux2eks or bottlerocket public_ip = false # Use this to enable public IP for EC2 instances; only for public subnets used in launch templates ; pre_userdata = <<-EOT yum install -y amazon-ssm-agent systemctl enable amazon-ssm-agent && systemctl start amazon-ssm-agent EOT # 2> Node Group scaling configuration desired_size = 2 max_size = 2 min_size = 2 max_unavailable = 1 # or percentage = 20 # 3> Node Group compute configuration ami_type = \"AL2_ARM_64\" # AL2_x86_64, AL2_x86_64_GPU, AL2_ARM_64, CUSTOM, BOTTLEROCKET_ARM_64, BOTTLEROCKET_x86_64 capacity_type = \"ON_DEMAND\" # ON_DEMAND or SPOT instance_types = [\"m6g.large\"] # List of instances to get capacity from multipe pools block_device_mappings = [ { device_name = \"/dev/xvda\" volume_type = \"gp3\" volume_size = 100 } ] # 4> Node Group network configuration subnet_ids = [] # Defaults to private subnet-ids used by EKS Controle plane. Define your private/public subnets list with comma separated subnet_ids = ['subnet1','subnet2','subnet3'] k8s_taints = [] k8s_labels = { Environment = \"preprod\" Zone = \"dev\" WorkerType = \"ON_DEMAND\" } additional_tags = { ExtraTag = \"m6g-on-demand\" Name = \"m6g-on-demand\" subnet_type = \"private\" } launch_template_tags = { SomeAwsProviderDefaultTag1: \"TRUE\" SomeAwsProviderDefaultTag2: \"TRUE\" } } #---------------------------------------------------------# # Bottlerocket ARM instance type Worker Group #---------------------------------------------------------# # Checkout this doc https://github.com/bottlerocket-os/bottlerocket for configuring userdata for Launch Templates bottlerocket_arm = { # 1> Node Group configuration node_group_name = \"btl-arm\" # Max 40 characters for node group name create_launch_template = true # false will use the default launch template launch_template_os = \"bottlerocket\" # amazonlinux2eks or bottlerocket public_ip = false # Use this to enable public IP for EC2 instances; only for public subnets used in launch templates ; # 2> Node Group scaling configuration desired_size = 2 max_size = 2 min_size = 2 max_unavailable = 1 # or percentage = 20 # 3> Node Group compute configuration ami_type = \"BOTTLEROCKET_ARM_64\" # AL2_x86_64, AL2_x86_64_GPU, AL2_ARM_64, CUSTOM, BOTTLEROCKET_ARM_64, BOTTLEROCKET_x86_64 capacity_type = \"ON_DEMAND\" # ON_DEMAND or SPOT instance_types = [\"m6g.large\"] # List of instances to get capacity from multipe pools disk_size = 50 # 4> Node Group network configuration subnet_ids = [] # Defaults to private subnet-ids used by EKS Controle plane. Define your private/public subnets list with comma separated subnet_ids = ['subnet1','subnet2','subnet3'] k8s_taints = [] k8s_labels = { Environment = \"preprod\" Zone = \"dev\" WorkerType = \"ON_DEMAND\" } additional_tags = { ExtraTag = \"m6g-on-demand\" Name = \"m6g-on-demand\" subnet_type = \"private\" } launch_template_tags = { SomeAwsProviderDefaultTag1: \"TRUE\" SomeAwsProviderDefaultTag2: \"TRUE\" } } #---------------------------------------------------------# # Bottlerocket instance type Worker Group #---------------------------------------------------------# # Checkout this doc https://github.com/bottlerocket-os/bottlerocket for configuring userdata for Launch Templates bottlerocket_x86 = { # 1> Node Group configuration - Part1 node_group_name = \"btl-x86\" # Max 40 characters for node group name create_launch_template = true # false will use the default launch template launch_template_os = \"bottlerocket\" # amazonlinux2eks or bottlerocket public_ip = false # Use this to enable public IP for EC2 instances; only for public subnets used in launch templates ; # 2> Node Group scaling configuration desired_size = 2 max_size = 2 min_size = 2 max_unavailable = 1 # or percentage = 20 # 3> Node Group compute configuration ami_type = \"BOTTLEROCKET_x86_64\" # AL2_x86_64, AL2_x86_64_GPU, AL2_ARM_64, CUSTOM, BOTTLEROCKET_ARM_64, BOTTLEROCKET_x86_64 capacity_type = \"ON_DEMAND\" # ON_DEMAND or SPOT instance_types = [\"m5.large\"] # List of instances to get capacity from multipe pools block_device_mappings = [ { device_name = \"/dev/xvda\" volume_type = \"gp3\" volume_size = 100 } ] # 4> Node Group network configuration subnet_ids = [] # Defaults to private subnet-ids used by EKS Controle plane. Define your private/public subnets list with comma separated subnet_ids = ['subnet1','subnet2','subnet3'] k8s_taints = [] k8s_labels = { Environment = \"preprod\" Zone = \"dev\" WorkerType = \"ON_DEMAND\" } additional_tags = { ExtraTag = \"m5x-on-demand\" Name = \"m5x-on-demand\" subnet_type = \"private\" } launch_template_tags = { SomeAwsProviderDefaultTag1: \"TRUE\" SomeAwsProviderDefaultTag2: \"TRUE\" } } #---------------------------------------------------------# # Managed Node groups with Launch templates using CUSTOM AMI with ContainerD runtime #---------------------------------------------------------# mng_custom_ami = { # Node Group configuration node_group_name = \"mng_custom_ami\" # Max 40 characters for node group name # custom_ami_id is optional when you provide ami_type. Enter the Custom AMI id if you want to use your own custom AMI custom_ami_id = data.aws_ami.amazonlinux2eks.id capacity_type = \"ON_DEMAND\" # ON_DEMAND or SPOT instance_types = [\"m5.large\"] # List of instances to get capacity from multipe pools # Launch template configuration create_launch_template = true # false will use the default launch template launch_template_os = \"amazonlinux2eks\" # amazonlinux2eks or bottlerocket # pre_userdata will be applied by using custom_ami_id or ami_type pre_userdata = <<-EOT yum install -y amazon-ssm-agent systemctl enable amazon-ssm-agent && systemctl start amazon-ssm-agent EOT # post_userdata will be applied only by using custom_ami_id post_userdata = <<-EOT echo \"Bootstrap successfully completed! You can further apply config or install to run after bootstrap if needed\" EOT # kubelet_extra_args used only when you pass custom_ami_id; # --node-labels is used to apply Kubernetes Labels to Nodes # --register-with-taints used to apply taints to Nodes # e.g., kubelet_extra_args='--node-labels=WorkerType=ON_DEMAND,noderole=spark --register-with-taints=ON_DEMAND=true:NoSchedule --max-pods=58', kubelet_extra_args = \"--node-labels=WorkerType=ON_DEMAND,noderole=spark --register-with-taints=test=true:NoSchedule --max-pods=20\" # bootstrap_extra_args used only when you pass custom_ami_id. Allows you to change the Container Runtime for Nodes # e.g., bootstrap_extra_args=\"--use-max-pods false --container-runtime containerd\" bootstrap_extra_args = \"--use-max-pods false --container-runtime containerd\" # Taints can be applied through EKS API or through Bootstrap script using kubelet_extra_args k8s_taints = [] # Node Labels can be applied through EKS API or through Bootstrap script using kubelet_extra_args k8s_labels = { Environment = \"preprod\" Zone = \"dev\" Runtime = \"containerd\" } enable_monitoring = true eni_delete = true public_ip = false # Use this to enable public IP for EC2 instances; only for public subnets used in launch templates # Node Group scaling configuration desired_size = 2 max_size = 2 min_size = 2 max_unavailable = 1 # or percentage = 20 block_device_mappings = [ { device_name = \"/dev/xvda\" volume_type = \"gp3\" volume_size = 150 } ] # Node Group network configuration subnet_type = \"private\" # public or private - Default uses the private subnets used in control plane if you don't pass the \"subnet_ids\" subnet_ids = [] # Defaults to private subnet-ids used by EKS Control plane. Define your private/public subnets list with comma separated subnet_ids = ['subnet1','subnet2','subnet3'] additional_iam_policies = [] # Attach additional IAM policies to the IAM role attached to this worker group # SSH ACCESS Optional - Recommended to use SSM Session manager remote_access = false ec2_ssh_key = \"\" ssh_security_group_id = \"\" additional_tags = { ExtraTag = \"mng-custom-ami\" Name = \"mng-custom-ami\" subnet_type = \"private\" } launch_template_tags = { SomeAwsProviderDefaultTag1: \"TRUE\" SomeAwsProviderDefaultTag2: \"TRUE\" } } Managed Node Groups with EC2 Spot Instances \u00b6 We recommend you to use managed-node groups (MNG) when using EC2 Spot instances. MNG creates the ASG for you following the Spot best practices: Configure the capacity_rebalance feature to true Manage the rebalance notification notice by launching a new instance proactively when there's an instance with a high-risk of being interrupted. This is instance is cordoned automatically so no new pods are scheduled there. Use capacity-optimized allocation strategy to launch an instance from the pool with more spare capacity Manage the instance interruption notice by draining the pods automatically to other nodes in the cluster. The below example demonstrates the minimum configuration required to deploy a managed node group using EC2 Spot instances. Notice how we're including more than one instance type for diversification purposes. Diversification is key, is how you'll get access to more spare capacity in EC2. You can use the Amazon EC2 Instance Selector CLI to get a list of instances that match your workload. # EKS MANAGED NODE GROUPS WITH SPOT INSTANCES spot_2vcpu_8mem = { node_group_name = \"mng-spot-2vcpu-8mem\" capacity_type = \"SPOT\" instance_types = [\"m5.large\", \"m4.large\", \"m6a.large\", \"m5a.large\", \"m5d.large\"] // Instances with same specs for memory and CPU so Cluster Autoscaler scales efficiently subnet_ids = [] # Mandatory Public or Private Subnet IDs disk_size = 100 # disk_size will be ignored when using Launch Templates k8s_taints = [{ key = \"spotInstance\", value = \"true\", effect = \"NO_SCHEDULE\" }] // Avoid scheduling stateful workloads in SPOT nodes } The below example demonstrates advanced configuration options for a managed node group with a custom launch templates. This is important if you decide to add the ability to scale-down to zero nodes. Cluster autoscaler needs to be able to identify which nodes to scale-down, and you do it by adding custom tags. # EKS MANAGED NODE GROUPS WITH SPOT INSTANCES spot_2vcpu_8mem = { node_group_name = \"mng-spot-2vcpu-8mem\" capacity_type = \"SPOT\" instance_types = [\"m5.large\", \"m4.large\", \"m6a.large\", \"m5a.large\", \"m5d.large\"] // Instances with same specs for memory and CPU # Node Group network configuration subnet_type = \"private\" # public or private - Default uses the private subnets used in control plane if you don't pass the \"subnet_ids\" subnet_ids = [] # Defaults to private subnet-ids used by EKS Control plane. Define your private/public subnets list with comma separated subnet_ids = ['subnet1','subnet2','subnet3'] k8s_taints = [{ key = \"spotInstance\", value = \"true\", effect = \"NO_SCHEDULE\" }] // Avoid scheduling stateful workloads in SPOT nodes min_size = 0 // Scale-down to zero nodes when no workloads are running, useful for pre-production environments # Launch template configuration create_launch_template = true # false will use the default launch template launch_template_os = \"amazonlinux2eks\" # amazonlinux2eks or bottlerocket # This is so cluster autoscaler can identify which node (using ASGs tags) to scale-down to zero nodes additional_tags = { \"k8s.io/cluster-autoscaler/node-template/label/eks.amazonaws.com/capacityType\" = \"SPOT\" \"k8s.io/cluster-autoscaler/node-template/label/eks/node_group_name\" = \"mng-spot-2vcpu-8mem\" } } Cluser autoscaler has the ability to set priorities on which node groups to scale by using the priority expander. To configure it, you need to add the following configuration in the eks_blueprints_kubernetes_addons block, like this: enable_cluster_autoscaler = true cluster_autoscaler_helm_config = { set = [ { name = \"extraArgs.expander\" value = \"priority\" }, { name = \"expanderPriorities\" value = <<-EOT 100: - .*-spot-2vcpu-8mem.* 90: - .*-spot-4vcpu-16mem.* 10: - .* EOT } ] } NOTE : You should not set to true both create_launch_template and remote_access or you'll end-up with new managed nodegroups that won't be able to join the cluster. Self-managed Node Groups \u00b6 The below example demonstrates the minimum configuration required to deploy a Self-managed node group. # EKS SELF MANAGED NODE GROUPS self_managed_node_groups = { self_mg_5 = { node_group_name = \"self-managed-ondemand\" launch_template_os = \"amazonlinux2eks\" subnet_ids = module.vpc.private_subnets } } The below example demonstrates advanced configuration options for a self-managed node group. - --node-labels parameter is used to apply labels to Nodes for self-managed node groups. e.g., kubelet_extra_args=\"--node-labels=WorkerType=SPOT,noderole=spark - --register-with-taints is used to apply taints to Nodes for self-managed node groups. e.g., kubelet_extra_args='--register-with-taints=spot=true:NoSchedule --max-pods=58' , self_managed_node_groups = { self_mg_5 = { node_group_name = \"self-managed-ondemand\" instance_type = \"m5.large\" custom_ami_id = \"ami-0dfaa019a300f219c\" # Bring your own custom AMI generated by Packer/ImageBuilder/Puppet etc. capacity_type = \"\" # Optional Use this only for SPOT capacity as capacity_type = \"spot\" launch_template_os = \"amazonlinux2eks\" # amazonlinux2eks or bottlerocket or windows pre_userdata = <<-EOT yum install -y amazon-ssm-agent systemctl enable amazon-ssm-agent && systemctl start amazon-ssm-agent EOT post_userdata = \"\" create_iam_role = false # Changing `create_iam_role=false` to bring your own IAM Role iam_role_arn = \"<ENTER_IAM_ROLE_ARN>\" # custom IAM role for aws-auth mapping; used when create_iam_role = false iam_instance_profile_name = \"<ENTER_IAM_INSTANCE_PROFILE_NAME>\" # IAM instance profile name for Launch templates; used when create_iam_role = false kubelet_extra_args = \"--node-labels=WorkerType=ON_DEMAND,noderole=spark --register-with-taints=test=true:NoSchedule --max-pods=20\" bootstrap_extra_args = \"\" block_device_mapping = [ { device_name = \"/dev/xvda\" # mount point to / volume_type = \"gp3\" volume_size = 20 }, { device_name = \"/dev/xvdf\" # mount point to /local1 (it could be local2, depending upon the disks are attached during boot) volume_type = \"gp3\" volume_size = 50 iops = 3000 throughput = 125 }, { device_name = \"/dev/xvdg\" # mount point to /local2 (it could be local1, depending upon the disks are attached during boot) volume_type = \"gp3\" volume_size = 100 iops = 3000 throughput = 125 } ] enable_monitoring = false public_ip = false # Enable only for public subnets # AUTOSCALING max_size = 3 min_size = 1 subnet_ids = [] # Mandatory Public or Private Subnet IDs additional_tags = { ExtraTag = \"m5x-on-demand\" Name = \"m5x-on-demand\" subnet_type = \"private\" } launch_template_tags = { SomeAwsProviderDefaultTag1: \"TRUE\" SomeAwsProviderDefaultTag2: \"TRUE\" } additional_iam_policies = [] }, } With the previous described example at block_device_mapping , in case you choose an instance that has local NVMe storage, you will achieve the three specified EBS disks plus all local NVMe disks that instance brings. For example, for an m5d.large you will end up with the following mount points: / for device named /dev/xvda , /local1 for device named /dev/xvdf , /local2 for device named /dev/xvdg , and /local3 for instance storage (in such case a disk with 70GB). Check the following references as you may desire: Amazon EBS and NVMe on Linux instances . AWS NVMe drivers for Windows instances EC2 Instance Update \u2013 M5 Instances with Local NVMe Storage (M5d) Self-Managed Node Groups with EC2 Spot Instances \u00b6 We recommend you to use managed-node groups (MNG) when using EC2 Spot instances. However, if you need to use self-managed node groups, you need to configure the ASG with the following Spot best practices: Configure the capacity_rebalance feature to true Use the capacity-optimized allocation strategy to launch an instance from the pool with more spare capacity Deploy the Node Termination Handler (NTH) to manage the rebalance recommendation and instance termination notice The below example demonstrates the minimum configuration required to deploy a self-managed node group. Notice how we're including more than one instance type for diversification purposes. Diversification is key, is how you'll get access to more spare capacity in EC2. You can use the Amazon EC2 Instance Selector CLI to get a list of instances that match your workload. spot_2vcpu_8mem = { node_group_name = \"smng-spot-2vcpu-8mem\" capacity_type = \"spot\" capacity_rebalance = true instance_types = [\"m5.large\", \"m4.large\", \"m6a.large\", \"m5a.large\", \"m5d.large\"] min_size = 0 subnet_ids = module.vpc.private_subnets launch_template_os = \"amazonlinux2eks\" # amazonlinux2eks or bottlerocket k8s_taints = [{ key = \"spotInstance\", value = \"true\", effect = \"NO_SCHEDULE\" }] } You need to deploy the NTH as an add-on, so make sure you include the following within the eks_blueprints_kubernetes_addons block: auto_scaling_group_names = module.eks_blueprints.self_managed_node_group_autoscaling_groups enable_aws_node_termination_handler = true Cluser autoscaler has the ability to set priorities on which node groups to scale by using the priority expander. To configure it, you need to add the following configuration in the eks_blueprints_kubernetes_addons block, like this: enable_cluster_autoscaler = true cluster_autoscaler_helm_config = { set = [ { name = \"extraArgs.expander\" value = \"priority\" }, { name = \"expanderPriorities\" value = <<-EOT 100: - .*-spot-2vcpu-8mem.* 90: - .*-spot-4vcpu-16mem.* 10: - .* EOT } ] } Fargate Profile \u00b6 The example below demonstrates how you can customize a Fargate profile for your cluster. fargate_profiles = { default = { fargate_profile_name = \"default\" fargate_profile_namespaces = [{ namespace = \"default\" k8s_labels = { Environment = \"preprod\" Zone = \"dev\" env = \"fargate\" } }] subnet_ids = [] # Provide list of private subnets additional_tags = { ExtraTag = \"Fargate\" } }, multi = { fargate_profile_name = \"multi-namespaces\" create_iam_role = false # Changing `create_iam_role=false` to bring your own IAM Role iam_role_arn = \"<ENTER_YOUR_IAM_ROLE>\" # custom IAM role for aws-auth mapping; used when `create_iam_role = false` additional_iam_policies = [] # additional IAM policies fargate_profile_namespaces = [{ namespace = \"default\" k8s_labels = { Environment = \"preprod\" Zone = \"dev\" OS = \"Fargate\" WorkerType = \"FARGATE\" Namespace = \"default\" } }, { namespace = \"sales\" k8s_labels = { Environment = \"preprod\" Zone = \"dev\" OS = \"Fargate\" WorkerType = \"FARGATE\" Namespace = \"default\" } }] subnet_ids = [] # Provide list of private subnets additional_tags = { ExtraTag = \"Fargate\" } }, } Windows Self-Managed Node Groups \u00b6 The example below demonstrates the minimum configuration required to deploy a Self-managed node group of Windows nodes. Refer to the AWS EKS user guide for more information about Windows support in EKS. # SELF-MANAGED NODE GROUP with Windows support enable_windows_support = true self_managed_node_groups = { ng_od_windows = { node_group_name = \"ng-od-windows\" launch_template_os = \"windows\" instance_type = \"m5n.large\" subnet_ids = module.vpc.private_subnets min_size = 2 } } In clusters where Windows support is enabled, workloads should have explicit node assignments configured using nodeSelector or affinity , as described in the Kubernetes document Assigning Pods to Nodes . For example, if you are enabling the metrics-server Kubernetes add-on (Helm chart), use the following configuration to ensure its pods are assigned to Linux nodes. See the EKS Cluster with Windows Support example for full Terraform configuration and workload deployment samples. enable_metrics_server = true metrics_server_helm_config = { set = [ { name = \"nodeSelector.kubernetes\\\\.io/os\" value = \"linux\" } ] }","title":"Node Groups"},{"location":"node-groups/#node-groups","text":"The framework uses dedicated sub modules for creating AWS Managed Node Groups , Self-managed Node groups and Fargate profiles . These modules provide flexibility to add or remove managed/self-managed node groups/fargate profiles by simply adding/removing map of values to input config. See example . The aws-auth ConfigMap handled by this module allow your nodes to join your cluster, and you also use this ConfigMap to add RBAC access to IAM users and roles. Each Node Group can have dedicated IAM role, Launch template and Security Group to improve the security.","title":"Node Groups"},{"location":"node-groups/#additional-iam-roles-users-and-accounts","text":"Access to EKS cluster using AWS IAM entities is enabled by the AWS IAM Authenticator for Kubernetes, which runs on the Amazon EKS control plane. The authenticator gets its configuration information from the aws-auth ConfigMap . The following config grants additional AWS IAM users or roles the ability to interact with your cluster. However, the best practice is to leverage soft-multitenancy with the help of Teams module. Teams feature helps to manage users with dedicated namespaces, RBAC, IAM roles and register users with aws-auth to provide access to the EKS Cluster. The below example demonstrates adding additional IAM Roles, IAM Users and Accounts using EKS Blueprints module module \"eks_blueprints\" { source = \"github.com/aws-ia/terraform-aws-eks-blueprints\" # EKS CLUSTER cluster_version = \"1.21\" # EKS Cluster Version vpc_id = \"<vpcid>\" # Enter VPC ID private_subnet_ids = [\"<subnet-a>\", \"<subnet-b>\", \"<subnet-c>\"] # Enter Private Subnet IDs # List of map_roles map_roles = [ { rolearn = \"arn:aws:iam::<aws-account-id>:role/<role-name>\" # The ARN of the IAM role username = \"ops-role\" # The user name within Kubernetes to map to the IAM role groups = [\"system:masters\"] # A list of groups within Kubernetes to which the role is mapped; Checkout K8s Role and Rolebindings } ] # List of map_users map_users = [ { userarn = \"arn:aws:iam::<aws-account-id>:user/<username>\" # The ARN of the IAM user to add. username = \"opsuser\" # The user name within Kubernetes to map to the IAM role groups = [\"system:masters\"] # A list of groups within Kubernetes to which the role is mapped; Checkout K8s Role and Rolebindings } ] map_accounts = [\"123456789\", \"9876543321\"] # List of AWS account ids }","title":"Additional IAM Roles, Users and Accounts"},{"location":"node-groups/#managed-node-groups","text":"The below example demonstrates the minimum configuration required to deploy a managed node group. # EKS MANAGED NODE GROUPS managed_node_groups = { mng = { node_group_name = \"mng-ondemand\" instance_types = [\"m5.large\"] subnet_ids = [] # Mandatory Public or Private Subnet IDs disk_size = 100 # disk_size will be ignored when using Launch Templates } } The below example demonstrates advanced configuration options for a managed node group with launch templates. managed_node_groups = { # Managed Node groups with Launch templates using AMI TYPE mng_lt = { # Node Group configuration node_group_name = \"mng-lt\" create_launch_template = true # false will use the default launch template launch_template_os = \"amazonlinux2eks\" # amazonlinux2eks or windows or bottlerocket public_ip = false # Use this to enable public IP for EC2 instances; only for public subnets used in launch templates ; enable_monitoring = true create_iam_role = false # default is true; set to false to bring your own IAM Role with iam_role_arn option iam_role_arn = \"<ENTER-YOUR-IAM-ROLE>\" # Node groups creates a new IAM role if `iam_role_arn` is not specified pre_userdata = <<-EOT yum install -y amazon-ssm-agent systemctl enable amazon-ssm-agent && systemctl start amazon-ssm-agent EOT # Node Group scaling configuration desired_size = 3 max_size = 3 min_size = 3 # Node Group update configuration. Set the maximum number or percentage of unavailable nodes to be tolerated during the node group version update. update_config = [{ max_unavailable_percentage = 30 }] # Node Group compute configuration ami_type = \"AL2_x86_64\" # Amazon Linux 2(AL2_x86_64), AL2_x86_64_GPU, AL2_ARM_64, BOTTLEROCKET_x86_64, BOTTLEROCKET_ARM_64 release_version = \"\" # Enter AMI release version to deploy the latest AMI released by AWS. Used only when you specify ami_type capacity_type = \"ON_DEMAND\" # ON_DEMAND or SPOT instance_types = [\"m5.large\"] # List of instances to get capacity from multipe pools block_device_mappings = [ { device_name = \"/dev/xvda\" volume_type = \"gp3\" volume_size = 100 }, { device_name = \"/dev/xvdf\" # mount point to /local1 (it could be local2, depending upon the disks are attached during boot) volume_type = \"gp3\" # The volume type. Can be standard, gp2, gp3, io1, io2, sc1 or st1 (Default: gp3). volume_size = 100 delete_on_termination = true encrypted = true kms_key_id = \"\" # Custom KMS Key can be used to encrypt the disk iops = 3000 throughput = 125 } ] # Node Group network configuration subnet_ids = [] # Mandatory - # Define private/public subnets list with comma separated [\"subnet1\",\"subnet2\",\"subnet3\"] additional_iam_policies = [] # Attach additional IAM policies to the IAM role attached to this worker group # SSH ACCESS Optional - Recommended to use SSM Session manager remote_access = false ec2_ssh_key = \"\" ssh_security_group_id = \"\" # Taints can be applied through EKS API or through Bootstrap script using kubelet_extra_args # e.g., k8s_taints = [{key= \"spot\", value=\"true\", \"effect\"=\"NO_SCHEDULE\"}] k8s_taints = [{key= \"purpose\", value=\"execution\", effect=\"NO_SCHEDULE\"}] # Node Labels can be applied through EKS API or through Bootstrap script using kubelet_extra_args k8s_labels = { Environment = \"preprod\" Zone = \"dev\" WorkerType = \"ON_DEMAND\" } additional_tags = { ExtraTag = \"m4-on-demand\" Name = \"m4-on-demand\" subnet_type = \"private\" } launch_template_tags = { SomeAwsProviderDefaultTag1: \"TRUE\" SomeAwsProviderDefaultTag2: \"TRUE\" } } } The below example demonstrates advanced configuration options using GPU instances/ARM instances/Bottlerocket and custom AMIs managed node groups. #---------------------------------------------------------# # GPU instance type Worker Group #---------------------------------------------------------# gpu = { # 1> Node Group configuration - Part1 node_group_name = \"gpu-mg5\" # Max 40 characters for node group name create_launch_template = true # false will use the default launch template launch_template_os = \"amazonlinux2eks\" # amazonlinux2eks or bottlerocket public_ip = false # Use this to enable public IP for EC2 instances; only for public subnets used in launch templates ; pre_userdata = <<-EOT yum install -y amazon-ssm-agent systemctl enable amazon-ssm-agent && systemctl start amazon-ssm-agent EOT # 2> Node Group scaling configuration desired_size = 2 max_size = 2 min_size = 2 max_unavailable = 1 # or percentage = 20 # 3> Node Group compute configuration ami_type = \"AL2_x86_64_GPU\" # AL2_x86_64, AL2_x86_64_GPU, AL2_ARM_64, CUSTOM capacity_type = \"ON_DEMAND\" # ON_DEMAND or SPOT instance_types = [\"m5.large\"] # List of instances to get capacity from multipe pools block_device_mappings = [ { device_name = \"/dev/xvda\" volume_type = \"gp3\" volume_size = 100 } ] # 4> Node Group network configuration subnet_ids = [] # Defaults to private subnet-ids used by EKS Controle plane. Define your private/public subnets list with comma separated subnet_ids = ['subnet1','subnet2','subnet3'] k8s_taints = [] k8s_labels = { Environment = \"preprod\" Zone = \"dev\" WorkerType = \"ON_DEMAND\" } additional_tags = { ExtraTag = \"m5x-on-demand\" Name = \"m5x-on-demand\" subnet_type = \"private\" } launch_template_tags = { SomeAwsProviderDefaultTag1: \"TRUE\" SomeAwsProviderDefaultTag2: \"TRUE\" } } #---------------------------------------------------------# # ARM instance type Worker Group #---------------------------------------------------------# arm = { # 1> Node Group configuration - Part1 node_group_name = \"arm-m6g-2vcpu-8gb\" # Max 40 characters for node group name create_launch_template = true # false will use the default launch template launch_template_os = \"amazonlinux2eks\" # amazonlinux2eks or bottlerocket public_ip = false # Use this to enable public IP for EC2 instances; only for public subnets used in launch templates ; pre_userdata = <<-EOT yum install -y amazon-ssm-agent systemctl enable amazon-ssm-agent && systemctl start amazon-ssm-agent EOT # 2> Node Group scaling configuration desired_size = 2 max_size = 2 min_size = 2 max_unavailable = 1 # or percentage = 20 # 3> Node Group compute configuration ami_type = \"AL2_ARM_64\" # AL2_x86_64, AL2_x86_64_GPU, AL2_ARM_64, CUSTOM, BOTTLEROCKET_ARM_64, BOTTLEROCKET_x86_64 capacity_type = \"ON_DEMAND\" # ON_DEMAND or SPOT instance_types = [\"m6g.large\"] # List of instances to get capacity from multipe pools block_device_mappings = [ { device_name = \"/dev/xvda\" volume_type = \"gp3\" volume_size = 100 } ] # 4> Node Group network configuration subnet_ids = [] # Defaults to private subnet-ids used by EKS Controle plane. Define your private/public subnets list with comma separated subnet_ids = ['subnet1','subnet2','subnet3'] k8s_taints = [] k8s_labels = { Environment = \"preprod\" Zone = \"dev\" WorkerType = \"ON_DEMAND\" } additional_tags = { ExtraTag = \"m6g-on-demand\" Name = \"m6g-on-demand\" subnet_type = \"private\" } launch_template_tags = { SomeAwsProviderDefaultTag1: \"TRUE\" SomeAwsProviderDefaultTag2: \"TRUE\" } } #---------------------------------------------------------# # Bottlerocket ARM instance type Worker Group #---------------------------------------------------------# # Checkout this doc https://github.com/bottlerocket-os/bottlerocket for configuring userdata for Launch Templates bottlerocket_arm = { # 1> Node Group configuration node_group_name = \"btl-arm\" # Max 40 characters for node group name create_launch_template = true # false will use the default launch template launch_template_os = \"bottlerocket\" # amazonlinux2eks or bottlerocket public_ip = false # Use this to enable public IP for EC2 instances; only for public subnets used in launch templates ; # 2> Node Group scaling configuration desired_size = 2 max_size = 2 min_size = 2 max_unavailable = 1 # or percentage = 20 # 3> Node Group compute configuration ami_type = \"BOTTLEROCKET_ARM_64\" # AL2_x86_64, AL2_x86_64_GPU, AL2_ARM_64, CUSTOM, BOTTLEROCKET_ARM_64, BOTTLEROCKET_x86_64 capacity_type = \"ON_DEMAND\" # ON_DEMAND or SPOT instance_types = [\"m6g.large\"] # List of instances to get capacity from multipe pools disk_size = 50 # 4> Node Group network configuration subnet_ids = [] # Defaults to private subnet-ids used by EKS Controle plane. Define your private/public subnets list with comma separated subnet_ids = ['subnet1','subnet2','subnet3'] k8s_taints = [] k8s_labels = { Environment = \"preprod\" Zone = \"dev\" WorkerType = \"ON_DEMAND\" } additional_tags = { ExtraTag = \"m6g-on-demand\" Name = \"m6g-on-demand\" subnet_type = \"private\" } launch_template_tags = { SomeAwsProviderDefaultTag1: \"TRUE\" SomeAwsProviderDefaultTag2: \"TRUE\" } } #---------------------------------------------------------# # Bottlerocket instance type Worker Group #---------------------------------------------------------# # Checkout this doc https://github.com/bottlerocket-os/bottlerocket for configuring userdata for Launch Templates bottlerocket_x86 = { # 1> Node Group configuration - Part1 node_group_name = \"btl-x86\" # Max 40 characters for node group name create_launch_template = true # false will use the default launch template launch_template_os = \"bottlerocket\" # amazonlinux2eks or bottlerocket public_ip = false # Use this to enable public IP for EC2 instances; only for public subnets used in launch templates ; # 2> Node Group scaling configuration desired_size = 2 max_size = 2 min_size = 2 max_unavailable = 1 # or percentage = 20 # 3> Node Group compute configuration ami_type = \"BOTTLEROCKET_x86_64\" # AL2_x86_64, AL2_x86_64_GPU, AL2_ARM_64, CUSTOM, BOTTLEROCKET_ARM_64, BOTTLEROCKET_x86_64 capacity_type = \"ON_DEMAND\" # ON_DEMAND or SPOT instance_types = [\"m5.large\"] # List of instances to get capacity from multipe pools block_device_mappings = [ { device_name = \"/dev/xvda\" volume_type = \"gp3\" volume_size = 100 } ] # 4> Node Group network configuration subnet_ids = [] # Defaults to private subnet-ids used by EKS Controle plane. Define your private/public subnets list with comma separated subnet_ids = ['subnet1','subnet2','subnet3'] k8s_taints = [] k8s_labels = { Environment = \"preprod\" Zone = \"dev\" WorkerType = \"ON_DEMAND\" } additional_tags = { ExtraTag = \"m5x-on-demand\" Name = \"m5x-on-demand\" subnet_type = \"private\" } launch_template_tags = { SomeAwsProviderDefaultTag1: \"TRUE\" SomeAwsProviderDefaultTag2: \"TRUE\" } } #---------------------------------------------------------# # Managed Node groups with Launch templates using CUSTOM AMI with ContainerD runtime #---------------------------------------------------------# mng_custom_ami = { # Node Group configuration node_group_name = \"mng_custom_ami\" # Max 40 characters for node group name # custom_ami_id is optional when you provide ami_type. Enter the Custom AMI id if you want to use your own custom AMI custom_ami_id = data.aws_ami.amazonlinux2eks.id capacity_type = \"ON_DEMAND\" # ON_DEMAND or SPOT instance_types = [\"m5.large\"] # List of instances to get capacity from multipe pools # Launch template configuration create_launch_template = true # false will use the default launch template launch_template_os = \"amazonlinux2eks\" # amazonlinux2eks or bottlerocket # pre_userdata will be applied by using custom_ami_id or ami_type pre_userdata = <<-EOT yum install -y amazon-ssm-agent systemctl enable amazon-ssm-agent && systemctl start amazon-ssm-agent EOT # post_userdata will be applied only by using custom_ami_id post_userdata = <<-EOT echo \"Bootstrap successfully completed! You can further apply config or install to run after bootstrap if needed\" EOT # kubelet_extra_args used only when you pass custom_ami_id; # --node-labels is used to apply Kubernetes Labels to Nodes # --register-with-taints used to apply taints to Nodes # e.g., kubelet_extra_args='--node-labels=WorkerType=ON_DEMAND,noderole=spark --register-with-taints=ON_DEMAND=true:NoSchedule --max-pods=58', kubelet_extra_args = \"--node-labels=WorkerType=ON_DEMAND,noderole=spark --register-with-taints=test=true:NoSchedule --max-pods=20\" # bootstrap_extra_args used only when you pass custom_ami_id. Allows you to change the Container Runtime for Nodes # e.g., bootstrap_extra_args=\"--use-max-pods false --container-runtime containerd\" bootstrap_extra_args = \"--use-max-pods false --container-runtime containerd\" # Taints can be applied through EKS API or through Bootstrap script using kubelet_extra_args k8s_taints = [] # Node Labels can be applied through EKS API or through Bootstrap script using kubelet_extra_args k8s_labels = { Environment = \"preprod\" Zone = \"dev\" Runtime = \"containerd\" } enable_monitoring = true eni_delete = true public_ip = false # Use this to enable public IP for EC2 instances; only for public subnets used in launch templates # Node Group scaling configuration desired_size = 2 max_size = 2 min_size = 2 max_unavailable = 1 # or percentage = 20 block_device_mappings = [ { device_name = \"/dev/xvda\" volume_type = \"gp3\" volume_size = 150 } ] # Node Group network configuration subnet_type = \"private\" # public or private - Default uses the private subnets used in control plane if you don't pass the \"subnet_ids\" subnet_ids = [] # Defaults to private subnet-ids used by EKS Control plane. Define your private/public subnets list with comma separated subnet_ids = ['subnet1','subnet2','subnet3'] additional_iam_policies = [] # Attach additional IAM policies to the IAM role attached to this worker group # SSH ACCESS Optional - Recommended to use SSM Session manager remote_access = false ec2_ssh_key = \"\" ssh_security_group_id = \"\" additional_tags = { ExtraTag = \"mng-custom-ami\" Name = \"mng-custom-ami\" subnet_type = \"private\" } launch_template_tags = { SomeAwsProviderDefaultTag1: \"TRUE\" SomeAwsProviderDefaultTag2: \"TRUE\" } }","title":"Managed Node Groups"},{"location":"node-groups/#managed-node-groups-with-ec2-spot-instances","text":"We recommend you to use managed-node groups (MNG) when using EC2 Spot instances. MNG creates the ASG for you following the Spot best practices: Configure the capacity_rebalance feature to true Manage the rebalance notification notice by launching a new instance proactively when there's an instance with a high-risk of being interrupted. This is instance is cordoned automatically so no new pods are scheduled there. Use capacity-optimized allocation strategy to launch an instance from the pool with more spare capacity Manage the instance interruption notice by draining the pods automatically to other nodes in the cluster. The below example demonstrates the minimum configuration required to deploy a managed node group using EC2 Spot instances. Notice how we're including more than one instance type for diversification purposes. Diversification is key, is how you'll get access to more spare capacity in EC2. You can use the Amazon EC2 Instance Selector CLI to get a list of instances that match your workload. # EKS MANAGED NODE GROUPS WITH SPOT INSTANCES spot_2vcpu_8mem = { node_group_name = \"mng-spot-2vcpu-8mem\" capacity_type = \"SPOT\" instance_types = [\"m5.large\", \"m4.large\", \"m6a.large\", \"m5a.large\", \"m5d.large\"] // Instances with same specs for memory and CPU so Cluster Autoscaler scales efficiently subnet_ids = [] # Mandatory Public or Private Subnet IDs disk_size = 100 # disk_size will be ignored when using Launch Templates k8s_taints = [{ key = \"spotInstance\", value = \"true\", effect = \"NO_SCHEDULE\" }] // Avoid scheduling stateful workloads in SPOT nodes } The below example demonstrates advanced configuration options for a managed node group with a custom launch templates. This is important if you decide to add the ability to scale-down to zero nodes. Cluster autoscaler needs to be able to identify which nodes to scale-down, and you do it by adding custom tags. # EKS MANAGED NODE GROUPS WITH SPOT INSTANCES spot_2vcpu_8mem = { node_group_name = \"mng-spot-2vcpu-8mem\" capacity_type = \"SPOT\" instance_types = [\"m5.large\", \"m4.large\", \"m6a.large\", \"m5a.large\", \"m5d.large\"] // Instances with same specs for memory and CPU # Node Group network configuration subnet_type = \"private\" # public or private - Default uses the private subnets used in control plane if you don't pass the \"subnet_ids\" subnet_ids = [] # Defaults to private subnet-ids used by EKS Control plane. Define your private/public subnets list with comma separated subnet_ids = ['subnet1','subnet2','subnet3'] k8s_taints = [{ key = \"spotInstance\", value = \"true\", effect = \"NO_SCHEDULE\" }] // Avoid scheduling stateful workloads in SPOT nodes min_size = 0 // Scale-down to zero nodes when no workloads are running, useful for pre-production environments # Launch template configuration create_launch_template = true # false will use the default launch template launch_template_os = \"amazonlinux2eks\" # amazonlinux2eks or bottlerocket # This is so cluster autoscaler can identify which node (using ASGs tags) to scale-down to zero nodes additional_tags = { \"k8s.io/cluster-autoscaler/node-template/label/eks.amazonaws.com/capacityType\" = \"SPOT\" \"k8s.io/cluster-autoscaler/node-template/label/eks/node_group_name\" = \"mng-spot-2vcpu-8mem\" } } Cluser autoscaler has the ability to set priorities on which node groups to scale by using the priority expander. To configure it, you need to add the following configuration in the eks_blueprints_kubernetes_addons block, like this: enable_cluster_autoscaler = true cluster_autoscaler_helm_config = { set = [ { name = \"extraArgs.expander\" value = \"priority\" }, { name = \"expanderPriorities\" value = <<-EOT 100: - .*-spot-2vcpu-8mem.* 90: - .*-spot-4vcpu-16mem.* 10: - .* EOT } ] } NOTE : You should not set to true both create_launch_template and remote_access or you'll end-up with new managed nodegroups that won't be able to join the cluster.","title":"Managed Node Groups with EC2 Spot Instances"},{"location":"node-groups/#self-managed-node-groups","text":"The below example demonstrates the minimum configuration required to deploy a Self-managed node group. # EKS SELF MANAGED NODE GROUPS self_managed_node_groups = { self_mg_5 = { node_group_name = \"self-managed-ondemand\" launch_template_os = \"amazonlinux2eks\" subnet_ids = module.vpc.private_subnets } } The below example demonstrates advanced configuration options for a self-managed node group. - --node-labels parameter is used to apply labels to Nodes for self-managed node groups. e.g., kubelet_extra_args=\"--node-labels=WorkerType=SPOT,noderole=spark - --register-with-taints is used to apply taints to Nodes for self-managed node groups. e.g., kubelet_extra_args='--register-with-taints=spot=true:NoSchedule --max-pods=58' , self_managed_node_groups = { self_mg_5 = { node_group_name = \"self-managed-ondemand\" instance_type = \"m5.large\" custom_ami_id = \"ami-0dfaa019a300f219c\" # Bring your own custom AMI generated by Packer/ImageBuilder/Puppet etc. capacity_type = \"\" # Optional Use this only for SPOT capacity as capacity_type = \"spot\" launch_template_os = \"amazonlinux2eks\" # amazonlinux2eks or bottlerocket or windows pre_userdata = <<-EOT yum install -y amazon-ssm-agent systemctl enable amazon-ssm-agent && systemctl start amazon-ssm-agent EOT post_userdata = \"\" create_iam_role = false # Changing `create_iam_role=false` to bring your own IAM Role iam_role_arn = \"<ENTER_IAM_ROLE_ARN>\" # custom IAM role for aws-auth mapping; used when create_iam_role = false iam_instance_profile_name = \"<ENTER_IAM_INSTANCE_PROFILE_NAME>\" # IAM instance profile name for Launch templates; used when create_iam_role = false kubelet_extra_args = \"--node-labels=WorkerType=ON_DEMAND,noderole=spark --register-with-taints=test=true:NoSchedule --max-pods=20\" bootstrap_extra_args = \"\" block_device_mapping = [ { device_name = \"/dev/xvda\" # mount point to / volume_type = \"gp3\" volume_size = 20 }, { device_name = \"/dev/xvdf\" # mount point to /local1 (it could be local2, depending upon the disks are attached during boot) volume_type = \"gp3\" volume_size = 50 iops = 3000 throughput = 125 }, { device_name = \"/dev/xvdg\" # mount point to /local2 (it could be local1, depending upon the disks are attached during boot) volume_type = \"gp3\" volume_size = 100 iops = 3000 throughput = 125 } ] enable_monitoring = false public_ip = false # Enable only for public subnets # AUTOSCALING max_size = 3 min_size = 1 subnet_ids = [] # Mandatory Public or Private Subnet IDs additional_tags = { ExtraTag = \"m5x-on-demand\" Name = \"m5x-on-demand\" subnet_type = \"private\" } launch_template_tags = { SomeAwsProviderDefaultTag1: \"TRUE\" SomeAwsProviderDefaultTag2: \"TRUE\" } additional_iam_policies = [] }, } With the previous described example at block_device_mapping , in case you choose an instance that has local NVMe storage, you will achieve the three specified EBS disks plus all local NVMe disks that instance brings. For example, for an m5d.large you will end up with the following mount points: / for device named /dev/xvda , /local1 for device named /dev/xvdf , /local2 for device named /dev/xvdg , and /local3 for instance storage (in such case a disk with 70GB). Check the following references as you may desire: Amazon EBS and NVMe on Linux instances . AWS NVMe drivers for Windows instances EC2 Instance Update \u2013 M5 Instances with Local NVMe Storage (M5d)","title":"Self-managed Node Groups"},{"location":"node-groups/#self-managed-node-groups-with-ec2-spot-instances","text":"We recommend you to use managed-node groups (MNG) when using EC2 Spot instances. However, if you need to use self-managed node groups, you need to configure the ASG with the following Spot best practices: Configure the capacity_rebalance feature to true Use the capacity-optimized allocation strategy to launch an instance from the pool with more spare capacity Deploy the Node Termination Handler (NTH) to manage the rebalance recommendation and instance termination notice The below example demonstrates the minimum configuration required to deploy a self-managed node group. Notice how we're including more than one instance type for diversification purposes. Diversification is key, is how you'll get access to more spare capacity in EC2. You can use the Amazon EC2 Instance Selector CLI to get a list of instances that match your workload. spot_2vcpu_8mem = { node_group_name = \"smng-spot-2vcpu-8mem\" capacity_type = \"spot\" capacity_rebalance = true instance_types = [\"m5.large\", \"m4.large\", \"m6a.large\", \"m5a.large\", \"m5d.large\"] min_size = 0 subnet_ids = module.vpc.private_subnets launch_template_os = \"amazonlinux2eks\" # amazonlinux2eks or bottlerocket k8s_taints = [{ key = \"spotInstance\", value = \"true\", effect = \"NO_SCHEDULE\" }] } You need to deploy the NTH as an add-on, so make sure you include the following within the eks_blueprints_kubernetes_addons block: auto_scaling_group_names = module.eks_blueprints.self_managed_node_group_autoscaling_groups enable_aws_node_termination_handler = true Cluser autoscaler has the ability to set priorities on which node groups to scale by using the priority expander. To configure it, you need to add the following configuration in the eks_blueprints_kubernetes_addons block, like this: enable_cluster_autoscaler = true cluster_autoscaler_helm_config = { set = [ { name = \"extraArgs.expander\" value = \"priority\" }, { name = \"expanderPriorities\" value = <<-EOT 100: - .*-spot-2vcpu-8mem.* 90: - .*-spot-4vcpu-16mem.* 10: - .* EOT } ] }","title":"Self-Managed Node Groups with EC2 Spot Instances"},{"location":"node-groups/#fargate-profile","text":"The example below demonstrates how you can customize a Fargate profile for your cluster. fargate_profiles = { default = { fargate_profile_name = \"default\" fargate_profile_namespaces = [{ namespace = \"default\" k8s_labels = { Environment = \"preprod\" Zone = \"dev\" env = \"fargate\" } }] subnet_ids = [] # Provide list of private subnets additional_tags = { ExtraTag = \"Fargate\" } }, multi = { fargate_profile_name = \"multi-namespaces\" create_iam_role = false # Changing `create_iam_role=false` to bring your own IAM Role iam_role_arn = \"<ENTER_YOUR_IAM_ROLE>\" # custom IAM role for aws-auth mapping; used when `create_iam_role = false` additional_iam_policies = [] # additional IAM policies fargate_profile_namespaces = [{ namespace = \"default\" k8s_labels = { Environment = \"preprod\" Zone = \"dev\" OS = \"Fargate\" WorkerType = \"FARGATE\" Namespace = \"default\" } }, { namespace = \"sales\" k8s_labels = { Environment = \"preprod\" Zone = \"dev\" OS = \"Fargate\" WorkerType = \"FARGATE\" Namespace = \"default\" } }] subnet_ids = [] # Provide list of private subnets additional_tags = { ExtraTag = \"Fargate\" } }, }","title":"Fargate Profile"},{"location":"node-groups/#windows-self-managed-node-groups","text":"The example below demonstrates the minimum configuration required to deploy a Self-managed node group of Windows nodes. Refer to the AWS EKS user guide for more information about Windows support in EKS. # SELF-MANAGED NODE GROUP with Windows support enable_windows_support = true self_managed_node_groups = { ng_od_windows = { node_group_name = \"ng-od-windows\" launch_template_os = \"windows\" instance_type = \"m5n.large\" subnet_ids = module.vpc.private_subnets min_size = 2 } } In clusters where Windows support is enabled, workloads should have explicit node assignments configured using nodeSelector or affinity , as described in the Kubernetes document Assigning Pods to Nodes . For example, if you are enabling the metrics-server Kubernetes add-on (Helm chart), use the following configuration to ensure its pods are assigned to Linux nodes. See the EKS Cluster with Windows Support example for full Terraform configuration and workload deployment samples. enable_metrics_server = true metrics_server_helm_config = { set = [ { name = \"nodeSelector.kubernetes\\\\.io/os\" value = \"linux\" } ] }","title":"Windows Self-Managed Node Groups"},{"location":"teams/","text":"Teams \u00b6 Introduction \u00b6 EKS Blueprints provides support for onboarding and managing teams and easily configuring cluster access. We currently support two Team types: application_teams and platform_teams . Application Teams represent teams managing workloads running in cluster namespaces and Platform Teams represents platform administrators who have admin access (masters group) to clusters. You can reference the aws-eks-teams module to create your own team implementations. ApplicationTeam \u00b6 To create an application_team for your cluster, you will need to supply a team name, with the options to pass map of labels, map of resource quotas, existing IAM entities (user/roles), and a directory where you may optionally place any policy definitions and generic manifests for the team. These manifests will be applied by EKS Blueprints and will be outside of the team control. NOTE: When the manifests are applied, namespaces are not checked. Therefore, you are responsible for namespace settings in the yaml files. As of today (2020-05-01), resource kubernetes_manifest can only be used ( terraform plan/apply... ) only after the cluster has been created and the cluster API can be accessed. Read \"Before you use this resource\" section for more information. To overcome this limitation, you can add/enable manifests_dir after you applied and created the cluster first. We are working on a better solution for this. Application Team Example \u00b6 # EKS Application Teams application_teams = { # First Team team-blue = { \"labels\" = { \"appName\" = \"example\", \"projectName\" = \"example\", \"environment\" = \"example\", \"domain\" = \"example\", \"uuid\" = \"example\", } \"quota\" = { \"requests.cpu\" = \"1000m\", \"requests.memory\" = \"4Gi\", \"limits.cpu\" = \"2000m\", \"limits.memory\" = \"8Gi\", \"pods\" = \"10\", \"secrets\" = \"10\", \"services\" = \"10\" } manifests_dir = \"./manifests\" # Belows are examples of IAM users and roles users = [ \"arn:aws:iam::123456789012:user/blue-team-user\", \"arn:aws:iam::123456789012:role/blue-team-sso-iam-role\" ] } # Second Team team-red = { \"labels\" = { \"appName\" = \"example2\", \"projectName\" = \"example2\", } \"quota\" = { \"requests.cpu\" = \"2000m\", \"requests.memory\" = \"8Gi\", \"limits.cpu\" = \"4000m\", \"limits.memory\" = \"16Gi\", \"pods\" = \"20\", \"secrets\" = \"20\", \"services\" = \"20\" } manifests_dir = \"./manifests2\" users = [ \"arn:aws:iam::123456789012:role/other-sso-iam-role\" ] } } EKS Blueprints will do the following for every provided team: Create a namespace Register quotas Register IAM users for cross-account access Create a shared role for cluster access. Alternatively, an existing role can be supplied. Register provided users/roles in the aws-auth configmap for kubectl and console access to the cluster and namespace. (Optionally) read all additional manifests (e.g., network policies, OPA policies, others) stored in a provided directory, and apply them. PlatformTeam \u00b6 To create an Platform Team for your cluster, simply use platform_teams . You will need to supply a team name and and all users/roles. Platform Team Example \u00b6 platform_teams = { admin-team-name-example = { users = [ \"arn:aws:iam::123456789012:user/admin-user\", \"arn:aws:iam::123456789012:role/org-admin-role\" ] } } Platform Team does the following: Registers IAM users for admin access to the cluster ( kubectl and console). Registers an existing role (or create a new role) for cluster access with trust relationship with the provided/created role. Cluster Access ( kubectl ) \u00b6 The output will contain the IAM roles for every application( application_teams_iam_role_arn ) or platform team( platform_teams_iam_role_arn ). To update your kubeconfig, you can run the following command: aws eks update-kubeconfig --name ${eks_cluster_id} --region ${AWS_REGION} --role-arn ${TEAM_ROLE_ARN} Make sure to replace the ${eks_cluster_id} , ${AWS_REGION} and ${TEAM_ROLE_ARN} with the actual values.","title":"Teams"},{"location":"teams/#teams","text":"","title":"Teams"},{"location":"teams/#introduction","text":"EKS Blueprints provides support for onboarding and managing teams and easily configuring cluster access. We currently support two Team types: application_teams and platform_teams . Application Teams represent teams managing workloads running in cluster namespaces and Platform Teams represents platform administrators who have admin access (masters group) to clusters. You can reference the aws-eks-teams module to create your own team implementations.","title":"Introduction"},{"location":"teams/#applicationteam","text":"To create an application_team for your cluster, you will need to supply a team name, with the options to pass map of labels, map of resource quotas, existing IAM entities (user/roles), and a directory where you may optionally place any policy definitions and generic manifests for the team. These manifests will be applied by EKS Blueprints and will be outside of the team control. NOTE: When the manifests are applied, namespaces are not checked. Therefore, you are responsible for namespace settings in the yaml files. As of today (2020-05-01), resource kubernetes_manifest can only be used ( terraform plan/apply... ) only after the cluster has been created and the cluster API can be accessed. Read \"Before you use this resource\" section for more information. To overcome this limitation, you can add/enable manifests_dir after you applied and created the cluster first. We are working on a better solution for this.","title":"ApplicationTeam"},{"location":"teams/#application-team-example","text":"# EKS Application Teams application_teams = { # First Team team-blue = { \"labels\" = { \"appName\" = \"example\", \"projectName\" = \"example\", \"environment\" = \"example\", \"domain\" = \"example\", \"uuid\" = \"example\", } \"quota\" = { \"requests.cpu\" = \"1000m\", \"requests.memory\" = \"4Gi\", \"limits.cpu\" = \"2000m\", \"limits.memory\" = \"8Gi\", \"pods\" = \"10\", \"secrets\" = \"10\", \"services\" = \"10\" } manifests_dir = \"./manifests\" # Belows are examples of IAM users and roles users = [ \"arn:aws:iam::123456789012:user/blue-team-user\", \"arn:aws:iam::123456789012:role/blue-team-sso-iam-role\" ] } # Second Team team-red = { \"labels\" = { \"appName\" = \"example2\", \"projectName\" = \"example2\", } \"quota\" = { \"requests.cpu\" = \"2000m\", \"requests.memory\" = \"8Gi\", \"limits.cpu\" = \"4000m\", \"limits.memory\" = \"16Gi\", \"pods\" = \"20\", \"secrets\" = \"20\", \"services\" = \"20\" } manifests_dir = \"./manifests2\" users = [ \"arn:aws:iam::123456789012:role/other-sso-iam-role\" ] } } EKS Blueprints will do the following for every provided team: Create a namespace Register quotas Register IAM users for cross-account access Create a shared role for cluster access. Alternatively, an existing role can be supplied. Register provided users/roles in the aws-auth configmap for kubectl and console access to the cluster and namespace. (Optionally) read all additional manifests (e.g., network policies, OPA policies, others) stored in a provided directory, and apply them.","title":"Application Team Example"},{"location":"teams/#platformteam","text":"To create an Platform Team for your cluster, simply use platform_teams . You will need to supply a team name and and all users/roles.","title":"PlatformTeam"},{"location":"teams/#platform-team-example","text":"platform_teams = { admin-team-name-example = { users = [ \"arn:aws:iam::123456789012:user/admin-user\", \"arn:aws:iam::123456789012:role/org-admin-role\" ] } } Platform Team does the following: Registers IAM users for admin access to the cluster ( kubectl and console). Registers an existing role (or create a new role) for cluster access with trust relationship with the provided/created role.","title":"Platform Team Example"},{"location":"teams/#cluster-access-kubectl","text":"The output will contain the IAM roles for every application( application_teams_iam_role_arn ) or platform team( platform_teams_iam_role_arn ). To update your kubeconfig, you can run the following command: aws eks update-kubeconfig --name ${eks_cluster_id} --region ${AWS_REGION} --role-arn ${TEAM_ROLE_ARN} Make sure to replace the ${eks_cluster_id} , ${AWS_REGION} and ${TEAM_ROLE_ARN} with the actual values.","title":"Cluster Access (kubectl)"},{"location":"add-ons/","text":"Kubernetes Addons Module \u00b6 The kubernetes-addons module within EKS Blueprints allows you to configure the add-ons you would like deployed into you EKS cluster with simple true/false flags. The framework currently provides support for add-ons listed in the current folder. Add-on Management \u00b6 The framework provides two approaches to managing add-on configuration for your EKS clusters. They are: Via Terraform by leveraging the Terraform Helm provider . Via GitOps with ArgoCD . Terraform \u00b6 The default method for managing add-on configuration is via Terraform. By default, each individual add-on module will do the following: Create any AWS resources needed to support add-on functionality. Deploy a Helm chart into your EKS cluster by leveraging the Terraform Helm provider. In order to deploy an add-on with default configuration, simply enable the add-on via Terraform properties. module \"eks_blueprints_kubernetes_addons\" { source = \"github.com/aws-ia/terraform-aws-eks-blueprints//modules/kubernetes-addons\" cluster_id = <EKS-CLUSTER-ID> # EKS Addons enable_amazon_eks_aws_ebs_csi_driver = true enable_amazon_eks_coredns = true enable_amazon_eks_kube_proxy = true enable_amazon_eks_vpc_cni = true #K8s Add-ons enable_argocd = true enable_aws_for_fluentbit = true enable_aws_load_balancer_controller = true enable_cluster_autoscaler = true enable_metrics_server = true } To customize the behavior of the Helm charts that are ultimately deployed, you can supply custom Helm configuration. The following demonstrates how you can supply this configuration, including a dedicated values.yaml file. enable_metrics_server = true metrics_server_helm_config = { name = \"metrics-server\" repository = \"https://kubernetes-sigs.github.io/metrics-server/\" chart = \"metrics-server\" version = \"3.8.1\" namespace = \"kube-system\" timeout = \"1200\" # (Optional) Example to pass values.yaml from your local repo values = [templatefile(\"${path.module}/values.yaml\", { operating_system = \"linux\" })] } Each add-on module is configured to fetch Helm Charts from Open Source, public Helm repositories and Docker images from Docker Hub/Public ECR repositories. This requires outbound Internet connection from your EKS Cluster. If you would like to use private repositories, you can download Docker images for each add-on and push them to an AWS ECR repository. ECR can be accessed from within a private existing VPC using an ECR VPC endpoint. For instructions on how to download existing images and push them to ECR, see ECR instructions . GitOps with ArgoCD \u00b6 To indicate that you would like to manage add-ons via ArgoCD, you must do the following: Enable the ArgoCD add-on by setting argocd_enable to true . Specify you would like ArgoCD to be responsible for deploying your add-ons by setting argocd_manage_add_ons to true . This will prevent the individual Terraform add-on modules from deploying Helm charts. Pass Application configuration for your add-ons repository via the argocd_applications property. Note, that the add_on_application flag in your Application configuration must be set to true . enable_argocd = true argocd_manage_add_ons = true argocd_applications = { infra = { namespace = \"argocd\" path = \"<path>\" repo_url = \"<repo_url>\" values = {} add_on_application = true # Indicates the root add-on application. } } GitOps Bridge \u00b6 When managing add-ons via ArgoCD, certain AWS resources may still need to be created via Terraform in order to support add-on functionality (e.g. IAM Roles and Services Account). Certain resource values will also need to passed from Terraform to ArgoCD via the ArgoCD Application resource's values map. We refer to this concept as the GitOps Bridge To ensure that AWS resources needed for add-on functionality are created, you still need to indicate in Terraform configuration which add-ons will be managed via ArgoCD. To do so, simply enable the add-ons via their boolean properties. enable_metrics_server = true # Deploys Metrics Server Addon enable_cluster_autoscaler = true # Deploys Cluster Autoscaler Addon enable_prometheus = true # Deploys Prometheus Addon This will indicate to each add-on module that it should create the necessary AWS resources and pass the relevant values to the ArgoCD Application resource via the Application's values map.","title":"Overview"},{"location":"add-ons/#kubernetes-addons-module","text":"The kubernetes-addons module within EKS Blueprints allows you to configure the add-ons you would like deployed into you EKS cluster with simple true/false flags. The framework currently provides support for add-ons listed in the current folder.","title":"Kubernetes Addons Module"},{"location":"add-ons/#add-on-management","text":"The framework provides two approaches to managing add-on configuration for your EKS clusters. They are: Via Terraform by leveraging the Terraform Helm provider . Via GitOps with ArgoCD .","title":"Add-on Management"},{"location":"add-ons/#terraform","text":"The default method for managing add-on configuration is via Terraform. By default, each individual add-on module will do the following: Create any AWS resources needed to support add-on functionality. Deploy a Helm chart into your EKS cluster by leveraging the Terraform Helm provider. In order to deploy an add-on with default configuration, simply enable the add-on via Terraform properties. module \"eks_blueprints_kubernetes_addons\" { source = \"github.com/aws-ia/terraform-aws-eks-blueprints//modules/kubernetes-addons\" cluster_id = <EKS-CLUSTER-ID> # EKS Addons enable_amazon_eks_aws_ebs_csi_driver = true enable_amazon_eks_coredns = true enable_amazon_eks_kube_proxy = true enable_amazon_eks_vpc_cni = true #K8s Add-ons enable_argocd = true enable_aws_for_fluentbit = true enable_aws_load_balancer_controller = true enable_cluster_autoscaler = true enable_metrics_server = true } To customize the behavior of the Helm charts that are ultimately deployed, you can supply custom Helm configuration. The following demonstrates how you can supply this configuration, including a dedicated values.yaml file. enable_metrics_server = true metrics_server_helm_config = { name = \"metrics-server\" repository = \"https://kubernetes-sigs.github.io/metrics-server/\" chart = \"metrics-server\" version = \"3.8.1\" namespace = \"kube-system\" timeout = \"1200\" # (Optional) Example to pass values.yaml from your local repo values = [templatefile(\"${path.module}/values.yaml\", { operating_system = \"linux\" })] } Each add-on module is configured to fetch Helm Charts from Open Source, public Helm repositories and Docker images from Docker Hub/Public ECR repositories. This requires outbound Internet connection from your EKS Cluster. If you would like to use private repositories, you can download Docker images for each add-on and push them to an AWS ECR repository. ECR can be accessed from within a private existing VPC using an ECR VPC endpoint. For instructions on how to download existing images and push them to ECR, see ECR instructions .","title":"Terraform"},{"location":"add-ons/#gitops-with-argocd","text":"To indicate that you would like to manage add-ons via ArgoCD, you must do the following: Enable the ArgoCD add-on by setting argocd_enable to true . Specify you would like ArgoCD to be responsible for deploying your add-ons by setting argocd_manage_add_ons to true . This will prevent the individual Terraform add-on modules from deploying Helm charts. Pass Application configuration for your add-ons repository via the argocd_applications property. Note, that the add_on_application flag in your Application configuration must be set to true . enable_argocd = true argocd_manage_add_ons = true argocd_applications = { infra = { namespace = \"argocd\" path = \"<path>\" repo_url = \"<repo_url>\" values = {} add_on_application = true # Indicates the root add-on application. } }","title":"GitOps with ArgoCD"},{"location":"add-ons/#gitops-bridge","text":"When managing add-ons via ArgoCD, certain AWS resources may still need to be created via Terraform in order to support add-on functionality (e.g. IAM Roles and Services Account). Certain resource values will also need to passed from Terraform to ArgoCD via the ArgoCD Application resource's values map. We refer to this concept as the GitOps Bridge To ensure that AWS resources needed for add-on functionality are created, you still need to indicate in Terraform configuration which add-ons will be managed via ArgoCD. To do so, simply enable the add-ons via their boolean properties. enable_metrics_server = true # Deploys Metrics Server Addon enable_cluster_autoscaler = true # Deploys Cluster Autoscaler Addon enable_prometheus = true # Deploys Prometheus Addon This will indicate to each add-on module that it should create the necessary AWS resources and pass the relevant values to the ArgoCD Application resource via the Application's values map.","title":"GitOps Bridge"},{"location":"add-ons/agones/","text":"Agones \u00b6 Agones is an open source platform for deploying, hosting, scaling, and orchestrating dedicated game servers for large scale multiplayer games on Kubernetes. For complete project documentation, please visit the Agones documentation site . Usage \u00b6 Agones can be deployed by enabling the add-on via the following. enable_agones = true You can optionally customize the Helm chart that deploys Agones via the following configuration. NOTE: Agones requires a Node group in Public Subnets and enable Public IP enable_agones = true # Optional agones_helm_config agones_helm_config = { name = \"agones\" chart = \"agones\" repository = \"https://agones.dev/chart/stable\" version = \"1.21.0\" namespace = \"agones-system\" # Agones recommends to install in it's own namespace such as `agones-system` as shown here. You can specify any namespace other than `kube-system` values = [templatefile(\"${path.module}/helm_values/agones-values.yaml\", { expose_udp = true gameserver_namespaces = \"{${join(\",\", [\"default\", \"xbox-gameservers\", \"xbox-gameservers\"])}}\" gameserver_minport = 7000 gameserver_maxport = 8000 })] } GitOps Configuration \u00b6 The following properties are made available for use when managing the add-on via GitOps. agones = { enable = true }","title":"Agones"},{"location":"add-ons/agones/#agones","text":"Agones is an open source platform for deploying, hosting, scaling, and orchestrating dedicated game servers for large scale multiplayer games on Kubernetes. For complete project documentation, please visit the Agones documentation site .","title":"Agones"},{"location":"add-ons/agones/#usage","text":"Agones can be deployed by enabling the add-on via the following. enable_agones = true You can optionally customize the Helm chart that deploys Agones via the following configuration. NOTE: Agones requires a Node group in Public Subnets and enable Public IP enable_agones = true # Optional agones_helm_config agones_helm_config = { name = \"agones\" chart = \"agones\" repository = \"https://agones.dev/chart/stable\" version = \"1.21.0\" namespace = \"agones-system\" # Agones recommends to install in it's own namespace such as `agones-system` as shown here. You can specify any namespace other than `kube-system` values = [templatefile(\"${path.module}/helm_values/agones-values.yaml\", { expose_udp = true gameserver_namespaces = \"{${join(\",\", [\"default\", \"xbox-gameservers\", \"xbox-gameservers\"])}}\" gameserver_minport = 7000 gameserver_maxport = 8000 })] }","title":"Usage"},{"location":"add-ons/agones/#gitops-configuration","text":"The following properties are made available for use when managing the add-on via GitOps. agones = { enable = true }","title":"GitOps Configuration"},{"location":"add-ons/apache-airflow/","text":"Apache Airflow add-on \u00b6 This document describes the details of the best practices for building and deploying Self-managed Highly Scalable Apache Airflow cluster on Kubernetes(Amazon EKS) Cluster . Alternatively, Amazon also provides a fully managed Apache Airflow service(MWAA). Please see this example if you are looking to build Amazon MWAA. Apache Airflow is used for the scheduling and orchestration of data pipelines or workflows. Orchestration of data pipelines refers to the sequencing, coordination, scheduling, and managing complex data pipelines from diverse sources. A workflow is represented as a DAG (a Directed Acyclic Graph), and contains individual pieces of work called Tasks , arranged with dependencies and data flows taken into account. Production considerations for running Apache Airflow on EKS \u00b6 Airflow Metadata Database \u00b6 It is advised to set up an external database for the Airflow metastore. The default Helm chart deploys a Postgres database running in a container but this should be used only for development. Apache Airflow recommends to use MySQL or Postgres. This deployment configures the highly available Amazon RDS Postgres database as external database. PgBouncer for Amazon Postgres RDS \u00b6 Airflow can open a lot of database connections due to its distributed nature and using a connection pooler can significantly reduce the number of open connections on the database. This deployment enables the PgBouncer for Postgres Webserver Secret Key \u00b6 You should set a static webserver secret key when deploying with this chart as it will help ensure your Airflow components only restart when necessary. This deployment creates Kubernetes secret for Webserver Secret Key and applies to Airflow Managing DAG Files with GitHub and EFS \u00b6 It's recommended to Mounting DAGs using Git-Sync sidecar with Persistence enabled. Developers can create a repo to store the DAGs and configure to sync with Airflow servers. This deployment provisions EFS(Amazon Elastic File System) through Persistent Volume Claim with an access mode of ReadWriteMany. The Airflow scheduler pod will sync DAGs from a git repository onto the PVC every configured number of seconds. The other pods will read the synced DAGs. GitSync is configured with a sample repo with this example. This can be replaced with your internal GitHub repo Managing Log Files with S3 with IRSA \u00b6 Airflow writes logs for tasks in a way that allows you to see the logs for each task separately in the Airflow UI. Core Airflow implements writing and serving logs locally. However, you can also write logs to remote services via community providers, or write your own loggers. This example configures S3 bucket to store the Airflow logs. IAM roles for server account(IRSA) is configured for Airflow pods to access this S3 bucket. Airflow StatsD Metrics \u00b6 This example configures to send the metrics to an existing StatsD to Prometheus endpoint. This can be configured to send it to external StatsD instance Airflow Executors (Celery Vs Kubernetes) \u00b6 This deployment uses Kubernetes Executor. With KubernetesExecutor, each task runs in its own pod. The pod is created when the task is queued, and terminates when the task completes. With KubernetesExecutor, the workers (pods) talk directly to the same Postgres backend as the Scheduler and can to a large degree take on the labor of task monitoring. KubernetesExecutor can work well when your tasks are not very uniform with respect to resource requirements or images. Each task on the Kubernetes executor gets its own pod, which allows you to pass an executor_config in your task params. This lets you assign resources at the task level by passing an executor_config. e.g, the first task may be a sensor that only requires a few resources, but the downstream tasks have to run on your GPU node pool with a higher CPU request. See the code snippet below Since each task is a pod, it is managed independently of the code deploys. This is great for longer running tasks or environments with a lot of users, as users can push new code without fear of interrupting that task. This makes the k8s executor the most fault-tolerant option, as running tasks won\u2019t be affected when code is pushed In contrast to CeleryExecutor, KubernetesExecutor does not require additional components such as Redis, but does require access to Kubernetes cluster. Pod monitoring can be done with native Kubernetes tools A Kubernetes watcher is a thread that can subscribe to every change that occurs in Kubernetes\u2019 database. It is alerted when pods start, run, end, and fail. By monitoring this stream, the KubernetesExecutor can discover that the worker crashed and correctly report the task as failed Airflow Schedulers \u00b6 The Airflow scheduler monitors all tasks and DAGs, then triggers the task instances once their dependencies are complete. Ths deployment uses HA scheduler with two replicas to take advantage of the existing metadata database. Accessing Airflow Web UI \u00b6 This deployment example uses internet facing Load Balancer to easily access the WebUI however it's not recommended for Production. You can modify the values.yaml to set the Load Balancer to internal and upload certificate to use HTTPS. Ensure access to the WebUI using internal domain and network. Checkout the examples of deploying and using Apache Airflow on Amazon EKS. Usage \u00b6 The Apache Airflow can be deployed by enabling the add-on via the following. enable_airflow = true For production workloads, you can use this example with custom Helm Config. enable_airflow = true airflow_helm_config = { name = \"airflow\" chart = \"airflow\" repository = \"https://airflow.apache.org\" version = \"1.6.0\" namespace = module.airflow_irsa.namespace create_namespace = false timeout = 360 description = \"Apache Airflow v2 Helm chart deployment configuration\" # Check the example for `values.yaml` file values = [templatefile(\"${path.module}/values.yaml\", { # Airflow Postgres RDS Config airflow_db_user = \"airflow\" airflow_db_name = module.db.db_instance_name airflow_db_host = element(split(\":\", module.db.db_instance_endpoint), 0) # S3 bucket config for Logs s3_bucket_name = aws_s3_bucket.this.id webserver_secret_name = local.airflow_webserver_secret_name airflow_service_account = local.airflow_service_account })] set_sensitive = [ { name = \"data.metadataConnection.pass\" value = data.aws_secretsmanager_secret_version.postgres.secret_string } ] } Once deployed, you will be able to see the deployment status kubectl get deployment -n airflow NAME READY UP-TO-DATE AVAILABLE AGE airflow-pgbouncer 1 /1 1 1 77m airflow-scheduler 2 /2 2 2 77m airflow-statsd 1 /1 1 1 77m airflow-triggerer 1 /1 1 1 77m airflow-webserver 2 /2 2 2 77m","title":"Apache Airflow add-on"},{"location":"add-ons/apache-airflow/#apache-airflow-add-on","text":"This document describes the details of the best practices for building and deploying Self-managed Highly Scalable Apache Airflow cluster on Kubernetes(Amazon EKS) Cluster . Alternatively, Amazon also provides a fully managed Apache Airflow service(MWAA). Please see this example if you are looking to build Amazon MWAA. Apache Airflow is used for the scheduling and orchestration of data pipelines or workflows. Orchestration of data pipelines refers to the sequencing, coordination, scheduling, and managing complex data pipelines from diverse sources. A workflow is represented as a DAG (a Directed Acyclic Graph), and contains individual pieces of work called Tasks , arranged with dependencies and data flows taken into account.","title":"Apache Airflow add-on"},{"location":"add-ons/apache-airflow/#production-considerations-for-running-apache-airflow-on-eks","text":"","title":"Production considerations for running Apache Airflow on EKS"},{"location":"add-ons/apache-airflow/#airflow-metadata-database","text":"It is advised to set up an external database for the Airflow metastore. The default Helm chart deploys a Postgres database running in a container but this should be used only for development. Apache Airflow recommends to use MySQL or Postgres. This deployment configures the highly available Amazon RDS Postgres database as external database.","title":"Airflow Metadata Database"},{"location":"add-ons/apache-airflow/#pgbouncer-for-amazon-postgres-rds","text":"Airflow can open a lot of database connections due to its distributed nature and using a connection pooler can significantly reduce the number of open connections on the database. This deployment enables the PgBouncer for Postgres","title":"PgBouncer for Amazon Postgres RDS"},{"location":"add-ons/apache-airflow/#webserver-secret-key","text":"You should set a static webserver secret key when deploying with this chart as it will help ensure your Airflow components only restart when necessary. This deployment creates Kubernetes secret for Webserver Secret Key and applies to Airflow","title":"Webserver Secret Key"},{"location":"add-ons/apache-airflow/#managing-dag-files-with-github-and-efs","text":"It's recommended to Mounting DAGs using Git-Sync sidecar with Persistence enabled. Developers can create a repo to store the DAGs and configure to sync with Airflow servers. This deployment provisions EFS(Amazon Elastic File System) through Persistent Volume Claim with an access mode of ReadWriteMany. The Airflow scheduler pod will sync DAGs from a git repository onto the PVC every configured number of seconds. The other pods will read the synced DAGs. GitSync is configured with a sample repo with this example. This can be replaced with your internal GitHub repo","title":"Managing DAG Files with GitHub and EFS"},{"location":"add-ons/apache-airflow/#managing-log-files-with-s3-with-irsa","text":"Airflow writes logs for tasks in a way that allows you to see the logs for each task separately in the Airflow UI. Core Airflow implements writing and serving logs locally. However, you can also write logs to remote services via community providers, or write your own loggers. This example configures S3 bucket to store the Airflow logs. IAM roles for server account(IRSA) is configured for Airflow pods to access this S3 bucket.","title":"Managing Log Files with S3 with IRSA"},{"location":"add-ons/apache-airflow/#airflow-statsd-metrics","text":"This example configures to send the metrics to an existing StatsD to Prometheus endpoint. This can be configured to send it to external StatsD instance","title":"Airflow StatsD Metrics"},{"location":"add-ons/apache-airflow/#airflow-executors-celery-vs-kubernetes","text":"This deployment uses Kubernetes Executor. With KubernetesExecutor, each task runs in its own pod. The pod is created when the task is queued, and terminates when the task completes. With KubernetesExecutor, the workers (pods) talk directly to the same Postgres backend as the Scheduler and can to a large degree take on the labor of task monitoring. KubernetesExecutor can work well when your tasks are not very uniform with respect to resource requirements or images. Each task on the Kubernetes executor gets its own pod, which allows you to pass an executor_config in your task params. This lets you assign resources at the task level by passing an executor_config. e.g, the first task may be a sensor that only requires a few resources, but the downstream tasks have to run on your GPU node pool with a higher CPU request. See the code snippet below Since each task is a pod, it is managed independently of the code deploys. This is great for longer running tasks or environments with a lot of users, as users can push new code without fear of interrupting that task. This makes the k8s executor the most fault-tolerant option, as running tasks won\u2019t be affected when code is pushed In contrast to CeleryExecutor, KubernetesExecutor does not require additional components such as Redis, but does require access to Kubernetes cluster. Pod monitoring can be done with native Kubernetes tools A Kubernetes watcher is a thread that can subscribe to every change that occurs in Kubernetes\u2019 database. It is alerted when pods start, run, end, and fail. By monitoring this stream, the KubernetesExecutor can discover that the worker crashed and correctly report the task as failed","title":"Airflow Executors (Celery Vs Kubernetes)"},{"location":"add-ons/apache-airflow/#airflow-schedulers","text":"The Airflow scheduler monitors all tasks and DAGs, then triggers the task instances once their dependencies are complete. Ths deployment uses HA scheduler with two replicas to take advantage of the existing metadata database.","title":"Airflow Schedulers"},{"location":"add-ons/apache-airflow/#accessing-airflow-web-ui","text":"This deployment example uses internet facing Load Balancer to easily access the WebUI however it's not recommended for Production. You can modify the values.yaml to set the Load Balancer to internal and upload certificate to use HTTPS. Ensure access to the WebUI using internal domain and network. Checkout the examples of deploying and using Apache Airflow on Amazon EKS.","title":"Accessing Airflow Web UI"},{"location":"add-ons/apache-airflow/#usage","text":"The Apache Airflow can be deployed by enabling the add-on via the following. enable_airflow = true For production workloads, you can use this example with custom Helm Config. enable_airflow = true airflow_helm_config = { name = \"airflow\" chart = \"airflow\" repository = \"https://airflow.apache.org\" version = \"1.6.0\" namespace = module.airflow_irsa.namespace create_namespace = false timeout = 360 description = \"Apache Airflow v2 Helm chart deployment configuration\" # Check the example for `values.yaml` file values = [templatefile(\"${path.module}/values.yaml\", { # Airflow Postgres RDS Config airflow_db_user = \"airflow\" airflow_db_name = module.db.db_instance_name airflow_db_host = element(split(\":\", module.db.db_instance_endpoint), 0) # S3 bucket config for Logs s3_bucket_name = aws_s3_bucket.this.id webserver_secret_name = local.airflow_webserver_secret_name airflow_service_account = local.airflow_service_account })] set_sensitive = [ { name = \"data.metadataConnection.pass\" value = data.aws_secretsmanager_secret_version.postgres.secret_string } ] } Once deployed, you will be able to see the deployment status kubectl get deployment -n airflow NAME READY UP-TO-DATE AVAILABLE AGE airflow-pgbouncer 1 /1 1 1 77m airflow-scheduler 2 /2 2 2 77m airflow-statsd 1 /1 1 1 77m airflow-triggerer 1 /1 1 1 77m airflow-webserver 2 /2 2 2 77m","title":"Usage"},{"location":"add-ons/argocd/","text":"ArgoCD \u00b6 ArgoCD Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. Application definitions, configurations, and environments should be declarative and version controlled. Application deployment and lifecycle management should be automated, auditable, and easy to understand. Usage \u00b6 ArgoCD can be deployed by enabling the add-on via the following. enable_argocd = true Admin Password \u00b6 ArgoCD has a built-in admin user that has full access to the ArgoCD instance. By default, Argo will create a password for the admin user. See the ArgoCD documentation for additional details on managing users. Customizing the Helm Chart \u00b6 You can customize the Helm chart that deploys ArgoCD via the following configuration: argocd_helm_config = { name = \"argo-cd\" chart = \"argo-cd\" repository = \"https://argoproj.github.io/argo-helm\" version = \"<chart_version>\" namespace = \"argocd\" timeout = \"1200\" create_namespace = true values = [templatefile(\"${path.module}/argocd-values.yaml\", {})] } Bootstrapping \u00b6 The framework provides an approach to bootstrapping workloads and/or additional add-ons by leveraging the ArgoCD App of Apps pattern. The following code example demonstrates how you can supply information for a repository in order to bootstrap multiple workloads in a new EKS cluster. The example leverages a sample App of Apps repository . argocd_applications = { addons = { path = \"chart\" repo_url = \"https://github.com/aws-samples/eks-blueprints-add-ons.git\" add_on_application = true # Indicates the root add-on application. } } Add-ons \u00b6 A common operational pattern for EKS customers is to leverage Infrastructure as Code to provision EKS clusters (in addition to other AWS resources), and ArgoCD to manage cluster add-ons. This can present a challenge when add-ons managed by ArgoCD depend on AWS resource values which are created via Terraform execution (such as an IAM ARN for an add-on that leverages IRSA), to function properly. The framework provides an approach to bridging the gap between Terraform and ArgoCD by leveraging the ArgoCD App of Apps pattern. To indicate that ArgoCD should responsible for managing cluster add-ons (applying add-on Helm charts to a cluster), you can set the argocd_manage_add_ons property to true. When this flag is set, the framework will still provision all AWS resources necessary to support add-on functionality, but it will not apply Helm charts directly via the Terraform Helm provider. Next, identify which ArgoCD Application will serve as the add-on configuration repository by setting the add_on_application flag to true. When this flag is set, the framework will aggregate AWS resource values that are needed for each add-on into an object. It will then pass that object to ArgoCD via the values map of the Application resource. See here for the values object that gets passed to the ArgoCD add-ons Application. Sample configuration can be found below: enable_argocd = true argocd_manage_add_ons = true argocd_applications = { addons = { path = \"chart\" repo_url = \"https://github.com/aws-samples/eks-blueprints-add-ons.git\" add_on_application = true # Indicates the root add-on application. } } Private Repositories \u00b6 In order to leverage ArgoCD with private Git repositories, you must supply a private SSH key to Argo. The framework provides support for doing so via an integration with AWS Secrets Manager. To leverage private repositories, do the following: Create a new secret in AWS Secrets Manager for your desired region. The value for the secret should be a private SSH key for your Git provider. Set the ssh_key_secret_name in each Application's configuration as the name of the secret. Internally, the framework will create a Kubernetes Secret, which ArgoCD will leverage when making requests to your Git provider. See the example configuration below. enable_argocd = true argocd_manage_add_ons = true argocd_applications = { addons = { path = \"chart\" repo_url = \"git@github.com:aws-samples/eks-blueprints-add-ons.git\" project = \"default\" add_on_application = true # Indicates the root add-on application. ssh_key_secret_name = \"github-ssh-key\" # Needed for private repos insecure = false # Set to true to disable the server's certificate verification } } Complete Example \u00b6 The following demonstrates a complete example for configuring ArgoCD. enable_argocd = true argocd_manage_add_ons = true argocd_helm_config = { name = \"argo-cd\" chart = \"argo-cd\" repository = \"https://argoproj.github.io/argo-helm\" version = \"3.29.5\" namespace = \"argocd\" timeout = \"1200\" create_namespace = true values = [templatefile(\"${path.module}/argocd-values.yaml\", {})] } argocd_applications = { workloads = { path = \"envs/dev\" repo_url = \"https://github.com/aws-samples/eks-blueprints-workloads.git\" values = {} type = \"helm\" # Optional, defaults to helm. } kustomize_apps = { /* This points to a single application with no overlays, but it could easily point to a a specific overlay for an environment like \"dev\", and/or utilize the ArgoCD app of apps model to install many additional ArgoCD apps. */ path = \"argocd-example-apps/kustomize-guestbook/\" repo_url = \"https://github.com/argoproj/argocd-example-apps.git\" type = \"kustomize\" } addons = { path = \"chart\" repo_url = \"git@github.com:aws-samples/eks-blueprints-add-ons.git\" add_on_application = true # Indicates the root add-on application. # If provided, the type must be set to \"helm\" for the root add-on application. ssh_key_secret_name = \"github-ssh-key\" # Needed for private repos values = {} type = \"helm\" # Optional, defaults to helm. } }","title":"ArgoCD"},{"location":"add-ons/argocd/#argocd","text":"ArgoCD Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. Application definitions, configurations, and environments should be declarative and version controlled. Application deployment and lifecycle management should be automated, auditable, and easy to understand.","title":"ArgoCD"},{"location":"add-ons/argocd/#usage","text":"ArgoCD can be deployed by enabling the add-on via the following. enable_argocd = true","title":"Usage"},{"location":"add-ons/argocd/#admin-password","text":"ArgoCD has a built-in admin user that has full access to the ArgoCD instance. By default, Argo will create a password for the admin user. See the ArgoCD documentation for additional details on managing users.","title":"Admin Password"},{"location":"add-ons/argocd/#customizing-the-helm-chart","text":"You can customize the Helm chart that deploys ArgoCD via the following configuration: argocd_helm_config = { name = \"argo-cd\" chart = \"argo-cd\" repository = \"https://argoproj.github.io/argo-helm\" version = \"<chart_version>\" namespace = \"argocd\" timeout = \"1200\" create_namespace = true values = [templatefile(\"${path.module}/argocd-values.yaml\", {})] }","title":"Customizing the Helm Chart"},{"location":"add-ons/argocd/#bootstrapping","text":"The framework provides an approach to bootstrapping workloads and/or additional add-ons by leveraging the ArgoCD App of Apps pattern. The following code example demonstrates how you can supply information for a repository in order to bootstrap multiple workloads in a new EKS cluster. The example leverages a sample App of Apps repository . argocd_applications = { addons = { path = \"chart\" repo_url = \"https://github.com/aws-samples/eks-blueprints-add-ons.git\" add_on_application = true # Indicates the root add-on application. } }","title":"Bootstrapping"},{"location":"add-ons/argocd/#add-ons","text":"A common operational pattern for EKS customers is to leverage Infrastructure as Code to provision EKS clusters (in addition to other AWS resources), and ArgoCD to manage cluster add-ons. This can present a challenge when add-ons managed by ArgoCD depend on AWS resource values which are created via Terraform execution (such as an IAM ARN for an add-on that leverages IRSA), to function properly. The framework provides an approach to bridging the gap between Terraform and ArgoCD by leveraging the ArgoCD App of Apps pattern. To indicate that ArgoCD should responsible for managing cluster add-ons (applying add-on Helm charts to a cluster), you can set the argocd_manage_add_ons property to true. When this flag is set, the framework will still provision all AWS resources necessary to support add-on functionality, but it will not apply Helm charts directly via the Terraform Helm provider. Next, identify which ArgoCD Application will serve as the add-on configuration repository by setting the add_on_application flag to true. When this flag is set, the framework will aggregate AWS resource values that are needed for each add-on into an object. It will then pass that object to ArgoCD via the values map of the Application resource. See here for the values object that gets passed to the ArgoCD add-ons Application. Sample configuration can be found below: enable_argocd = true argocd_manage_add_ons = true argocd_applications = { addons = { path = \"chart\" repo_url = \"https://github.com/aws-samples/eks-blueprints-add-ons.git\" add_on_application = true # Indicates the root add-on application. } }","title":"Add-ons"},{"location":"add-ons/argocd/#private-repositories","text":"In order to leverage ArgoCD with private Git repositories, you must supply a private SSH key to Argo. The framework provides support for doing so via an integration with AWS Secrets Manager. To leverage private repositories, do the following: Create a new secret in AWS Secrets Manager for your desired region. The value for the secret should be a private SSH key for your Git provider. Set the ssh_key_secret_name in each Application's configuration as the name of the secret. Internally, the framework will create a Kubernetes Secret, which ArgoCD will leverage when making requests to your Git provider. See the example configuration below. enable_argocd = true argocd_manage_add_ons = true argocd_applications = { addons = { path = \"chart\" repo_url = \"git@github.com:aws-samples/eks-blueprints-add-ons.git\" project = \"default\" add_on_application = true # Indicates the root add-on application. ssh_key_secret_name = \"github-ssh-key\" # Needed for private repos insecure = false # Set to true to disable the server's certificate verification } }","title":"Private Repositories"},{"location":"add-ons/argocd/#complete-example","text":"The following demonstrates a complete example for configuring ArgoCD. enable_argocd = true argocd_manage_add_ons = true argocd_helm_config = { name = \"argo-cd\" chart = \"argo-cd\" repository = \"https://argoproj.github.io/argo-helm\" version = \"3.29.5\" namespace = \"argocd\" timeout = \"1200\" create_namespace = true values = [templatefile(\"${path.module}/argocd-values.yaml\", {})] } argocd_applications = { workloads = { path = \"envs/dev\" repo_url = \"https://github.com/aws-samples/eks-blueprints-workloads.git\" values = {} type = \"helm\" # Optional, defaults to helm. } kustomize_apps = { /* This points to a single application with no overlays, but it could easily point to a a specific overlay for an environment like \"dev\", and/or utilize the ArgoCD app of apps model to install many additional ArgoCD apps. */ path = \"argocd-example-apps/kustomize-guestbook/\" repo_url = \"https://github.com/argoproj/argocd-example-apps.git\" type = \"kustomize\" } addons = { path = \"chart\" repo_url = \"git@github.com:aws-samples/eks-blueprints-add-ons.git\" add_on_application = true # Indicates the root add-on application. # If provided, the type must be set to \"helm\" for the root add-on application. ssh_key_secret_name = \"github-ssh-key\" # Needed for private repos values = {} type = \"helm\" # Optional, defaults to helm. } }","title":"Complete Example"},{"location":"add-ons/aws-cloudwatch-metrics/","text":"AWS CloudWatch Metrics \u00b6 Use CloudWatch Container Insights to collect, aggregate, and summarize metrics and logs from your containerized applications and microservices. CloudWatch automatically collects metrics for many resources, such as CPU, memory, disk, and network. Container Insights also provides diagnostic information, such as container restart failures, to help you isolate issues and resolve them quickly. You can also set CloudWatch alarms on metrics that Container Insights collects. Container Insights collects data as performance log events using embedded metric format. These performance log events are entries that use a structured JSON schema that enables high-cardinality data to be ingested and stored at scale. From this data, CloudWatch creates aggregated metrics at the cluster, node, pod, task, and service level as CloudWatch metrics. The metrics that Container Insights collects are available in CloudWatch automatic dashboards, and also viewable in the Metrics section of the CloudWatch console. Usage \u00b6 aws-cloudwatch-metrics can be deployed by enabling the add-on via the following. enable_aws_cloudwatch_metrics = true You can optionally customize the Helm chart that deploys aws_cloudwatch_metrics via the following configuration. enable_aws_cloudwatch_metrics = true aws_cloudwatch_metrics_irsa_policies = [\"IAM Policies\"] aws_cloudwatch_metrics_helm_config = { name = \"aws-cloudwatch-metrics\" chart = \"aws-cloudwatch-metrics\" repository = \"https://aws.github.io/eks-charts\" version = \"0.0.7\" namespace = \"amazon-cloudwatch\" values = [templatefile(\"${path.module}/values.yaml\", { eks_cluster_id = var.addon_context.eks_cluster_id })] } GitOps Configuration \u00b6 The following properties are made available for use when managing the add-on via GitOps. awsCloudWatchMetrics = { enable = true }","title":"AWS CloudWatch Metrics"},{"location":"add-ons/aws-cloudwatch-metrics/#aws-cloudwatch-metrics","text":"Use CloudWatch Container Insights to collect, aggregate, and summarize metrics and logs from your containerized applications and microservices. CloudWatch automatically collects metrics for many resources, such as CPU, memory, disk, and network. Container Insights also provides diagnostic information, such as container restart failures, to help you isolate issues and resolve them quickly. You can also set CloudWatch alarms on metrics that Container Insights collects. Container Insights collects data as performance log events using embedded metric format. These performance log events are entries that use a structured JSON schema that enables high-cardinality data to be ingested and stored at scale. From this data, CloudWatch creates aggregated metrics at the cluster, node, pod, task, and service level as CloudWatch metrics. The metrics that Container Insights collects are available in CloudWatch automatic dashboards, and also viewable in the Metrics section of the CloudWatch console.","title":"AWS CloudWatch Metrics"},{"location":"add-ons/aws-cloudwatch-metrics/#usage","text":"aws-cloudwatch-metrics can be deployed by enabling the add-on via the following. enable_aws_cloudwatch_metrics = true You can optionally customize the Helm chart that deploys aws_cloudwatch_metrics via the following configuration. enable_aws_cloudwatch_metrics = true aws_cloudwatch_metrics_irsa_policies = [\"IAM Policies\"] aws_cloudwatch_metrics_helm_config = { name = \"aws-cloudwatch-metrics\" chart = \"aws-cloudwatch-metrics\" repository = \"https://aws.github.io/eks-charts\" version = \"0.0.7\" namespace = \"amazon-cloudwatch\" values = [templatefile(\"${path.module}/values.yaml\", { eks_cluster_id = var.addon_context.eks_cluster_id })] }","title":"Usage"},{"location":"add-ons/aws-cloudwatch-metrics/#gitops-configuration","text":"The following properties are made available for use when managing the add-on via GitOps. awsCloudWatchMetrics = { enable = true }","title":"GitOps Configuration"},{"location":"add-ons/aws-efs-csi-driver/","text":"AWS EFS CSI Driver \u00b6 This add-on deploys the AWS EFS CSI driver into an EKS cluster. Usage \u00b6 The AWS EFS CSI driver can be deployed by enabling the add-on via the following. Check out the full example to deploy an EKS Cluster with EFS backing the dynamic provisioning of persistent volumes. enable_aws_efs_csi_driver = true Once deployed, you will be able to see a number of supporting resources in the kube-system namespace. $ kubectl get deployment efs-csi-controller -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE efs-csi-controller 2 /2 2 2 4m29s $ kubectl get daemonset efs-csi-node -n kube-system NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE efs-csi-node 3 3 3 3 3 beta.kubernetes.io/os = linux 4m32s You can optionally customize the Helm chart that deploys the driver via the following configuration. enable_aws_efs_csi_driver = true # Optional aws_efs_csi_driver_helm_config aws_efs_csi_driver_helm_config = { repository = \"https://kubernetes-sigs.github.io/aws-efs-csi-driver/\" version = \"2.2.3\" } aws_efs_csi_driver_irsa_policies = [\"<ADDITIONAL_IAM_POLICY_ARN>\"] GitOps Configuration \u00b6 ArgoCD with App of Apps GitOps enabled for this Add-on by enabling the following variable argocd_manage_add_ons = true The following is configured to ArgoCD App of Apps for this Add-on. argocd_gitops_config = { enable = true serviceAccountName = local.service_account_name }","title":"AWS EFS CSI Driver"},{"location":"add-ons/aws-efs-csi-driver/#aws-efs-csi-driver","text":"This add-on deploys the AWS EFS CSI driver into an EKS cluster.","title":"AWS EFS CSI Driver"},{"location":"add-ons/aws-efs-csi-driver/#usage","text":"The AWS EFS CSI driver can be deployed by enabling the add-on via the following. Check out the full example to deploy an EKS Cluster with EFS backing the dynamic provisioning of persistent volumes. enable_aws_efs_csi_driver = true Once deployed, you will be able to see a number of supporting resources in the kube-system namespace. $ kubectl get deployment efs-csi-controller -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE efs-csi-controller 2 /2 2 2 4m29s $ kubectl get daemonset efs-csi-node -n kube-system NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE efs-csi-node 3 3 3 3 3 beta.kubernetes.io/os = linux 4m32s You can optionally customize the Helm chart that deploys the driver via the following configuration. enable_aws_efs_csi_driver = true # Optional aws_efs_csi_driver_helm_config aws_efs_csi_driver_helm_config = { repository = \"https://kubernetes-sigs.github.io/aws-efs-csi-driver/\" version = \"2.2.3\" } aws_efs_csi_driver_irsa_policies = [\"<ADDITIONAL_IAM_POLICY_ARN>\"]","title":"Usage"},{"location":"add-ons/aws-efs-csi-driver/#gitops-configuration","text":"ArgoCD with App of Apps GitOps enabled for this Add-on by enabling the following variable argocd_manage_add_ons = true The following is configured to ArgoCD App of Apps for this Add-on. argocd_gitops_config = { enable = true serviceAccountName = local.service_account_name }","title":"GitOps Configuration"},{"location":"add-ons/aws-for-fluent-bit/","text":"AWS for Fluent Bit \u00b6 Fluent Bit is an open source Log Processor and Forwarder which allows you to collect any data like metrics and logs from different sources, enrich them with filters and send them to multiple destinations. AWS for Fluent Bit \u00b6 AWS provides a Fluent Bit image with plugins for both CloudWatch Logs and Kinesis Data Firehose. The AWS for Fluent Bit image is available on the Amazon ECR Public Gallery. For more details, see aws-for-fluent-bit on the Amazon ECR Public Gallery. Usage \u00b6 aws-for-fluent-bit can be deployed by enabling the add-on via the following. This add-on is configured to stream the worker node logs to CloudWatch Logs by default. It can further be configured to stream the logs to additional destinations like Kinesis Data Firehose, Kinesis Data Streams and Amazon OpenSearch Service by passing the custom values.yaml . See this Helm Chart for more details. enable_aws_for_fluentbit = true You can optionally customize the Helm chart that deploys aws_for_fluentbit via the following configuration. enable_aws_for_fluentbit = true aws_for_fluentbit_irsa_policies = [\"IAM Policies\"] # Add list of additional policies to IRSA to enable access to Kinesis, OpenSearch etc. aws_for_fluentbit_cw_log_group_retention = 90 aws_for_fluentbit_helm_config = { name = \"aws-for-fluent-bit\" chart = \"aws-for-fluent-bit\" repository = \"https://aws.github.io/eks-charts\" version = \"0.1.0\" namespace = \"logging\" aws_for_fluent_bit_cw_log_group = \"/${local.cluster_id}/worker-fluentbit-logs\" # Optional create_namespace = true values = [templatefile(\"${path.module}/values.yaml\", { region = data.aws_region.current.name, aws_for_fluent_bit_cw_log_group = \"/${local.cluster_id}/worker-fluentbit-logs\" })] set = [ { name = \"nodeSelector.kubernetes\\\\.io/os\" value = \"linux\" } ] } GitOps Configuration \u00b6 The following properties are made available for use when managing the add-on via GitOps. awsForFluentBit = { enable = true logGroupName = \"<log_group_name>\" } Externally-Created CloudWatch Log Group(s) \u00b6 If the CloudWatch log group FluentBit puts logs to is required to be encrypted by an existing KMS customer-managed key, then the CloudWatch log group needs to be created external to the kubernetes-addons module and passed in. Creating the CloudWatch log group externally is also useful if FluentBit is putting logs to multiple log groups because all the log groups can be created in the same code file. To do this, set the create log group flag to false and supply the previously-created log group name. aws_for_fluentbit_create_cw_log_group = false aws_for_fluentbit_cw_log_group_name = aws_cloudwatch_log_group.application.name","title":"AWS for Fluent Bit"},{"location":"add-ons/aws-for-fluent-bit/#aws-for-fluent-bit","text":"Fluent Bit is an open source Log Processor and Forwarder which allows you to collect any data like metrics and logs from different sources, enrich them with filters and send them to multiple destinations.","title":"AWS for Fluent Bit"},{"location":"add-ons/aws-for-fluent-bit/#aws-for-fluent-bit_1","text":"AWS provides a Fluent Bit image with plugins for both CloudWatch Logs and Kinesis Data Firehose. The AWS for Fluent Bit image is available on the Amazon ECR Public Gallery. For more details, see aws-for-fluent-bit on the Amazon ECR Public Gallery.","title":"AWS for Fluent Bit"},{"location":"add-ons/aws-for-fluent-bit/#usage","text":"aws-for-fluent-bit can be deployed by enabling the add-on via the following. This add-on is configured to stream the worker node logs to CloudWatch Logs by default. It can further be configured to stream the logs to additional destinations like Kinesis Data Firehose, Kinesis Data Streams and Amazon OpenSearch Service by passing the custom values.yaml . See this Helm Chart for more details. enable_aws_for_fluentbit = true You can optionally customize the Helm chart that deploys aws_for_fluentbit via the following configuration. enable_aws_for_fluentbit = true aws_for_fluentbit_irsa_policies = [\"IAM Policies\"] # Add list of additional policies to IRSA to enable access to Kinesis, OpenSearch etc. aws_for_fluentbit_cw_log_group_retention = 90 aws_for_fluentbit_helm_config = { name = \"aws-for-fluent-bit\" chart = \"aws-for-fluent-bit\" repository = \"https://aws.github.io/eks-charts\" version = \"0.1.0\" namespace = \"logging\" aws_for_fluent_bit_cw_log_group = \"/${local.cluster_id}/worker-fluentbit-logs\" # Optional create_namespace = true values = [templatefile(\"${path.module}/values.yaml\", { region = data.aws_region.current.name, aws_for_fluent_bit_cw_log_group = \"/${local.cluster_id}/worker-fluentbit-logs\" })] set = [ { name = \"nodeSelector.kubernetes\\\\.io/os\" value = \"linux\" } ] }","title":"Usage"},{"location":"add-ons/aws-for-fluent-bit/#gitops-configuration","text":"The following properties are made available for use when managing the add-on via GitOps. awsForFluentBit = { enable = true logGroupName = \"<log_group_name>\" }","title":"GitOps Configuration"},{"location":"add-ons/aws-for-fluent-bit/#externally-created-cloudwatch-log-groups","text":"If the CloudWatch log group FluentBit puts logs to is required to be encrypted by an existing KMS customer-managed key, then the CloudWatch log group needs to be created external to the kubernetes-addons module and passed in. Creating the CloudWatch log group externally is also useful if FluentBit is putting logs to multiple log groups because all the log groups can be created in the same code file. To do this, set the create log group flag to false and supply the previously-created log group name. aws_for_fluentbit_create_cw_log_group = false aws_for_fluentbit_cw_log_group_name = aws_cloudwatch_log_group.application.name","title":"Externally-Created CloudWatch Log Group(s)"},{"location":"add-ons/aws-fsx-csi-driver/","text":"Amazon FSx for Lustre CSI Driver \u00b6 Fully managed shared storage built on the world's most popular high-performance file system. This add-on deploys the Amazon FSx for Lustre CSI Driver into an EKS cluster. Checkout the examples of using FSx for Lustre with EMR on EKS Spark Jobs. Usage \u00b6 The Amazon FSx for Lustre CSI Driver can be deployed by enabling the add-on via the following. enable_aws_fsx_csi_driver = true You can optionally customize the Helm chart that deploys enable_aws_fsx_csi_driver via the following configuration. enable_aws_fsx_csi_driver = true aws_fsx_csi_driver_helm_config = { name = \"aws-fsx-csi-driver\" chart = \"aws-fsx-csi-driver\" repository = \"https://kubernetes-sigs.github.io/aws-fsx-csi-driver/\" version = \"1.4.2\" namespace = \"kube-system\" values = [templatefile(\"${path.module}/aws-fsx-csi-driver-values.yaml\", {})] # Create this `aws-fsx-csi-driver-values.yaml` file with your own custom values } aws_fsx_csi_driver_irsa_policies = [\"<ADDITIONAL_IAM_POLICY_ARN>\"] Once deployed, you will be able to see a number of supporting resources in the kube-system namespace. $ kubectl get deployment fsx-csi-controller -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE fsx-csi-controller 2 /2 2 2 4m29s $ kubectl get daemonset fsx-csi-node -n kube-system NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE fsx-csi-node 3 3 3 3 3 kubernetes.io/os = linux 4m32s GitOps Configuration \u00b6 ArgoCD with App of Apps GitOps enabled for this Add-on by enabling the following variable argocd_manage_add_ons = true The following is configured to ArgoCD App of Apps for this Add-on. argocd_gitops_config = { enable = true serviceAccountName = local.service_account_name }","title":"Amazon FSx for Lustre CSI Driver"},{"location":"add-ons/aws-fsx-csi-driver/#amazon-fsx-for-lustre-csi-driver","text":"Fully managed shared storage built on the world's most popular high-performance file system. This add-on deploys the Amazon FSx for Lustre CSI Driver into an EKS cluster. Checkout the examples of using FSx for Lustre with EMR on EKS Spark Jobs.","title":"Amazon FSx for Lustre CSI Driver"},{"location":"add-ons/aws-fsx-csi-driver/#usage","text":"The Amazon FSx for Lustre CSI Driver can be deployed by enabling the add-on via the following. enable_aws_fsx_csi_driver = true You can optionally customize the Helm chart that deploys enable_aws_fsx_csi_driver via the following configuration. enable_aws_fsx_csi_driver = true aws_fsx_csi_driver_helm_config = { name = \"aws-fsx-csi-driver\" chart = \"aws-fsx-csi-driver\" repository = \"https://kubernetes-sigs.github.io/aws-fsx-csi-driver/\" version = \"1.4.2\" namespace = \"kube-system\" values = [templatefile(\"${path.module}/aws-fsx-csi-driver-values.yaml\", {})] # Create this `aws-fsx-csi-driver-values.yaml` file with your own custom values } aws_fsx_csi_driver_irsa_policies = [\"<ADDITIONAL_IAM_POLICY_ARN>\"] Once deployed, you will be able to see a number of supporting resources in the kube-system namespace. $ kubectl get deployment fsx-csi-controller -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE fsx-csi-controller 2 /2 2 2 4m29s $ kubectl get daemonset fsx-csi-node -n kube-system NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE fsx-csi-node 3 3 3 3 3 kubernetes.io/os = linux 4m32s","title":"Usage"},{"location":"add-ons/aws-fsx-csi-driver/#gitops-configuration","text":"ArgoCD with App of Apps GitOps enabled for this Add-on by enabling the following variable argocd_manage_add_ons = true The following is configured to ArgoCD App of Apps for this Add-on. argocd_gitops_config = { enable = true serviceAccountName = local.service_account_name }","title":"GitOps Configuration"},{"location":"add-ons/aws-load-balancer-controller/","text":"AWS Load Balancer Controller \u00b6 The AWS Load Balancer Controller manages AWS Elastic Load Balancers for a Kubernetes cluster. The controller provisions the following resources: An AWS Application Load Balancer (ALB) when you create a Kubernetes Ingress. An AWS Network Load Balancer (NLB) when you create a Kubernetes Service of type LoadBalancer. For more information about AWS Load Balancer Controller please see the official documentation . Usage \u00b6 enable_aws_load_balancer_controller = true You can optionally customize the Helm chart that deploys aws-lb-ingress-controller via the following configuration. enable_aws_load_balancer_controller = true # Optional aws_load_balancer_controller_helm_config = { name = \"aws-load-balancer-controller\" chart = \"aws-load-balancer-controller\" repository = \"https://aws.github.io/eks-charts\" version = \"1.3.1\" namespace = \"kube-system\" values = [templatefile(\"${path.module}/values.yaml\", {})] } To validate that controller is running, ensure that controller deployment is in RUNNING state: # Assuming controller is installed in kube-system namespace $ kubectl get deployments -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE aws-load-balancer-controller 2 /2 2 2 3m58s AWS Service annotations for LB Ingress Controller \u00b6 Here is the link to get the AWS ELB service annotations for LB Ingress controller. GitOps Configuration \u00b6 The following properties are made available for use when managing the add-on via GitOps. awsLoadBalancerController = { enable = true serviceAccountName = \"<service_account_name>\" }","title":"AWS Load Balancer Controller"},{"location":"add-ons/aws-load-balancer-controller/#aws-load-balancer-controller","text":"The AWS Load Balancer Controller manages AWS Elastic Load Balancers for a Kubernetes cluster. The controller provisions the following resources: An AWS Application Load Balancer (ALB) when you create a Kubernetes Ingress. An AWS Network Load Balancer (NLB) when you create a Kubernetes Service of type LoadBalancer. For more information about AWS Load Balancer Controller please see the official documentation .","title":"AWS Load Balancer Controller"},{"location":"add-ons/aws-load-balancer-controller/#usage","text":"enable_aws_load_balancer_controller = true You can optionally customize the Helm chart that deploys aws-lb-ingress-controller via the following configuration. enable_aws_load_balancer_controller = true # Optional aws_load_balancer_controller_helm_config = { name = \"aws-load-balancer-controller\" chart = \"aws-load-balancer-controller\" repository = \"https://aws.github.io/eks-charts\" version = \"1.3.1\" namespace = \"kube-system\" values = [templatefile(\"${path.module}/values.yaml\", {})] } To validate that controller is running, ensure that controller deployment is in RUNNING state: # Assuming controller is installed in kube-system namespace $ kubectl get deployments -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE aws-load-balancer-controller 2 /2 2 2 3m58s","title":"Usage"},{"location":"add-ons/aws-load-balancer-controller/#aws-service-annotations-for-lb-ingress-controller","text":"Here is the link to get the AWS ELB service annotations for LB Ingress controller.","title":"AWS Service annotations for LB Ingress Controller"},{"location":"add-ons/aws-load-balancer-controller/#gitops-configuration","text":"The following properties are made available for use when managing the add-on via GitOps. awsLoadBalancerController = { enable = true serviceAccountName = \"<service_account_name>\" }","title":"GitOps Configuration"},{"location":"add-ons/aws-node-termination-handler/","text":"AWS Node Termination Handler \u00b6 This project ensures that the Kubernetes control plane responds appropriately to events that can cause your EC2 instance to become unavailable, such as EC2 maintenance events, EC2 Spot interruptions, ASG Scale-In, ASG AZ Rebalance, and EC2 Instance Termination via the API or Console. If not handled, your application code may not stop gracefully, take longer to recover full availability, or accidentally schedule work to nodes that are going down. For more information see README.md . The aws-node-termination-handler (NTH) can operate in two different modes: Instance Metadata Service (IMDS) or the Queue Processor. In the EKS Blueprints, we provision the NTH in Queue Processor mode. This means that NTH will monitor an SQS queue of events from Amazon EventBridge for ASG lifecycle events, EC2 status change events, Spot Interruption Termination Notice events, and Spot Rebalance Recommendation events. When NTH detects an instance is going down, NTH uses the Kubernetes API to cordon the node to ensure no new work is scheduled there, then drain it, removing any existing work. The NTH will be deployed in the kube-system namespace. AWS resources required as part of the setup of NTH will be provisioned for you. These include: Node group ASG tagged with key=aws-node-termination-handler/managed AutoScaling Group Termination Lifecycle Hook Amazon Simple Queue Service (SQS) Queue Amazon EventBridge Rule IAM Role for the aws-node-termination-handler Queue Processing Pods Usage \u00b6 enable_aws_node_termination_handler = true You can optionally customize the Helm chart that deploys aws-node-termination-handler via the following configuration. enable_aws_node_termination_handler = true aws_node_termination_handler_helm_config = { name = \"aws-node-termination-handler\" chart = \"aws-node-termination-handler\" repository = \"https://aws.github.io/eks-charts\" version = \"0.16.0\" timeout = \"1200\" } To validate that controller is running, ensure that controller deployment is in RUNNING state: # Assuming controller is installed in kube-system namespace $ kubectl get deployments -n kube-system aws-node-termination-handler 1 /1 1 1 5d9h","title":"AWS Node Termination Handler"},{"location":"add-ons/aws-node-termination-handler/#aws-node-termination-handler","text":"This project ensures that the Kubernetes control plane responds appropriately to events that can cause your EC2 instance to become unavailable, such as EC2 maintenance events, EC2 Spot interruptions, ASG Scale-In, ASG AZ Rebalance, and EC2 Instance Termination via the API or Console. If not handled, your application code may not stop gracefully, take longer to recover full availability, or accidentally schedule work to nodes that are going down. For more information see README.md . The aws-node-termination-handler (NTH) can operate in two different modes: Instance Metadata Service (IMDS) or the Queue Processor. In the EKS Blueprints, we provision the NTH in Queue Processor mode. This means that NTH will monitor an SQS queue of events from Amazon EventBridge for ASG lifecycle events, EC2 status change events, Spot Interruption Termination Notice events, and Spot Rebalance Recommendation events. When NTH detects an instance is going down, NTH uses the Kubernetes API to cordon the node to ensure no new work is scheduled there, then drain it, removing any existing work. The NTH will be deployed in the kube-system namespace. AWS resources required as part of the setup of NTH will be provisioned for you. These include: Node group ASG tagged with key=aws-node-termination-handler/managed AutoScaling Group Termination Lifecycle Hook Amazon Simple Queue Service (SQS) Queue Amazon EventBridge Rule IAM Role for the aws-node-termination-handler Queue Processing Pods","title":"AWS Node Termination Handler"},{"location":"add-ons/aws-node-termination-handler/#usage","text":"enable_aws_node_termination_handler = true You can optionally customize the Helm chart that deploys aws-node-termination-handler via the following configuration. enable_aws_node_termination_handler = true aws_node_termination_handler_helm_config = { name = \"aws-node-termination-handler\" chart = \"aws-node-termination-handler\" repository = \"https://aws.github.io/eks-charts\" version = \"0.16.0\" timeout = \"1200\" } To validate that controller is running, ensure that controller deployment is in RUNNING state: # Assuming controller is installed in kube-system namespace $ kubectl get deployments -n kube-system aws-node-termination-handler 1 /1 1 1 5d9h","title":"Usage"},{"location":"add-ons/aws-privateca-issuer/","text":"aws-privateca-issuer \u00b6 AWS ACM Private CA is a module of the AWS Certificate Manager that can setup and manage private CAs. cert-manager is a Kubernetes add-on to automate the management and issuance of TLS certificates from various issuing sources. It will ensure certificates are valid and up to date periodically, and attempt to renew certificates at an appropriate time before expiry. This module aws-pca-issuer is a addon for cert-manager that issues certificates using AWS ACM PCA. See the aws-privateca-issuer documentation . Usage \u00b6 aws_privateca_issuer can be deployed by enabling the add-on via the following. enable_cert_manager = true enable_aws_privateca_issuer = true Create AWSPCAClusterIssuer custom resource definition (CRD). It is a Kubernetes resources that represent certificate authorities (CAs) from AWS ACM and are able to generate signed certificates by honoring certificate signing requests. For more details on external Issuer types, please check aws-privateca-issuer resource \"kubernetes_manifest\" \"cluster-pca-issuer\" { manifest = { apiVersion = \"awspca.cert-manager.io/v1beta1\" kind = \"AWSPCAClusterIssuer\" metadata = { name = \"logical.name.of.this.issuer\" } spec = { arn = \"ARN for AWS PCA\" region: \"data.aws_region.current.id OR AWS region of the AWS PCA\" } } } Create Certificate CRD. Certificates define a desired X.509 certificate which will be renewed and kept up to date. For more details on how to specify and request Certificate resources, please check Certificate Resources guide . A Certificate is a namespaced resource that references AWSPCAClusterIssuer (created in above step) that determine what will be honoring the certificate request. resource \"kubernetes_manifest\" \"example_pca_certificate\" { manifest = { apiVersion = \"cert-manager.io/v1\" kind = \"Certificate\" metadata = { name = \"name of the certificate\" namespace = \"default or any namespace\" } spec = { commonName = \"common name for your certificate\" duration = \"duration\" issuerRef = { group = \"awspca.cert-manager.io\" kind = \"AWSPCAClusterIssuer\" name: \"name of AWSPCAClusterIssuer created above\" } renewBefore = \"360h0m0s\" secretName = \"name of the secret where certificate will be mounted\" usages = [ \"server auth\", \"client auth\" ] privateKey = { algorithm: \"RSA\" size: 2048 } } } } When a Certificate is created, a corresponding CertificateRequest resource is created by cert-manager containing the encoded X.509 certificate request, Issuer reference, and other options based upon the specification of the Certificate resource. This Certificate CRD will tell cert-manager to attempt to use the Issuer (as AWS ACM) to obtain a certificate key pair for the specified domains. If successful, the resulting TLS key and certificate will be stored in a kubernetes secret named , with keys of tls.key, and tls.crt respectively. This secret will live in the same namespace as the Certificate resource. Now, you may run kubectl get Certificate to view the status of Certificate Request from AWS PCA. NAME READY SECRET AGE example True aws001-preprod-dev-eks-clusterissuer 3h35m If the status is True , that means, the tls.crt , tls.key and ca.crt will all be available in Kubernetes Secret aws001-preprod-dev-eks-clusterissuer Name: aws001-preprod-dev-eks-clusterissuer Namespace: default Labels: <none> Annotations: cert-manager.io/alt-names: cert-manager.io/certificate-name: example cert-manager.io/common-name: example.com cert-manager.io/ip-sans: cert-manager.io/issuer-group: awspca.cert-manager.io cert-manager.io/issuer-kind: AWSPCAClusterIssuer cert-manager.io/issuer-name: aws001-preprod-dev-eks cert-manager.io/uri-sans: Type: kubernetes.io/tls Data ==== ca.crt: 1785 bytes tls.crt: 1517 bytes tls.key: 1679 bytes","title":"aws-privateca-issuer"},{"location":"add-ons/aws-privateca-issuer/#aws-privateca-issuer","text":"AWS ACM Private CA is a module of the AWS Certificate Manager that can setup and manage private CAs. cert-manager is a Kubernetes add-on to automate the management and issuance of TLS certificates from various issuing sources. It will ensure certificates are valid and up to date periodically, and attempt to renew certificates at an appropriate time before expiry. This module aws-pca-issuer is a addon for cert-manager that issues certificates using AWS ACM PCA. See the aws-privateca-issuer documentation .","title":"aws-privateca-issuer"},{"location":"add-ons/aws-privateca-issuer/#usage","text":"aws_privateca_issuer can be deployed by enabling the add-on via the following. enable_cert_manager = true enable_aws_privateca_issuer = true Create AWSPCAClusterIssuer custom resource definition (CRD). It is a Kubernetes resources that represent certificate authorities (CAs) from AWS ACM and are able to generate signed certificates by honoring certificate signing requests. For more details on external Issuer types, please check aws-privateca-issuer resource \"kubernetes_manifest\" \"cluster-pca-issuer\" { manifest = { apiVersion = \"awspca.cert-manager.io/v1beta1\" kind = \"AWSPCAClusterIssuer\" metadata = { name = \"logical.name.of.this.issuer\" } spec = { arn = \"ARN for AWS PCA\" region: \"data.aws_region.current.id OR AWS region of the AWS PCA\" } } } Create Certificate CRD. Certificates define a desired X.509 certificate which will be renewed and kept up to date. For more details on how to specify and request Certificate resources, please check Certificate Resources guide . A Certificate is a namespaced resource that references AWSPCAClusterIssuer (created in above step) that determine what will be honoring the certificate request. resource \"kubernetes_manifest\" \"example_pca_certificate\" { manifest = { apiVersion = \"cert-manager.io/v1\" kind = \"Certificate\" metadata = { name = \"name of the certificate\" namespace = \"default or any namespace\" } spec = { commonName = \"common name for your certificate\" duration = \"duration\" issuerRef = { group = \"awspca.cert-manager.io\" kind = \"AWSPCAClusterIssuer\" name: \"name of AWSPCAClusterIssuer created above\" } renewBefore = \"360h0m0s\" secretName = \"name of the secret where certificate will be mounted\" usages = [ \"server auth\", \"client auth\" ] privateKey = { algorithm: \"RSA\" size: 2048 } } } } When a Certificate is created, a corresponding CertificateRequest resource is created by cert-manager containing the encoded X.509 certificate request, Issuer reference, and other options based upon the specification of the Certificate resource. This Certificate CRD will tell cert-manager to attempt to use the Issuer (as AWS ACM) to obtain a certificate key pair for the specified domains. If successful, the resulting TLS key and certificate will be stored in a kubernetes secret named , with keys of tls.key, and tls.crt respectively. This secret will live in the same namespace as the Certificate resource. Now, you may run kubectl get Certificate to view the status of Certificate Request from AWS PCA. NAME READY SECRET AGE example True aws001-preprod-dev-eks-clusterissuer 3h35m If the status is True , that means, the tls.crt , tls.key and ca.crt will all be available in Kubernetes Secret aws001-preprod-dev-eks-clusterissuer Name: aws001-preprod-dev-eks-clusterissuer Namespace: default Labels: <none> Annotations: cert-manager.io/alt-names: cert-manager.io/certificate-name: example cert-manager.io/common-name: example.com cert-manager.io/ip-sans: cert-manager.io/issuer-group: awspca.cert-manager.io cert-manager.io/issuer-kind: AWSPCAClusterIssuer cert-manager.io/issuer-name: aws001-preprod-dev-eks cert-manager.io/uri-sans: Type: kubernetes.io/tls Data ==== ca.crt: 1785 bytes tls.crt: 1517 bytes tls.key: 1679 bytes","title":"Usage"},{"location":"add-ons/calico/","text":"Calico \u00b6 Calico is a widely adopted, battle-tested open source networking and network security solution for Kubernetes, virtual machines, and bare-metal workloads Calico provides two major services for Cloud Native applications: network connectivity between workloads and network security policy enforcement between workloads. Calico docs chart bootstraps Calico infrastructure on a Kubernetes cluster using the Helm package manager. For complete project documentation, please visit the Calico documentation site . Usage \u00b6 Calico can be deployed by enabling the add-on via the following. enable_calico = true Deploy Calico with custom values.yaml # Optional Map value; pass calico-values.yaml from consumer module calico_helm_config = { name = \"calico\" # (Required) Release name. repository = \"https://docs.projectcalico.org/charts\" # (Optional) Repository URL where to locate the requested chart. chart = \"tigera-operator\" # (Required) Chart name to be installed. version = \"v3.24.1\" # (Optional) Specify the exact chart version to install. If this is not specified, it defaults to the version set within default_helm_config: https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/main/modules/kubernetes-addons/calico/locals.tf namespace = \"tigera-operator\" # (Optional) The namespace to install the release into. values = [templatefile(\"${path.module}/calico-values.yaml\", {})] } GitOps Configuration \u00b6 The following properties are made available for use when managing the add-on via GitOps. calico = { enable = true }","title":"Calico"},{"location":"add-ons/calico/#calico","text":"Calico is a widely adopted, battle-tested open source networking and network security solution for Kubernetes, virtual machines, and bare-metal workloads Calico provides two major services for Cloud Native applications: network connectivity between workloads and network security policy enforcement between workloads. Calico docs chart bootstraps Calico infrastructure on a Kubernetes cluster using the Helm package manager. For complete project documentation, please visit the Calico documentation site .","title":"Calico"},{"location":"add-ons/calico/#usage","text":"Calico can be deployed by enabling the add-on via the following. enable_calico = true Deploy Calico with custom values.yaml # Optional Map value; pass calico-values.yaml from consumer module calico_helm_config = { name = \"calico\" # (Required) Release name. repository = \"https://docs.projectcalico.org/charts\" # (Optional) Repository URL where to locate the requested chart. chart = \"tigera-operator\" # (Required) Chart name to be installed. version = \"v3.24.1\" # (Optional) Specify the exact chart version to install. If this is not specified, it defaults to the version set within default_helm_config: https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/main/modules/kubernetes-addons/calico/locals.tf namespace = \"tigera-operator\" # (Optional) The namespace to install the release into. values = [templatefile(\"${path.module}/calico-values.yaml\", {})] }","title":"Usage"},{"location":"add-ons/calico/#gitops-configuration","text":"The following properties are made available for use when managing the add-on via GitOps. calico = { enable = true }","title":"GitOps Configuration"},{"location":"add-ons/cert-manager-csi-driver/","text":"cert-manager-csi-driver \u00b6 Cert Manager csi-driver is a Container Storage Interface (CSI) driver plugin for Kubernetes to work along cert-manager. The goal for this plugin is to seamlessly request and mount certificate key pairs to pods. This is useful for facilitating mTLS, or otherwise securing connections of pods with guaranteed present certificates whilst having all of the features that cert-manager provides. For complete project documentation, please visit the cert-manager-csi-driver documentation site . Usage \u00b6 cert-manger can be deployed by enabling the add-on via the following. enable_cert_manager_csi_driver = true GitOps Configuration \u00b6 The following properties are made available for use when managing the add-on via GitOps. certManagerCsiDriver = { enable = true }","title":"cert-manager-csi-driver"},{"location":"add-ons/cert-manager-csi-driver/#cert-manager-csi-driver","text":"Cert Manager csi-driver is a Container Storage Interface (CSI) driver plugin for Kubernetes to work along cert-manager. The goal for this plugin is to seamlessly request and mount certificate key pairs to pods. This is useful for facilitating mTLS, or otherwise securing connections of pods with guaranteed present certificates whilst having all of the features that cert-manager provides. For complete project documentation, please visit the cert-manager-csi-driver documentation site .","title":"cert-manager-csi-driver"},{"location":"add-ons/cert-manager-csi-driver/#usage","text":"cert-manger can be deployed by enabling the add-on via the following. enable_cert_manager_csi_driver = true","title":"Usage"},{"location":"add-ons/cert-manager-csi-driver/#gitops-configuration","text":"The following properties are made available for use when managing the add-on via GitOps. certManagerCsiDriver = { enable = true }","title":"GitOps Configuration"},{"location":"add-ons/cert-manager/","text":"cert-manager \u00b6 cert-manager adds certificates and certificate issuers as resource types in Kubernetes clusters, and simplifies the process of obtaining, renewing and using those certificates. For complete project documentation, please visit the cert-manager documentation site . Usage \u00b6 cert-manger can be deployed by enabling the add-on via the following. enable_cert_manager = true cert-manger can optionally leverage the cert_manager_domain_names global property of the kubernetes_addon submodule for DNS01 protocol. The value for this property should be a list of Route53 domains managed by your account. cert-manager is restricted to the zones from the list. cert_manager_domain_names = [<cluster_domain>, <another_cluster_domain>] With this add-on self-signed CA and Let's Encrypt cluster issuers will be installed. You can disable Let's Encrypt cluster issuers with: cert_manager_install_letsencrypt_issuers = false You can set an email address for expiration emails with: cert_manager_letsencrypt_email = \"user@example.com\" You can pass previously created secrets for use as imagePullSecrets on the Service Account cert_manager_kubernetes_svc_image_pull_secrets = [\"regcred\"] GitOps Configuration \u00b6 The following properties are made available for use when managing the add-on via GitOps. certManager = { enable = true }","title":"cert-manager"},{"location":"add-ons/cert-manager/#cert-manager","text":"cert-manager adds certificates and certificate issuers as resource types in Kubernetes clusters, and simplifies the process of obtaining, renewing and using those certificates. For complete project documentation, please visit the cert-manager documentation site .","title":"cert-manager"},{"location":"add-ons/cert-manager/#usage","text":"cert-manger can be deployed by enabling the add-on via the following. enable_cert_manager = true cert-manger can optionally leverage the cert_manager_domain_names global property of the kubernetes_addon submodule for DNS01 protocol. The value for this property should be a list of Route53 domains managed by your account. cert-manager is restricted to the zones from the list. cert_manager_domain_names = [<cluster_domain>, <another_cluster_domain>] With this add-on self-signed CA and Let's Encrypt cluster issuers will be installed. You can disable Let's Encrypt cluster issuers with: cert_manager_install_letsencrypt_issuers = false You can set an email address for expiration emails with: cert_manager_letsencrypt_email = \"user@example.com\" You can pass previously created secrets for use as imagePullSecrets on the Service Account cert_manager_kubernetes_svc_image_pull_secrets = [\"regcred\"]","title":"Usage"},{"location":"add-ons/cert-manager/#gitops-configuration","text":"The following properties are made available for use when managing the add-on via GitOps. certManager = { enable = true }","title":"GitOps Configuration"},{"location":"add-ons/chaos-mesh/","text":"Chaos Mesh \u00b6 Chaos Mesh is an open source cloud-native Chaos Engineering platform. It offers various types of fault simulation and has an enormous capability to orchestrate fault scenarios Chaos Mesh docs chart bootstraps Chaos Mesh infrastructure on a Kubernetes cluster using the Helm package manager. For complete project documentation, please visit the Chaos Mesh site . Usage \u00b6 Chaos Mesh can be deployed by enabling the add-on via the following. enable_chaos_mesh = true Deploy Chaos Mesh with custom values.yaml # Optional Map value; pass chaos-mesh-values.yaml from consumer module chaos_mesh_helm_config = { name = \"chaos-mesh\" # (Required) Release name. repository = \"https://charts.chaos-mesh.org\" # (Optional) Repository URL where to locate the requested chart. chart = \"chaos-mesh\" # (Required) Chart name to be installed. version = \"2.3.0\" # (Optional) Specify the exact chart version to install. If this is not specified, it defaults to the version set within default_helm_config: https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/main/modules/kubernetes-addons/chaos-mesh/locals.tf namespace = \"chaos-testing\" # (Optional) The namespace to install the release into. values = [templatefile(\"${path.module}/chaos-mesh-values.yaml\", {})] } GitOps Configuration \u00b6 The following properties are made available for use when managing the add-on via GitOps. chaosMesh = { enable = true }","title":"Chaos Mesh"},{"location":"add-ons/chaos-mesh/#chaos-mesh","text":"Chaos Mesh is an open source cloud-native Chaos Engineering platform. It offers various types of fault simulation and has an enormous capability to orchestrate fault scenarios Chaos Mesh docs chart bootstraps Chaos Mesh infrastructure on a Kubernetes cluster using the Helm package manager. For complete project documentation, please visit the Chaos Mesh site .","title":"Chaos Mesh"},{"location":"add-ons/chaos-mesh/#usage","text":"Chaos Mesh can be deployed by enabling the add-on via the following. enable_chaos_mesh = true Deploy Chaos Mesh with custom values.yaml # Optional Map value; pass chaos-mesh-values.yaml from consumer module chaos_mesh_helm_config = { name = \"chaos-mesh\" # (Required) Release name. repository = \"https://charts.chaos-mesh.org\" # (Optional) Repository URL where to locate the requested chart. chart = \"chaos-mesh\" # (Required) Chart name to be installed. version = \"2.3.0\" # (Optional) Specify the exact chart version to install. If this is not specified, it defaults to the version set within default_helm_config: https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/main/modules/kubernetes-addons/chaos-mesh/locals.tf namespace = \"chaos-testing\" # (Optional) The namespace to install the release into. values = [templatefile(\"${path.module}/chaos-mesh-values.yaml\", {})] }","title":"Usage"},{"location":"add-ons/chaos-mesh/#gitops-configuration","text":"The following properties are made available for use when managing the add-on via GitOps. chaosMesh = { enable = true }","title":"GitOps Configuration"},{"location":"add-ons/cilium/","text":"Cilium \u00b6 Cilium is open source software for transparently securing the network connectivity between application services deployed using Linux container management platforms like Docker and Kubernetes. Cilium can be set up in two manners: - In combination with the Amazon VPC CNI plugin . In this hybrid mode, the AWS VPC CNI plugin is responsible for setting up the virtual network devices as well as for IP address management (IPAM) via ENIs. After the initial networking is setup for a given pod, the Cilium CNI plugin is called to attach eBPF programs to the network devices set up by the AWS VPC CNI plugin in order to enforce network policies, perform load-balancing and provide encryption. Read the installation instruction here - As a replacement of Amazon VPC CNI , read the complete installation guideline here For complete project documentation, please visit the Cilium documentation site . Usage \u00b6 By Cilium in combination with the Amazon VPC CNI plugin by enabling the add-on via the following. enable_cilium = true Deploy Cilium with custom values.yaml # Optional Map value; pass cilium-values.yaml from consumer module cilium_helm_config = { name = \"cilium\" # (Required) Release name. repository = \"https://helm.cilium.io/\" # (Optional) Repository URL where to locate the requested chart. chart = \"cilium\" # (Required) Chart name to be installed. version = \"1.12.1\" # (Optional) Specify the exact chart version to install. If this is not specified, it defaults to the version set within default_helm_config: https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/main/modules/kubernetes-addons/cilium/locals.tf values = [templatefile(\"${path.module}/cilium-values.yaml\", {})] } Refer to the cilium default values file for complete values options for the chart GitOps Configuration \u00b6 The following properties are made available for use when managing the add-on via GitOps. cilium = { enable = true }","title":"Cilium"},{"location":"add-ons/cilium/#cilium","text":"Cilium is open source software for transparently securing the network connectivity between application services deployed using Linux container management platforms like Docker and Kubernetes. Cilium can be set up in two manners: - In combination with the Amazon VPC CNI plugin . In this hybrid mode, the AWS VPC CNI plugin is responsible for setting up the virtual network devices as well as for IP address management (IPAM) via ENIs. After the initial networking is setup for a given pod, the Cilium CNI plugin is called to attach eBPF programs to the network devices set up by the AWS VPC CNI plugin in order to enforce network policies, perform load-balancing and provide encryption. Read the installation instruction here - As a replacement of Amazon VPC CNI , read the complete installation guideline here For complete project documentation, please visit the Cilium documentation site .","title":"Cilium"},{"location":"add-ons/cilium/#usage","text":"By Cilium in combination with the Amazon VPC CNI plugin by enabling the add-on via the following. enable_cilium = true Deploy Cilium with custom values.yaml # Optional Map value; pass cilium-values.yaml from consumer module cilium_helm_config = { name = \"cilium\" # (Required) Release name. repository = \"https://helm.cilium.io/\" # (Optional) Repository URL where to locate the requested chart. chart = \"cilium\" # (Required) Chart name to be installed. version = \"1.12.1\" # (Optional) Specify the exact chart version to install. If this is not specified, it defaults to the version set within default_helm_config: https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/main/modules/kubernetes-addons/cilium/locals.tf values = [templatefile(\"${path.module}/cilium-values.yaml\", {})] } Refer to the cilium default values file for complete values options for the chart","title":"Usage"},{"location":"add-ons/cilium/#gitops-configuration","text":"The following properties are made available for use when managing the add-on via GitOps. cilium = { enable = true }","title":"GitOps Configuration"},{"location":"add-ons/cluster-autoscaler/","text":"Cluster Autoscaler \u00b6 Cluster Autoscaler is a tool that automatically adjusts the number of nodes in your cluster when: Pods fail due to insufficient resources, or Pods are rescheduled onto other nodes due to being in nodes that are underutilized for an extended period of time. The Cluster Autoscaler add-on adds support for Cluster Autoscaler to an EKS cluster. It is typically installed as a Deployment in your cluster. It uses leader election to ensure high availability, but scaling is one done via one replica at a time. Usage \u00b6 Cluster Autoscaler can be deployed by enabling the add-on via the following. enable_cluster_autoscaler = true GitOps Configuration \u00b6 The following properties are made available for use when managing the add-on via GitOps. clusterAutoscaler = { enable = true serviceAccountName = \"<service_account_name>\" }","title":"Cluster Autoscaler"},{"location":"add-ons/cluster-autoscaler/#cluster-autoscaler","text":"Cluster Autoscaler is a tool that automatically adjusts the number of nodes in your cluster when: Pods fail due to insufficient resources, or Pods are rescheduled onto other nodes due to being in nodes that are underutilized for an extended period of time. The Cluster Autoscaler add-on adds support for Cluster Autoscaler to an EKS cluster. It is typically installed as a Deployment in your cluster. It uses leader election to ensure high availability, but scaling is one done via one replica at a time.","title":"Cluster Autoscaler"},{"location":"add-ons/cluster-autoscaler/#usage","text":"Cluster Autoscaler can be deployed by enabling the add-on via the following. enable_cluster_autoscaler = true","title":"Usage"},{"location":"add-ons/cluster-autoscaler/#gitops-configuration","text":"The following properties are made available for use when managing the add-on via GitOps. clusterAutoscaler = { enable = true serviceAccountName = \"<service_account_name>\" }","title":"GitOps Configuration"},{"location":"add-ons/cluster-proportional-autoscaler/","text":"Horizontal cluster-proportional-autoscaler container \u00b6 Horizontal cluster-proportional-autoscaler watches over the number of schedulable nodes and cores of the cluster and resizes the number of replicas for the required resource. This functionality may be desirable for applications that need to be autoscaled with the size of the cluster, such as CoreDNS and other services that scale with the number of nodes/pods in the cluster. The cluster-proportional-autoscaler helps to scale the applications using deployment or replicationcontroller or replicaset. This is an alternative solution to Horizontal Pod Autoscaling. It is typically installed as a Deployment in your cluster. Usage \u00b6 This add-on requires both enable_coredns_autoscaler and coredns_autoscaler_helm_config as mandatory fields. cluster-proportional-autoscaler can be deployed by enabling the add-on via the following. The example shows how to enable cluster-proportional-autoscaler for CoreDNS Deployment . CoreDNS deployment is not configured with HPA. So, this add-on helps to scale CoreDNS Add-on according to the size of the nodes and cores. This Add-on can be used to scale any application with Deployment objects. enable_coredns_autoscaler = true coredns_autoscaler_helm_config = { name = \"cluster-proportional-autoscaler\" chart = \"cluster-proportional-autoscaler\" repository = \"https://kubernetes-sigs.github.io/cluster-proportional-autoscaler\" version = \"1.0.0\" namespace = \"kube-system\" timeout = \"300\" values = [ <<-EOT nameOverride: kube-dns-autoscaler # Formula for controlling the replicas. Adjust according to your needs # replicas = max( ceil( cores * 1/coresPerReplica ) , ceil( nodes * 1/nodesPerReplica ) ) config: linear: coresPerReplica: 256 nodesPerReplica: 16 min: 1 max: 100 preventSinglePointFailure: true includeUnschedulableNodes: true # Target to scale. In format: deployment/*, replicationcontroller/* or replicaset/* (not case sensitive). options: target: deployment/coredns # Notice the target as `deployment/coredns` serviceAccount: create: true name: kube-dns-autoscaler podSecurityContext: seccompProfile: type: RuntimeDefault supplementalGroups: [ 65534 ] fsGroup: 65534 resources: limits: cpu: 100m memory: 128Mi requests: cpu: 100m memory: 128Mi tolerations: - key: \"CriticalAddonsOnly\" operator: \"Exists\" description = \"Cluster Proportional Autoscaler for CoreDNS Service\" EOT ] } GitOps Configuration \u00b6 The following properties are made available for use when managing the add-on via GitOps. corednsAutoscaler = { enable = true }","title":"Horizontal cluster-proportional-autoscaler container"},{"location":"add-ons/cluster-proportional-autoscaler/#horizontal-cluster-proportional-autoscaler-container","text":"Horizontal cluster-proportional-autoscaler watches over the number of schedulable nodes and cores of the cluster and resizes the number of replicas for the required resource. This functionality may be desirable for applications that need to be autoscaled with the size of the cluster, such as CoreDNS and other services that scale with the number of nodes/pods in the cluster. The cluster-proportional-autoscaler helps to scale the applications using deployment or replicationcontroller or replicaset. This is an alternative solution to Horizontal Pod Autoscaling. It is typically installed as a Deployment in your cluster.","title":"Horizontal cluster-proportional-autoscaler container"},{"location":"add-ons/cluster-proportional-autoscaler/#usage","text":"This add-on requires both enable_coredns_autoscaler and coredns_autoscaler_helm_config as mandatory fields. cluster-proportional-autoscaler can be deployed by enabling the add-on via the following. The example shows how to enable cluster-proportional-autoscaler for CoreDNS Deployment . CoreDNS deployment is not configured with HPA. So, this add-on helps to scale CoreDNS Add-on according to the size of the nodes and cores. This Add-on can be used to scale any application with Deployment objects. enable_coredns_autoscaler = true coredns_autoscaler_helm_config = { name = \"cluster-proportional-autoscaler\" chart = \"cluster-proportional-autoscaler\" repository = \"https://kubernetes-sigs.github.io/cluster-proportional-autoscaler\" version = \"1.0.0\" namespace = \"kube-system\" timeout = \"300\" values = [ <<-EOT nameOverride: kube-dns-autoscaler # Formula for controlling the replicas. Adjust according to your needs # replicas = max( ceil( cores * 1/coresPerReplica ) , ceil( nodes * 1/nodesPerReplica ) ) config: linear: coresPerReplica: 256 nodesPerReplica: 16 min: 1 max: 100 preventSinglePointFailure: true includeUnschedulableNodes: true # Target to scale. In format: deployment/*, replicationcontroller/* or replicaset/* (not case sensitive). options: target: deployment/coredns # Notice the target as `deployment/coredns` serviceAccount: create: true name: kube-dns-autoscaler podSecurityContext: seccompProfile: type: RuntimeDefault supplementalGroups: [ 65534 ] fsGroup: 65534 resources: limits: cpu: 100m memory: 128Mi requests: cpu: 100m memory: 128Mi tolerations: - key: \"CriticalAddonsOnly\" operator: \"Exists\" description = \"Cluster Proportional Autoscaler for CoreDNS Service\" EOT ] }","title":"Usage"},{"location":"add-ons/cluster-proportional-autoscaler/#gitops-configuration","text":"The following properties are made available for use when managing the add-on via GitOps. corednsAutoscaler = { enable = true }","title":"GitOps Configuration"},{"location":"add-ons/crossplane/","text":"Crossplane \u00b6 Crossplane is an open source Kubernetes add-on that enables platform teams to assemble infrastructure from multiple vendors, and expose higher level self-service APIs for application teams to consume, without having to write any code. Crossplane is a control plane Allow engineers to model their infrastructure as declarative configuration Support managing a myriad of diverse infrastructure using \"provider\" plugins It's an open source tool with strong communities For complete project documentation, please visit the Crossplane . Usage \u00b6 Crossplane Deployment \u00b6 Crossplane can be deployed by enabling the add-on via the following. Check out the full example to deploy the EKS Cluster with Crossplane. enable_crossplane = true You can optionally customize the Helm chart that deploys Crossplane via the following configuration. enable_crossplane = true crossplane_helm_config = { name = \"crossplane\" chart = \"crossplane\" repository = \"https://charts.crossplane.io/stable/\" version = \"1.6.2\" namespace = \"crossplane-system\" values = [templatefile(\"${path.module}/values.yaml\", { service_account_name = var.service_account_name, operating_system = \"linux\" })] } crossplane_irsa_policies = [] # Optional to add additional policies to Crossplane IRSA Crossplane AWS Provider Deployment \u00b6 This module provides options to deploy the following AWS providers for Crossplane. These providers disabled by default, and it can be enabled using the config below. AWS Provider Terrajet AWS Provider NOTE: Crossplane requires Admin like permissions to create and update resources similar to Terraform deploy role. This example config uses AdministratorAccess, but you should select a policy with the minimum permissions required to provision your resources. Config to deploy AWS Provider # Creates ProviderConfig -> aws-provider crossplane_aws_provider = { enable = true provider_aws_version = \"v0.24.1\" # Get the latest version from https://github.com/crossplane/provider-aws additional_irsa_policies = [\"arn:aws:iam::aws:policy/AdministratorAccess\"] } Config to deploy Terrajet AWS Provider # Creates ProviderConfig -> jet-aws-provider crossplane_jet_aws_provider = { enable = true provider_aws_version = \"v0.4.1\" # Get the latest version from https://github.com/crossplane-contrib/provider-jet-aws additional_irsa_policies = [\"arn:aws:iam::aws:policy/AdministratorAccess\"] } Checkout the full example to deploy Crossplane with kubernetes-addons module","title":"Crossplane"},{"location":"add-ons/crossplane/#crossplane","text":"Crossplane is an open source Kubernetes add-on that enables platform teams to assemble infrastructure from multiple vendors, and expose higher level self-service APIs for application teams to consume, without having to write any code. Crossplane is a control plane Allow engineers to model their infrastructure as declarative configuration Support managing a myriad of diverse infrastructure using \"provider\" plugins It's an open source tool with strong communities For complete project documentation, please visit the Crossplane .","title":"Crossplane"},{"location":"add-ons/crossplane/#usage","text":"","title":"Usage"},{"location":"add-ons/crossplane/#crossplane-deployment","text":"Crossplane can be deployed by enabling the add-on via the following. Check out the full example to deploy the EKS Cluster with Crossplane. enable_crossplane = true You can optionally customize the Helm chart that deploys Crossplane via the following configuration. enable_crossplane = true crossplane_helm_config = { name = \"crossplane\" chart = \"crossplane\" repository = \"https://charts.crossplane.io/stable/\" version = \"1.6.2\" namespace = \"crossplane-system\" values = [templatefile(\"${path.module}/values.yaml\", { service_account_name = var.service_account_name, operating_system = \"linux\" })] } crossplane_irsa_policies = [] # Optional to add additional policies to Crossplane IRSA","title":"Crossplane Deployment"},{"location":"add-ons/crossplane/#crossplane-aws-provider-deployment","text":"This module provides options to deploy the following AWS providers for Crossplane. These providers disabled by default, and it can be enabled using the config below. AWS Provider Terrajet AWS Provider NOTE: Crossplane requires Admin like permissions to create and update resources similar to Terraform deploy role. This example config uses AdministratorAccess, but you should select a policy with the minimum permissions required to provision your resources. Config to deploy AWS Provider # Creates ProviderConfig -> aws-provider crossplane_aws_provider = { enable = true provider_aws_version = \"v0.24.1\" # Get the latest version from https://github.com/crossplane/provider-aws additional_irsa_policies = [\"arn:aws:iam::aws:policy/AdministratorAccess\"] } Config to deploy Terrajet AWS Provider # Creates ProviderConfig -> jet-aws-provider crossplane_jet_aws_provider = { enable = true provider_aws_version = \"v0.4.1\" # Get the latest version from https://github.com/crossplane-contrib/provider-jet-aws additional_irsa_policies = [\"arn:aws:iam::aws:policy/AdministratorAccess\"] } Checkout the full example to deploy Crossplane with kubernetes-addons module","title":"Crossplane AWS Provider Deployment"},{"location":"add-ons/csi-secrets-store-provider-aws/","text":"secrets-store-csi-driver-provider-aws \u00b6 AWS Secrets Manager and Config Provider for Secret Store CSI Driver allows you to get secret contents stored in AWS Key Management Service instance and use the Secrets Store CSI driver interface to mount them into Kubernetes pods. For detailed architectual overview, refer [How to use AWS Secrets & Configuration Provider with your Kubernetes Secrets Store CSI driver] (https://aws.amazon.com/blogs/security/how-to-use-aws-secrets-configuration-provider-with-kubernetes-secrets-store-csi-driver/) Usage \u00b6 csi-secrets-store-provider-aws can be deployed by enabling the add-ons via the following. enable_secrets_store_csi_driver = true enable_secrets_store_csi_driver_provider_aws = true","title":"secrets-store-csi-driver-provider-aws"},{"location":"add-ons/csi-secrets-store-provider-aws/#secrets-store-csi-driver-provider-aws","text":"AWS Secrets Manager and Config Provider for Secret Store CSI Driver allows you to get secret contents stored in AWS Key Management Service instance and use the Secrets Store CSI driver interface to mount them into Kubernetes pods. For detailed architectual overview, refer [How to use AWS Secrets & Configuration Provider with your Kubernetes Secrets Store CSI driver] (https://aws.amazon.com/blogs/security/how-to-use-aws-secrets-configuration-provider-with-kubernetes-secrets-store-csi-driver/)","title":"secrets-store-csi-driver-provider-aws"},{"location":"add-ons/csi-secrets-store-provider-aws/#usage","text":"csi-secrets-store-provider-aws can be deployed by enabling the add-ons via the following. enable_secrets_store_csi_driver = true enable_secrets_store_csi_driver_provider_aws = true","title":"Usage"},{"location":"add-ons/external-dns/","text":"ExternalDNS \u00b6 External DNS is a Kubernetes add-on that can automate the management of DNS records based on Ingress and Service resources. For complete project documentation, please visit the External DNS Github repository . Usage \u00b6 External DNS can be deployed by enabling the add-on via the following. enable_external_dns = true External DNS can optionally leverage the eks_cluster_domain global property of the kubernetes_addon submodule. The value for this property should be a Route53 domain managed by your account. ExternalDNS will leverage the value supplied for its zoneIdFilters property, which will restrict ExternalDNS to only create records for this domain. See docs here . eks_cluster_domain = <cluster_domain> Alternatively, you can supply a list of Route53 zone ARNs which external-dns will have access to create/manage records: external_dns_route53_zone_arns = [ \"arn:aws:route53::123456789012:hostedzone/Z1234567890\" ] You can optionally customize the Helm chart that deploys external-dns via the following configuration. enable_external_dns = true external_dns_helm_config = { name = \"external-dns\" chart = \"external-dns\" repository = \"https://charts.bitnami.com/bitnami\" version = \"6.1.6\" namespace = \"external-dns\" } GitOps Configuration \u00b6 The following properties are made available for use when managing the add-on via GitOps. external_dns = { enable = true zoneFilterIds = local.zone_filter_ids serviceAccountName = local.service_account_name }","title":"ExternalDNS"},{"location":"add-ons/external-dns/#externaldns","text":"External DNS is a Kubernetes add-on that can automate the management of DNS records based on Ingress and Service resources. For complete project documentation, please visit the External DNS Github repository .","title":"ExternalDNS"},{"location":"add-ons/external-dns/#usage","text":"External DNS can be deployed by enabling the add-on via the following. enable_external_dns = true External DNS can optionally leverage the eks_cluster_domain global property of the kubernetes_addon submodule. The value for this property should be a Route53 domain managed by your account. ExternalDNS will leverage the value supplied for its zoneIdFilters property, which will restrict ExternalDNS to only create records for this domain. See docs here . eks_cluster_domain = <cluster_domain> Alternatively, you can supply a list of Route53 zone ARNs which external-dns will have access to create/manage records: external_dns_route53_zone_arns = [ \"arn:aws:route53::123456789012:hostedzone/Z1234567890\" ] You can optionally customize the Helm chart that deploys external-dns via the following configuration. enable_external_dns = true external_dns_helm_config = { name = \"external-dns\" chart = \"external-dns\" repository = \"https://charts.bitnami.com/bitnami\" version = \"6.1.6\" namespace = \"external-dns\" }","title":"Usage"},{"location":"add-ons/external-dns/#gitops-configuration","text":"The following properties are made available for use when managing the add-on via GitOps. external_dns = { enable = true zoneFilterIds = local.zone_filter_ids serviceAccountName = local.service_account_name }","title":"GitOps Configuration"},{"location":"add-ons/external-secrets/","text":"External Secrets Operator \u00b6 External Secrets Operator is a Kubernetes operator that integrates external secret management systems like AWS Secrets Manager, HashiCorp Vault, Google Secrets Manager, Azure Key Vault and many more. The operator reads information from external APIs and automatically injects the values into a Kubernetes Secret. Usage \u00b6 The External Secrets Operator can be deployed by enabling the add-on via the following. enable_external_secrets = true You can optionally customize the Helm chart that deploys the operator via the following configuration. enable_external_secrets = true external_secrets_helm_config = { name = \"external-secrets\" chart = \"external-secrets\" repository = \"https://charts.external-secrets.io/\" version = \"0.5.9\" namespace = \"external-secrets\" } GitOps Configuration \u00b6 The following properties are made available for use when managing the add-on via GitOps. Refer to locals.tf for latest config. GitOps with ArgoCD Add-on repo is located here . argocd_gitops_config = { enable = true }","title":"External Secrets Operator"},{"location":"add-ons/external-secrets/#external-secrets-operator","text":"External Secrets Operator is a Kubernetes operator that integrates external secret management systems like AWS Secrets Manager, HashiCorp Vault, Google Secrets Manager, Azure Key Vault and many more. The operator reads information from external APIs and automatically injects the values into a Kubernetes Secret.","title":"External Secrets Operator"},{"location":"add-ons/external-secrets/#usage","text":"The External Secrets Operator can be deployed by enabling the add-on via the following. enable_external_secrets = true You can optionally customize the Helm chart that deploys the operator via the following configuration. enable_external_secrets = true external_secrets_helm_config = { name = \"external-secrets\" chart = \"external-secrets\" repository = \"https://charts.external-secrets.io/\" version = \"0.5.9\" namespace = \"external-secrets\" }","title":"Usage"},{"location":"add-ons/external-secrets/#gitops-configuration","text":"The following properties are made available for use when managing the add-on via GitOps. Refer to locals.tf for latest config. GitOps with ArgoCD Add-on repo is located here . argocd_gitops_config = { enable = true }","title":"GitOps Configuration"},{"location":"add-ons/fargate-fluent-bit/","text":"Fluent Bit for Fargate \u00b6 Fluent Bit for Fargate configures Fluent Bit to forward Fargate Container logs to CloudWatch. Usage \u00b6 Fluent Bit for Fargate can be deployed by enabling the add-on via the following. enable_fargate_fluentbit = true","title":"Fargate fluent bit"},{"location":"add-ons/fargate-fluent-bit/#fluent-bit-for-fargate","text":"Fluent Bit for Fargate configures Fluent Bit to forward Fargate Container logs to CloudWatch.","title":"Fluent Bit for Fargate"},{"location":"add-ons/fargate-fluent-bit/#usage","text":"Fluent Bit for Fargate can be deployed by enabling the add-on via the following. enable_fargate_fluentbit = true","title":"Usage"},{"location":"add-ons/gatekeeper/","text":"Gatekeeper \u00b6 Gatekeeper is an admission controller that validates requests to create and update Pods on Kubernetes clusters, using the Open Policy Agent (OPA). Using Gatekeeper allows administrators to define policies with a constraint, which is a set of conditions that permit or deny deployment behaviors in Kubernetes. For complete project documentation, please visit the Gatekeeper . For reference templates refer Templates Usage \u00b6 Gatekeeper can be deployed by enabling the add-on via the following. enable_gatekeeper = true You can optionally customize the Helm chart that deploys Gatekeeper via the following configuration. enable_gatekeeper = true # Optional gatekeeper_helm_config gatekeeper_helm_config = { name = \"gatekeeper\" chart = \"gatekeeper\" repository = \"https://open-policy-agent.github.io/gatekeeper/charts\" version = \"3.9.0\" namespace = \"gatekeeper-system\" values = [ <<-EOT clusterName: ${var.eks_cluster_id} EOT ] } GitOps Configuration \u00b6 The following properties are made available for use when managing the add-on via GitOps. argocd_gitops_config = { enable = true clusterName = var.eks_cluster_id }","title":"Gatekeeper"},{"location":"add-ons/gatekeeper/#gatekeeper","text":"Gatekeeper is an admission controller that validates requests to create and update Pods on Kubernetes clusters, using the Open Policy Agent (OPA). Using Gatekeeper allows administrators to define policies with a constraint, which is a set of conditions that permit or deny deployment behaviors in Kubernetes. For complete project documentation, please visit the Gatekeeper . For reference templates refer Templates","title":"Gatekeeper"},{"location":"add-ons/gatekeeper/#usage","text":"Gatekeeper can be deployed by enabling the add-on via the following. enable_gatekeeper = true You can optionally customize the Helm chart that deploys Gatekeeper via the following configuration. enable_gatekeeper = true # Optional gatekeeper_helm_config gatekeeper_helm_config = { name = \"gatekeeper\" chart = \"gatekeeper\" repository = \"https://open-policy-agent.github.io/gatekeeper/charts\" version = \"3.9.0\" namespace = \"gatekeeper-system\" values = [ <<-EOT clusterName: ${var.eks_cluster_id} EOT ] }","title":"Usage"},{"location":"add-ons/gatekeeper/#gitops-configuration","text":"The following properties are made available for use when managing the add-on via GitOps. argocd_gitops_config = { enable = true clusterName = var.eks_cluster_id }","title":"GitOps Configuration"},{"location":"add-ons/grafana/","text":"Grafana \u00b6 Grafana is an open source platform for monitoring and observability. Grafana addon can be deployed with EKS blueprints in Amazon EKS server. This add-on configures Prometheus and CloudWatch data sources. You can add more data sources using the values.yaml Usage \u00b6 Grafana can be deployed by enabling the add-on via the following. This example shows the usage of the Secrets Manager to create a new secret for Grafana adminPassword. This option sets a default adminPassword by the helm chart which can be extracted from kubernetes secrets with the name as grafana . enable_grafana = true You can optionally customize the Helm chart that deploys Grafana via the following configuration. Also, provide the adminPassword using set_sensitive values as shown in the example enable_grafana = true grafana_irsa_policies = [] # Optional to add additional policies to IRSA # Optional karpenter_helm_config grafana_helm_config = { name = \"grafana\" chart = \"grafana\" repository = \"https://grafana.github.io/helm-charts\" version = \"6.32.1\" namespace = \"grafana\" description = \"Grafana Helm Chart deployment configuration\" values = [templatefile(\"${path.module}/values.yaml\", {})] set_sensitive = [ { name = \"adminPassword\" value = \"<YOUR_SECURE_PASSWORD_FOR_GARFANA_ADMIN>\" } ] } GitOps Configuration \u00b6 The following properties are made available for use when managing the add-on via GitOps grafana = { enable = true }","title":"Grafana"},{"location":"add-ons/grafana/#grafana","text":"Grafana is an open source platform for monitoring and observability. Grafana addon can be deployed with EKS blueprints in Amazon EKS server. This add-on configures Prometheus and CloudWatch data sources. You can add more data sources using the values.yaml","title":"Grafana"},{"location":"add-ons/grafana/#usage","text":"Grafana can be deployed by enabling the add-on via the following. This example shows the usage of the Secrets Manager to create a new secret for Grafana adminPassword. This option sets a default adminPassword by the helm chart which can be extracted from kubernetes secrets with the name as grafana . enable_grafana = true You can optionally customize the Helm chart that deploys Grafana via the following configuration. Also, provide the adminPassword using set_sensitive values as shown in the example enable_grafana = true grafana_irsa_policies = [] # Optional to add additional policies to IRSA # Optional karpenter_helm_config grafana_helm_config = { name = \"grafana\" chart = \"grafana\" repository = \"https://grafana.github.io/helm-charts\" version = \"6.32.1\" namespace = \"grafana\" description = \"Grafana Helm Chart deployment configuration\" values = [templatefile(\"${path.module}/values.yaml\", {})] set_sensitive = [ { name = \"adminPassword\" value = \"<YOUR_SECURE_PASSWORD_FOR_GARFANA_ADMIN>\" } ] }","title":"Usage"},{"location":"add-ons/grafana/#gitops-configuration","text":"The following properties are made available for use when managing the add-on via GitOps grafana = { enable = true }","title":"GitOps Configuration"},{"location":"add-ons/karpenter/","text":"Karpenter \u00b6 Karpenter is an open-source node provisioning project built for Kubernetes. Karpenter automatically launches just the right compute resources to handle your cluster's applications. It is designed to let you take full advantage of the cloud with fast and simple compute provisioning for Kubernetes clusters. For complete project documentation, please visit the Karpenter . Usage \u00b6 Karpenter can be deployed by enabling the add-on via the following. Check out the full example to deploy the EKS Cluster with Karpenter. enable_karpenter = true You can optionally customize the Helm chart that deploys Karpenter via the following configuration. enable_karpenter = true # Optional karpenter_helm_config karpenter_helm_config = { name = \"karpenter\" chart = \"karpenter\" repository = \"https://charts.karpenter.sh\" version = \"0.6.3\" namespace = \"karpenter\" values = [templatefile(\"${path.module}/values.yaml\", { eks_cluster_id = var.eks_cluster_id, eks_cluster_endpoint = var.eks_cluster_endpoint, service_account_name = var.service_account_name, operating_system = \"linux\" })] } karpenter_irsa_policies = [] # Optional to add additional policies to IRSA GitOps Configuration \u00b6 The following properties are made available for use when managing the add-on via GitOps. Refer to locals.tf for latest config. GitOps with ArgoCD Add-on repo is located here argocd_gitops_config = { enable = true serviceAccountName = local.service_account_name controllerClusterName = var.eks_cluster_id controllerClusterEndpoint = local.eks_cluster_endpoint awsDefaultInstanceProfile = var.node_iam_instance_profile }","title":"Karpenter"},{"location":"add-ons/karpenter/#karpenter","text":"Karpenter is an open-source node provisioning project built for Kubernetes. Karpenter automatically launches just the right compute resources to handle your cluster's applications. It is designed to let you take full advantage of the cloud with fast and simple compute provisioning for Kubernetes clusters. For complete project documentation, please visit the Karpenter .","title":"Karpenter"},{"location":"add-ons/karpenter/#usage","text":"Karpenter can be deployed by enabling the add-on via the following. Check out the full example to deploy the EKS Cluster with Karpenter. enable_karpenter = true You can optionally customize the Helm chart that deploys Karpenter via the following configuration. enable_karpenter = true # Optional karpenter_helm_config karpenter_helm_config = { name = \"karpenter\" chart = \"karpenter\" repository = \"https://charts.karpenter.sh\" version = \"0.6.3\" namespace = \"karpenter\" values = [templatefile(\"${path.module}/values.yaml\", { eks_cluster_id = var.eks_cluster_id, eks_cluster_endpoint = var.eks_cluster_endpoint, service_account_name = var.service_account_name, operating_system = \"linux\" })] } karpenter_irsa_policies = [] # Optional to add additional policies to IRSA","title":"Usage"},{"location":"add-ons/karpenter/#gitops-configuration","text":"The following properties are made available for use when managing the add-on via GitOps. Refer to locals.tf for latest config. GitOps with ArgoCD Add-on repo is located here argocd_gitops_config = { enable = true serviceAccountName = local.service_account_name controllerClusterName = var.eks_cluster_id controllerClusterEndpoint = local.eks_cluster_endpoint awsDefaultInstanceProfile = var.node_iam_instance_profile }","title":"GitOps Configuration"},{"location":"add-ons/keda/","text":"KEDA \u00b6 KEDA is a Kubernetes-based Event Driven Autoscaler. With KEDA, you can drive the scaling of any container in Kubernetes based on the number of events needing to be processed. KEDA is a single-purpose and lightweight component that can be added into any Kubernetes cluster. KEDA works alongside standard Kubernetes components like the Horizontal Pod Autoscaler and can extend functionality without overwriting or duplication. With KEDA you can explicitly map the apps you want to use event-driven scale, with other apps continuing to function. This makes KEDA a flexible and safe option to run alongside any number of any other Kubernetes applications or frameworks.. KEDA chart bootstraps KEDA infrastructure on a Kubernetes cluster using the Helm package manager. For complete project documentation, please visit the KEDA documentation site . Usage \u00b6 KEDA can be deployed by enabling the add-on via the following. enable_keda = true Deploy KEDA with custom values.yaml # Optional Map value; pass keda-values.yaml from consumer module keda_helm_config = { name = \"keda\" # (Required) Release name. repository = \"https://kedacore.github.io/charts\" # (Optional) Repository URL where to locate the requested chart. chart = \"keda\" # (Required) Chart name to be installed. version = \"2.6.2\" # (Optional) Specify the exact chart version to install. If this is not specified, it defaults to the version set within default_helm_config: https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/main/modules/kubernetes-addons/keda/locals.tf namespace = \"keda\" # (Optional) The namespace to install the release into. values = [templatefile(\"${path.module}/keda-values.yaml\", {})] } GitOps Configuration \u00b6 The following properties are made available for use when managing the add-on via GitOps. keda = { enable = true serviceAccountName = \"<service_account_name>\" }","title":"KEDA"},{"location":"add-ons/keda/#keda","text":"KEDA is a Kubernetes-based Event Driven Autoscaler. With KEDA, you can drive the scaling of any container in Kubernetes based on the number of events needing to be processed. KEDA is a single-purpose and lightweight component that can be added into any Kubernetes cluster. KEDA works alongside standard Kubernetes components like the Horizontal Pod Autoscaler and can extend functionality without overwriting or duplication. With KEDA you can explicitly map the apps you want to use event-driven scale, with other apps continuing to function. This makes KEDA a flexible and safe option to run alongside any number of any other Kubernetes applications or frameworks.. KEDA chart bootstraps KEDA infrastructure on a Kubernetes cluster using the Helm package manager. For complete project documentation, please visit the KEDA documentation site .","title":"KEDA"},{"location":"add-ons/keda/#usage","text":"KEDA can be deployed by enabling the add-on via the following. enable_keda = true Deploy KEDA with custom values.yaml # Optional Map value; pass keda-values.yaml from consumer module keda_helm_config = { name = \"keda\" # (Required) Release name. repository = \"https://kedacore.github.io/charts\" # (Optional) Repository URL where to locate the requested chart. chart = \"keda\" # (Required) Chart name to be installed. version = \"2.6.2\" # (Optional) Specify the exact chart version to install. If this is not specified, it defaults to the version set within default_helm_config: https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/main/modules/kubernetes-addons/keda/locals.tf namespace = \"keda\" # (Optional) The namespace to install the release into. values = [templatefile(\"${path.module}/keda-values.yaml\", {})] }","title":"Usage"},{"location":"add-ons/keda/#gitops-configuration","text":"The following properties are made available for use when managing the add-on via GitOps. keda = { enable = true serviceAccountName = \"<service_account_name>\" }","title":"GitOps Configuration"},{"location":"add-ons/kube-prometheus-stack/","text":"kube-prometheus-stack \u00b6 kube-prometheus-stack is a a collection of Kubernetes manifests, Grafana dashboards, and Prometheus rules combined with documentation and scripts to provide easy to operate end-to-end Kubernetes cluster monitoring with Prometheus using the Prometheus Operator. Components installed by this chart in this package by default: The Prometheus Operator Highly available Prometheus Highly available Alertmanager Prometheus node-exporter kube-state-metrics Grafana Usage \u00b6 The default values.yaml file in this add-on has disabled the components that are unreachable in EKS environments, and an EBS Volume for Persistent Storage. You can override the defaults using the set helm_config key, and set the admin password with set_sensitive : enable_kube_prometheus_stack = true kube_prometheus_stack_helm_config = { set = [ { name = \"kubeProxy.enabled\" value = false } ], set_sensitive = [ { name = \"grafana.adminPassword\" value = data.aws_secretsmanager_secret_version.admin_password_version.secret_string } ] } Upgrading the Chart \u00b6 Be aware that it is likely necessary to update the CRDs when updating the Chart version. Refer to the Project documentation on upgrades for your specific versions: https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack#upgrading-chart For complete project documentation, please visit the kube-prometheus-stack Github repository .","title":"kube-prometheus-stack"},{"location":"add-ons/kube-prometheus-stack/#kube-prometheus-stack","text":"kube-prometheus-stack is a a collection of Kubernetes manifests, Grafana dashboards, and Prometheus rules combined with documentation and scripts to provide easy to operate end-to-end Kubernetes cluster monitoring with Prometheus using the Prometheus Operator. Components installed by this chart in this package by default: The Prometheus Operator Highly available Prometheus Highly available Alertmanager Prometheus node-exporter kube-state-metrics Grafana","title":"kube-prometheus-stack"},{"location":"add-ons/kube-prometheus-stack/#usage","text":"The default values.yaml file in this add-on has disabled the components that are unreachable in EKS environments, and an EBS Volume for Persistent Storage. You can override the defaults using the set helm_config key, and set the admin password with set_sensitive : enable_kube_prometheus_stack = true kube_prometheus_stack_helm_config = { set = [ { name = \"kubeProxy.enabled\" value = false } ], set_sensitive = [ { name = \"grafana.adminPassword\" value = data.aws_secretsmanager_secret_version.admin_password_version.secret_string } ] }","title":"Usage"},{"location":"add-ons/kube-prometheus-stack/#upgrading-the-chart","text":"Be aware that it is likely necessary to update the CRDs when updating the Chart version. Refer to the Project documentation on upgrades for your specific versions: https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack#upgrading-chart For complete project documentation, please visit the kube-prometheus-stack Github repository .","title":"Upgrading the Chart"},{"location":"add-ons/kube-state-metrics/","text":"Kube-State-Metrics \u00b6 kube-state-metrics (KSM) is a simple service that listens to the Kubernetes API server and generates metrics about the state of the objects. The metrics are exported on the HTTP endpoint /metrics on the listening port (default 8080). They are served as plaintext. They are designed to be consumed either by Prometheus itself or by a scraper that is compatible with scraping a Prometheus client endpoint. This add-on is implemented as an external add-on. For detailed documentation and usage of the add-on please refer to the add-on repository . Usage \u00b6 The following will deploy the KSM into an EKS Cluster. enable_kube_state_metrics = true Enable KSM with custom values.yaml enable_kube_state_metrics = true # Optional Map value kube_state_metrics_helm_config = { name = \"kube-state-metrics\" # (Required) Release name. repository = \"https://prometheus-community.github.io/helm-charts\" # (Optional) Repository URL where to locate the requested chart. chart = \"kube-state-metrics\" # (Required) Chart name to be installed. version = \"4.5.0\" namespace = \"kube-state-metrics\" values = [templatefile(\"${path.module}/values.yaml\", {}})] } GitOps Configuration \u00b6 The following properties are made available for use when managing the add-on via GitOps argocd_gitops_config = { enable = true serviceAccountName = local.service_account_name }","title":"Kube-State-Metrics"},{"location":"add-ons/kube-state-metrics/#kube-state-metrics","text":"kube-state-metrics (KSM) is a simple service that listens to the Kubernetes API server and generates metrics about the state of the objects. The metrics are exported on the HTTP endpoint /metrics on the listening port (default 8080). They are served as plaintext. They are designed to be consumed either by Prometheus itself or by a scraper that is compatible with scraping a Prometheus client endpoint. This add-on is implemented as an external add-on. For detailed documentation and usage of the add-on please refer to the add-on repository .","title":"Kube-State-Metrics"},{"location":"add-ons/kube-state-metrics/#usage","text":"The following will deploy the KSM into an EKS Cluster. enable_kube_state_metrics = true Enable KSM with custom values.yaml enable_kube_state_metrics = true # Optional Map value kube_state_metrics_helm_config = { name = \"kube-state-metrics\" # (Required) Release name. repository = \"https://prometheus-community.github.io/helm-charts\" # (Optional) Repository URL where to locate the requested chart. chart = \"kube-state-metrics\" # (Required) Chart name to be installed. version = \"4.5.0\" namespace = \"kube-state-metrics\" values = [templatefile(\"${path.module}/values.yaml\", {}})] }","title":"Usage"},{"location":"add-ons/kube-state-metrics/#gitops-configuration","text":"The following properties are made available for use when managing the add-on via GitOps argocd_gitops_config = { enable = true serviceAccountName = local.service_account_name }","title":"GitOps Configuration"},{"location":"add-ons/kubecost/","text":"Kubecost \u00b6 Kubecost provides real-time cost visibility and insights for teams using Kubernetes, helping you continuously reduce your cloud costs. Amazon EKS supports Kubecost, which you can use to monitor your costs broken down by Kubernetes resources including pods, nodes, namespaces, and labels. Cost monitoring docs provides steps to bootstrap Kubecost infrastructure on a EKS cluster using the Helm package manager. For complete project documentation, please visit the Kubecost documentation site . Note: If your cluster is version 1.23 or later, you must have the Amazon EBS CSI driver installed on your cluster. Usage \u00b6 Kubecost can be deployed by enabling the add-on via the following. enable_kubecost = true Deploy Kubecost with custom values.yaml # Optional Map value; pass kubecost-values.yaml from consumer module kubecost_helm_config = { name = \"kubecost\" # (Required) Release name. repository = \"oci://public.ecr.aws/kubecost\" # (Optional) Repository URL where to locate the requested chart. chart = \"cost-analyzer\" # (Required) Chart name to be installed. version = \"1.96.0\" # (Optional) Specify the exact chart version to install. If this is not specified, it defaults to the version set within default_helm_config: https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/main/modules/kubernetes-addons/kubecost/locals.tf namespace = \"kubecost\" # (Optional) The namespace to install the release into. values = [templatefile(\"${path.module}/kubecost-values.yaml\", {})] } GitOps Configuration \u00b6 The following properties are made available for use when managing the add-on via GitOps. kubecost = { enable = true }","title":"Kubecost"},{"location":"add-ons/kubecost/#kubecost","text":"Kubecost provides real-time cost visibility and insights for teams using Kubernetes, helping you continuously reduce your cloud costs. Amazon EKS supports Kubecost, which you can use to monitor your costs broken down by Kubernetes resources including pods, nodes, namespaces, and labels. Cost monitoring docs provides steps to bootstrap Kubecost infrastructure on a EKS cluster using the Helm package manager. For complete project documentation, please visit the Kubecost documentation site . Note: If your cluster is version 1.23 or later, you must have the Amazon EBS CSI driver installed on your cluster.","title":"Kubecost"},{"location":"add-ons/kubecost/#usage","text":"Kubecost can be deployed by enabling the add-on via the following. enable_kubecost = true Deploy Kubecost with custom values.yaml # Optional Map value; pass kubecost-values.yaml from consumer module kubecost_helm_config = { name = \"kubecost\" # (Required) Release name. repository = \"oci://public.ecr.aws/kubecost\" # (Optional) Repository URL where to locate the requested chart. chart = \"cost-analyzer\" # (Required) Chart name to be installed. version = \"1.96.0\" # (Optional) Specify the exact chart version to install. If this is not specified, it defaults to the version set within default_helm_config: https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/main/modules/kubernetes-addons/kubecost/locals.tf namespace = \"kubecost\" # (Optional) The namespace to install the release into. values = [templatefile(\"${path.module}/kubecost-values.yaml\", {})] }","title":"Usage"},{"location":"add-ons/kubecost/#gitops-configuration","text":"The following properties are made available for use when managing the add-on via GitOps. kubecost = { enable = true }","title":"GitOps Configuration"},{"location":"add-ons/kuberay-operator/","text":"KubeRay Operator \u00b6 KubeRay is an open source toolkit to run Ray applications on Kubernetes. For details on its design, please refer to the KubeRay documentation . \ud83d\uded1 This add-on should be considered as experimental and should only be used for proof of concept. Usage \u00b6 KubeRay operator can be deployed by enabling the add-on via the following. Basic Example \u00b6 enable_kuberay_operator = true Advanced Example \u00b6 Advanced example of KubeRay operator add-on is not currently supported as the upstream project does not publish a [Helm chart yet]. Please \ud83d\udc4d this issue . GitOps Configuration \u00b6 GitOps is not currently supported due to lack of a published Helm chart upstream. Please \ud83d\udc4d this issue .","title":"KubeRay Operator"},{"location":"add-ons/kuberay-operator/#kuberay-operator","text":"KubeRay is an open source toolkit to run Ray applications on Kubernetes. For details on its design, please refer to the KubeRay documentation . \ud83d\uded1 This add-on should be considered as experimental and should only be used for proof of concept.","title":"KubeRay Operator"},{"location":"add-ons/kuberay-operator/#usage","text":"KubeRay operator can be deployed by enabling the add-on via the following.","title":"Usage"},{"location":"add-ons/kuberay-operator/#basic-example","text":"enable_kuberay_operator = true","title":"Basic Example"},{"location":"add-ons/kuberay-operator/#advanced-example","text":"Advanced example of KubeRay operator add-on is not currently supported as the upstream project does not publish a [Helm chart yet]. Please \ud83d\udc4d this issue .","title":"Advanced Example"},{"location":"add-ons/kuberay-operator/#gitops-configuration","text":"GitOps is not currently supported due to lack of a published Helm chart upstream. Please \ud83d\udc4d this issue .","title":"GitOps Configuration"},{"location":"add-ons/kubernetes-dashboard/","text":"Kubernetes Dashboard \u00b6 Kubernetes Dashboard is a general purpose, web-based UI for Kubernetes clusters. It allows users to manage applications running in the cluster and troubleshoot them, as well as manage the cluster itself. Usage \u00b6 The following will deploy the Kubernetes Dashboard into an EKS Cluster. enable_kubernetes_dashboard = true Enable Kubernetes Dashboard with custom values.yaml enable_kubernetes_dashboard = true # Optional Map value kubernetes_dashboard_helm_config = { name = \"kubernetes-dashboard\" # (Required) Release name. repository = \"https://kubernetes.github.io/dashboard/\" # (Optional) Repository URL where to locate the requested chart. chart = \"kubernetes-dashboard\" # (Required) Chart name to be installed. version = \"5.2.0\" namespace = \"kube-system\" values = [templatefile(\"${path.module}/values.yaml\", {})] } GitOps Configuration \u00b6 The following properties are made available for use when managing the add-on via GitOps argocd_gitops_config = { enable = true serviceAccountName = local.service_account_name } Connecting to the Dashboard \u00b6 Follow the steps outlined here to connect to the dashboard","title":"Kubernetes Dashboard"},{"location":"add-ons/kubernetes-dashboard/#kubernetes-dashboard","text":"Kubernetes Dashboard is a general purpose, web-based UI for Kubernetes clusters. It allows users to manage applications running in the cluster and troubleshoot them, as well as manage the cluster itself.","title":"Kubernetes Dashboard"},{"location":"add-ons/kubernetes-dashboard/#usage","text":"The following will deploy the Kubernetes Dashboard into an EKS Cluster. enable_kubernetes_dashboard = true Enable Kubernetes Dashboard with custom values.yaml enable_kubernetes_dashboard = true # Optional Map value kubernetes_dashboard_helm_config = { name = \"kubernetes-dashboard\" # (Required) Release name. repository = \"https://kubernetes.github.io/dashboard/\" # (Optional) Repository URL where to locate the requested chart. chart = \"kubernetes-dashboard\" # (Required) Chart name to be installed. version = \"5.2.0\" namespace = \"kube-system\" values = [templatefile(\"${path.module}/values.yaml\", {})] }","title":"Usage"},{"location":"add-ons/kubernetes-dashboard/#gitops-configuration","text":"The following properties are made available for use when managing the add-on via GitOps argocd_gitops_config = { enable = true serviceAccountName = local.service_account_name }","title":"GitOps Configuration"},{"location":"add-ons/kubernetes-dashboard/#connecting-to-the-dashboard","text":"Follow the steps outlined here to connect to the dashboard","title":"Connecting to the Dashboard"},{"location":"add-ons/kyverno/","text":"Kyverno \u00b6 Kyverno is a policy engine that can help kubernetes clusters to enforce security and governance policies. This addon provides support for: 1. Kyverno 2. Kyverno policies 3. Kyverno policy reporter Usage \u00b6 Kyverno can be deployed by enabling the respective add-on(s) via the following. enable_kyverno = true enable_kyverno_policies = true enable_kyverno_policy_reporter = true GitOps Configuration \u00b6 The following properties are made available for use when managing the add-on via GitOps. kyverno = { enable = true } kyverno_policies = { enable = true } kyverno_policy_reporter = { enable = true }","title":"Kyverno"},{"location":"add-ons/kyverno/#kyverno","text":"Kyverno is a policy engine that can help kubernetes clusters to enforce security and governance policies. This addon provides support for: 1. Kyverno 2. Kyverno policies 3. Kyverno policy reporter","title":"Kyverno"},{"location":"add-ons/kyverno/#usage","text":"Kyverno can be deployed by enabling the respective add-on(s) via the following. enable_kyverno = true enable_kyverno_policies = true enable_kyverno_policy_reporter = true","title":"Usage"},{"location":"add-ons/kyverno/#gitops-configuration","text":"The following properties are made available for use when managing the add-on via GitOps. kyverno = { enable = true } kyverno_policies = { enable = true } kyverno_policy_reporter = { enable = true }","title":"GitOps Configuration"},{"location":"add-ons/local-volume-provisioner/","text":"Local volume provisioner \u00b6 Local volume provisioner manages PersistentVolume lifecycle for pre-allocated disks by detecting and creating PVs for each local disk on the host, and cleaning up the disks when released Usage \u00b6 Local volume provisioner can be deployed by enabling the add-on via the following. enable_local_volume_provisioner = true Deploy Local volume provisioner with custom values.yaml # Optional Map value; pass local-volume-provisioner-values.yaml from consumer module local_volume_provisioner_helm_config = { name = \"local-static-provisioner\" # (Required) Release name. namespace = \"local-static-provisioner\" # (Optional) The namespace to install the release into. values = [templatefile(\"${path.module}/local-volume-provisioner-values.yaml\", {})] }","title":"Local volume provisioner"},{"location":"add-ons/local-volume-provisioner/#local-volume-provisioner","text":"Local volume provisioner manages PersistentVolume lifecycle for pre-allocated disks by detecting and creating PVs for each local disk on the host, and cleaning up the disks when released","title":"Local volume provisioner"},{"location":"add-ons/local-volume-provisioner/#usage","text":"Local volume provisioner can be deployed by enabling the add-on via the following. enable_local_volume_provisioner = true Deploy Local volume provisioner with custom values.yaml # Optional Map value; pass local-volume-provisioner-values.yaml from consumer module local_volume_provisioner_helm_config = { name = \"local-static-provisioner\" # (Required) Release name. namespace = \"local-static-provisioner\" # (Optional) The namespace to install the release into. values = [templatefile(\"${path.module}/local-volume-provisioner-values.yaml\", {})] }","title":"Usage"},{"location":"add-ons/managed-add-ons/","text":"Amazon EKS Add-ons \u00b6 Amazon EKS add-ons provide installation and management of a curated set of add-ons for Amazon EKS clusters. All Amazon EKS add-ons include the latest security patches, bug fixes, and are validated by AWS to work with Amazon EKS. Amazon EKS add-ons allow you to consistently ensure that your Amazon EKS clusters are secure and stable and reduce the amount of work that you need to do in order to install, configure, and update add-ons. EKS currently provides support for the following managed add-ons. Name Description Amazon VPC CNI Native VPC networking for Kubernetes pods. CoreDNS A flexible, extensible DNS server that can serve as the Kubernetes cluster DNS. kube-proxy Enables network communication to your pods. Amazon EBS CSI Manage the Amazon EBS CSI driver as an Amazon EKS add-on. EKS managed add-ons can be enabled via the following. Note: EKS managed Add-ons can be converted to self-managed add-on with preserve field. preserve=true option removes Amazon EKS management of any settings and the ability for Amazon EKS to notify you of updates and automatically update the Amazon EKS add-on after you initiate an update, but preserves the add-on's software on your cluster. This option makes the add-on a self-managed add-on, rather than an Amazon EKS add-on. There is no downtime while deleting EKS managed Add-ons when preserve=true . This is a default option for enable_amazon_eks_vpc_cni , enable_amazon_eks_coredns and enable_amazon_eks_kube_proxy . Checkout this doc for more details. # EKS Addons enable_amazon_eks_vpc_cni = true # default is false #Optional amazon_eks_vpc_cni_config = { addon_name = \"vpc-cni\" addon_version = \"v1.11.2-eksbuild.1\" service_account = \"aws-node\" resolve_conflicts = \"OVERWRITE\" namespace = \"kube-system\" service_account_role_arn = \"\" preserve = true additional_iam_policies = [] tags = {} } enable_amazon_eks_coredns = true # default is false #Optional amazon_eks_coredns_config = { addon_name = \"coredns\" addon_version = \"v1.8.4-eksbuild.1\" service_account = \"coredns\" resolve_conflicts = \"OVERWRITE\" namespace = \"kube-system\" service_account_role_arn = \"\" preserve = true additional_iam_policies = [] tags = {} } enable_amazon_eks_kube_proxy = true # default is false #Optional amazon_eks_kube_proxy_config = { addon_name = \"kube-proxy\" addon_version = \"v1.21.2-eksbuild.2\" service_account = \"kube-proxy\" resolve_conflicts = \"OVERWRITE\" namespace = \"kube-system\" service_account_role_arn = \"\" preserve = true additional_iam_policies = [] tags = {} } enable_amazon_eks_aws_ebs_csi_driver = true # default is false #Optional amazon_eks_aws_ebs_csi_driver_config = { addon_name = \"aws-ebs-csi-driver\" addon_version = \"v1.4.0-eksbuild.preview\" service_account = \"ebs-csi-controller-sa\" resolve_conflicts = \"OVERWRITE\" namespace = \"kube-system\" additional_iam_policies = [] service_account_role_arn = \"\" tags = {} } Updating Managed Add-ons \u00b6 EKS won't modify any of your Kubernetes add-ons when you update a cluster to a newer Kubernetes version. As a result, it is important to upgrade EKS add-ons each time you upgrade an EKS cluster. Our Cluster Upgrade guide demonstrates how you can leverage this framework to upgrade your EKS cluster in addition to the EKS managed add-ons running in each cluster. Additional information on updating a EKS cluster can be found in the EKS documentation .","title":"Amazon EKS Add-ons"},{"location":"add-ons/managed-add-ons/#amazon-eks-add-ons","text":"Amazon EKS add-ons provide installation and management of a curated set of add-ons for Amazon EKS clusters. All Amazon EKS add-ons include the latest security patches, bug fixes, and are validated by AWS to work with Amazon EKS. Amazon EKS add-ons allow you to consistently ensure that your Amazon EKS clusters are secure and stable and reduce the amount of work that you need to do in order to install, configure, and update add-ons. EKS currently provides support for the following managed add-ons. Name Description Amazon VPC CNI Native VPC networking for Kubernetes pods. CoreDNS A flexible, extensible DNS server that can serve as the Kubernetes cluster DNS. kube-proxy Enables network communication to your pods. Amazon EBS CSI Manage the Amazon EBS CSI driver as an Amazon EKS add-on. EKS managed add-ons can be enabled via the following. Note: EKS managed Add-ons can be converted to self-managed add-on with preserve field. preserve=true option removes Amazon EKS management of any settings and the ability for Amazon EKS to notify you of updates and automatically update the Amazon EKS add-on after you initiate an update, but preserves the add-on's software on your cluster. This option makes the add-on a self-managed add-on, rather than an Amazon EKS add-on. There is no downtime while deleting EKS managed Add-ons when preserve=true . This is a default option for enable_amazon_eks_vpc_cni , enable_amazon_eks_coredns and enable_amazon_eks_kube_proxy . Checkout this doc for more details. # EKS Addons enable_amazon_eks_vpc_cni = true # default is false #Optional amazon_eks_vpc_cni_config = { addon_name = \"vpc-cni\" addon_version = \"v1.11.2-eksbuild.1\" service_account = \"aws-node\" resolve_conflicts = \"OVERWRITE\" namespace = \"kube-system\" service_account_role_arn = \"\" preserve = true additional_iam_policies = [] tags = {} } enable_amazon_eks_coredns = true # default is false #Optional amazon_eks_coredns_config = { addon_name = \"coredns\" addon_version = \"v1.8.4-eksbuild.1\" service_account = \"coredns\" resolve_conflicts = \"OVERWRITE\" namespace = \"kube-system\" service_account_role_arn = \"\" preserve = true additional_iam_policies = [] tags = {} } enable_amazon_eks_kube_proxy = true # default is false #Optional amazon_eks_kube_proxy_config = { addon_name = \"kube-proxy\" addon_version = \"v1.21.2-eksbuild.2\" service_account = \"kube-proxy\" resolve_conflicts = \"OVERWRITE\" namespace = \"kube-system\" service_account_role_arn = \"\" preserve = true additional_iam_policies = [] tags = {} } enable_amazon_eks_aws_ebs_csi_driver = true # default is false #Optional amazon_eks_aws_ebs_csi_driver_config = { addon_name = \"aws-ebs-csi-driver\" addon_version = \"v1.4.0-eksbuild.preview\" service_account = \"ebs-csi-controller-sa\" resolve_conflicts = \"OVERWRITE\" namespace = \"kube-system\" additional_iam_policies = [] service_account_role_arn = \"\" tags = {} }","title":"Amazon EKS Add-ons"},{"location":"add-ons/managed-add-ons/#updating-managed-add-ons","text":"EKS won't modify any of your Kubernetes add-ons when you update a cluster to a newer Kubernetes version. As a result, it is important to upgrade EKS add-ons each time you upgrade an EKS cluster. Our Cluster Upgrade guide demonstrates how you can leverage this framework to upgrade your EKS cluster in addition to the EKS managed add-ons running in each cluster. Additional information on updating a EKS cluster can be found in the EKS documentation .","title":"Updating Managed Add-ons"},{"location":"add-ons/metrics-server/","text":"Metrics Server \u00b6 Metrics Server is a scalable, efficient source of container resource metrics for Kubernetes built-in autoscaling pipelines. It is not deployed by default in Amazon EKS clusters. The Metrics Server is commonly used by other Kubernetes add-ons, such as the Horizontal Pod Autoscaler, Vertical Autoscaling or the Kubernetes Dashboard. Important : Don't use Metrics Server when you need an accurate source of resource usage metrics or as a monitoring solution. Usage \u00b6 Metrics Server can be deployed by enabling the add-on via the following. enable_metrics_server = true Once deployed, you can see metrics-server pod in the kube-system namespace. $ kubectl get deployments -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE metrics-server 1 /1 1 1 20m GitOps Configuration \u00b6 The following properties are made available for use when managing the add-on via GitOps metricsServer = { enable = true }","title":"Metrics Server"},{"location":"add-ons/metrics-server/#metrics-server","text":"Metrics Server is a scalable, efficient source of container resource metrics for Kubernetes built-in autoscaling pipelines. It is not deployed by default in Amazon EKS clusters. The Metrics Server is commonly used by other Kubernetes add-ons, such as the Horizontal Pod Autoscaler, Vertical Autoscaling or the Kubernetes Dashboard. Important : Don't use Metrics Server when you need an accurate source of resource usage metrics or as a monitoring solution.","title":"Metrics Server"},{"location":"add-ons/metrics-server/#usage","text":"Metrics Server can be deployed by enabling the add-on via the following. enable_metrics_server = true Once deployed, you can see metrics-server pod in the kube-system namespace. $ kubectl get deployments -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE metrics-server 1 /1 1 1 20m","title":"Usage"},{"location":"add-ons/metrics-server/#gitops-configuration","text":"The following properties are made available for use when managing the add-on via GitOps metricsServer = { enable = true }","title":"GitOps Configuration"},{"location":"add-ons/nginx/","text":"Nginx \u00b6 This add-on installs Nginx Ingress Controller on Amazon EKS. The Nginx ingress controller uses Nginx as a reverse proxy and load balancer. Other than handling Kubernetes ingress objects, this ingress controller can facilitate multi-tenancy and segregation of workload ingresses based on host name (host-based routing) and/or URL Path (path based routing). Usage \u00b6 Nginx Ingress Controller can be deployed by enabling the add-on via the following. Check out the full example to deploy the EKS Cluster with Nginx Ingress Controller. enable_ingress_nginx = true To validate that installation is successful run the following command: $ kubectl get po -n kube-system NAME READY STATUS RESTARTS AGE eks-blueprints-addon-ingress-nginx-78b8567p4q6 1 /1 Running 0 4d10h Note that the ingress controller is deployed in the ingress-nginx namespace. You can optionally customize the Helm chart that deploys nginx via the following configuration. enable_ingress_nginx = true # Optional ingress_nginx_helm_config ingress_nginx_helm_config = { repository = \"https://kubernetes.github.io/ingress-nginx\" version = \"4.0.17\" values = [file(\"${path.module}/values.yaml\")] } nginx_irsa_policies = [] # Optional to add additional policies to IRSA GitOps Configuration \u00b6 The following properties are made available for use when managing the add-on via GitOps. Refer to locals.tf for latest config. GitOps with ArgoCD Add-on repo is located here argocd_gitops_config = { enable = true serviceAccountName = local.service_account_name }","title":"Nginx"},{"location":"add-ons/nginx/#nginx","text":"This add-on installs Nginx Ingress Controller on Amazon EKS. The Nginx ingress controller uses Nginx as a reverse proxy and load balancer. Other than handling Kubernetes ingress objects, this ingress controller can facilitate multi-tenancy and segregation of workload ingresses based on host name (host-based routing) and/or URL Path (path based routing).","title":"Nginx"},{"location":"add-ons/nginx/#usage","text":"Nginx Ingress Controller can be deployed by enabling the add-on via the following. Check out the full example to deploy the EKS Cluster with Nginx Ingress Controller. enable_ingress_nginx = true To validate that installation is successful run the following command: $ kubectl get po -n kube-system NAME READY STATUS RESTARTS AGE eks-blueprints-addon-ingress-nginx-78b8567p4q6 1 /1 Running 0 4d10h Note that the ingress controller is deployed in the ingress-nginx namespace. You can optionally customize the Helm chart that deploys nginx via the following configuration. enable_ingress_nginx = true # Optional ingress_nginx_helm_config ingress_nginx_helm_config = { repository = \"https://kubernetes.github.io/ingress-nginx\" version = \"4.0.17\" values = [file(\"${path.module}/values.yaml\")] } nginx_irsa_policies = [] # Optional to add additional policies to IRSA","title":"Usage"},{"location":"add-ons/nginx/#gitops-configuration","text":"The following properties are made available for use when managing the add-on via GitOps. Refer to locals.tf for latest config. GitOps with ArgoCD Add-on repo is located here argocd_gitops_config = { enable = true serviceAccountName = local.service_account_name }","title":"GitOps Configuration"},{"location":"add-ons/nvidia-device-plugin/","text":"NVIDIA Device Plugin \u00b6 The NVIDIA device plugin for Kubernetes is a Daemonset that allows you to automatically: Expose the number of GPUs on each nodes of your cluster Keep track of the health of your GPUs Run GPU enabled containers in your Kubernetes cluster. For complete project documentation, please visit the NVIDIA Device Plugin . Additionally, refer to this AWS blog for more information on how the add-on can be tested. Usage \u00b6 NVIDIA device plugin can be deployed by enabling the add-on via the following. enable_nvidia_device_plugin = true You can optionally customize the Helm chart via the following configuration. enable_nvidia_device_plugin = true # Optional nvidia_device_plugin_helm_config nvidia_device_plugin_helm_config = { name = \"nvidia-device-plugin\" chart = \"nvidia-device-plugin\" repository = \"https://nvidia.github.io/k8s-device-plugin\" version = \"0.12.3\" namespace = \"nvidia-device-plugin\" values = [templatefile(\"${path.module}/values.yaml\", { ... })] } GitOps Configuration \u00b6 The following properties are made available for use when managing the add-on via GitOps. Refer to locals.tf for latest config. GitOps with ArgoCD Add-on repo is located here argocd_gitops_config = { enable = true }","title":"NVIDIA Device Plugin"},{"location":"add-ons/nvidia-device-plugin/#nvidia-device-plugin","text":"The NVIDIA device plugin for Kubernetes is a Daemonset that allows you to automatically: Expose the number of GPUs on each nodes of your cluster Keep track of the health of your GPUs Run GPU enabled containers in your Kubernetes cluster. For complete project documentation, please visit the NVIDIA Device Plugin . Additionally, refer to this AWS blog for more information on how the add-on can be tested.","title":"NVIDIA Device Plugin"},{"location":"add-ons/nvidia-device-plugin/#usage","text":"NVIDIA device plugin can be deployed by enabling the add-on via the following. enable_nvidia_device_plugin = true You can optionally customize the Helm chart via the following configuration. enable_nvidia_device_plugin = true # Optional nvidia_device_plugin_helm_config nvidia_device_plugin_helm_config = { name = \"nvidia-device-plugin\" chart = \"nvidia-device-plugin\" repository = \"https://nvidia.github.io/k8s-device-plugin\" version = \"0.12.3\" namespace = \"nvidia-device-plugin\" values = [templatefile(\"${path.module}/values.yaml\", { ... })] }","title":"Usage"},{"location":"add-ons/nvidia-device-plugin/#gitops-configuration","text":"The following properties are made available for use when managing the add-on via GitOps. Refer to locals.tf for latest config. GitOps with ArgoCD Add-on repo is located here argocd_gitops_config = { enable = true }","title":"GitOps Configuration"},{"location":"add-ons/prometheus/","text":"Prometheus \u00b6 Prometheus is an open source monitoring and alerting service. Prometheus joined the Cloud Native Computing Foundation in 2016 as the second hosted project, after Kubernetes. This project provides support for installing a open source Prometheus server in your EKS cluster and for deploying a new Prometheus instance via Amazon Managed Service for Prometheus . Usage \u00b6 The following will deploy the Prometheus server into an EKS Cluster and provision a new Amazon Managed Service for Prometheus instance. # Creates the AMP workspace and all the relevant IAM Roles enable_amazon_prometheus = true # Deploys Prometheus server with remote write to AWS AMP Workspace enable_prometheus = true Enable Prometheus with custom values.yaml #--------------------------------------- # Prometheus Server integration with Amazon Prometheus #--------------------------------------- # Amazon Prometheus Configuration to integrate with Prometheus Server Add-on enable_amazon_prometheus = true amazon_prometheus_workspace_endpoint = \"<Enter Amazon Workspace Endpoint>\" enable_prometheus = true # Optional Map value prometheus_helm_config = { name = \"prometheus\" # (Required) Release name. repository = \"https://prometheus-community.github.io/helm-charts\" # (Optional) Repository URL where to locate the requested chart. chart = \"prometheus\" # (Required) Chart name to be installed. version = \"15.3.0\" # (Optional) Specify the exact chart version to install. If this is not specified, it defaults to the version set within default_helm_config: https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/main/modules/kubernetes-addons/prometheus/locals.tf namespace = \"prometheus\" # (Optional) The namespace to install the release into. values = [templatefile(\"${path.module}/prometheus-values.yaml\", { operating_system = \"linux\" })] } GitOps Configuration \u00b6 The following properties are made available for use when managing the add-on via GitOps prometheus = { enable = true ampWorkspaceUrl = \"<workspace_url>\" roleArn = \"<role_arn>\" serviceAccountName = \"<service_account_name>\" }","title":"Prometheus"},{"location":"add-ons/prometheus/#prometheus","text":"Prometheus is an open source monitoring and alerting service. Prometheus joined the Cloud Native Computing Foundation in 2016 as the second hosted project, after Kubernetes. This project provides support for installing a open source Prometheus server in your EKS cluster and for deploying a new Prometheus instance via Amazon Managed Service for Prometheus .","title":"Prometheus"},{"location":"add-ons/prometheus/#usage","text":"The following will deploy the Prometheus server into an EKS Cluster and provision a new Amazon Managed Service for Prometheus instance. # Creates the AMP workspace and all the relevant IAM Roles enable_amazon_prometheus = true # Deploys Prometheus server with remote write to AWS AMP Workspace enable_prometheus = true Enable Prometheus with custom values.yaml #--------------------------------------- # Prometheus Server integration with Amazon Prometheus #--------------------------------------- # Amazon Prometheus Configuration to integrate with Prometheus Server Add-on enable_amazon_prometheus = true amazon_prometheus_workspace_endpoint = \"<Enter Amazon Workspace Endpoint>\" enable_prometheus = true # Optional Map value prometheus_helm_config = { name = \"prometheus\" # (Required) Release name. repository = \"https://prometheus-community.github.io/helm-charts\" # (Optional) Repository URL where to locate the requested chart. chart = \"prometheus\" # (Required) Chart name to be installed. version = \"15.3.0\" # (Optional) Specify the exact chart version to install. If this is not specified, it defaults to the version set within default_helm_config: https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/main/modules/kubernetes-addons/prometheus/locals.tf namespace = \"prometheus\" # (Optional) The namespace to install the release into. values = [templatefile(\"${path.module}/prometheus-values.yaml\", { operating_system = \"linux\" })] }","title":"Usage"},{"location":"add-ons/prometheus/#gitops-configuration","text":"The following properties are made available for use when managing the add-on via GitOps prometheus = { enable = true ampWorkspaceUrl = \"<workspace_url>\" roleArn = \"<role_arn>\" serviceAccountName = \"<service_account_name>\" }","title":"GitOps Configuration"},{"location":"add-ons/promtail/","text":"Promtail \u00b6 Promtail is an agent which ships the contents of local logs to a Loki instance. Promtail chart bootstraps Promtail infrastructure on a Kubernetes cluster using the Helm package manager. For complete project documentation, please visit the Promtail documentation site . Usage \u00b6 Promtail can be deployed by enabling the add-on via the following. enable_promtail = true Deploy Promtail with custom values.yaml # Optional Map value; pass promtail-values.yaml from consumer module promtail_helm_config = { name = \"promtail\" # (Required) Release name. repository = \"https://grafana.github.io/helm-charts\" # (Optional) Repository URL where to locate the requested chart. chart = \"promtail\" # (Required) Chart name to be installed. version = \"6.3.0\" # (Optional) Specify the exact chart version to install. If this is not specified, it defaults to the version set within default_helm_config: https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/main/modules/kubernetes-addons/promtail/locals.tf namespace = \"promtail\" # (Optional) The namespace to install the release into. values = [templatefile(\"${path.module}/promtail-values.yaml\", {})] } GitOps Configuration \u00b6 The following properties are made available for use when managing the add-on via GitOps. promtail = { enable = true }","title":"Promtail"},{"location":"add-ons/promtail/#promtail","text":"Promtail is an agent which ships the contents of local logs to a Loki instance. Promtail chart bootstraps Promtail infrastructure on a Kubernetes cluster using the Helm package manager. For complete project documentation, please visit the Promtail documentation site .","title":"Promtail"},{"location":"add-ons/promtail/#usage","text":"Promtail can be deployed by enabling the add-on via the following. enable_promtail = true Deploy Promtail with custom values.yaml # Optional Map value; pass promtail-values.yaml from consumer module promtail_helm_config = { name = \"promtail\" # (Required) Release name. repository = \"https://grafana.github.io/helm-charts\" # (Optional) Repository URL where to locate the requested chart. chart = \"promtail\" # (Required) Chart name to be installed. version = \"6.3.0\" # (Optional) Specify the exact chart version to install. If this is not specified, it defaults to the version set within default_helm_config: https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/main/modules/kubernetes-addons/promtail/locals.tf namespace = \"promtail\" # (Optional) The namespace to install the release into. values = [templatefile(\"${path.module}/promtail-values.yaml\", {})] }","title":"Usage"},{"location":"add-ons/promtail/#gitops-configuration","text":"The following properties are made available for use when managing the add-on via GitOps. promtail = { enable = true }","title":"GitOps Configuration"},{"location":"add-ons/secrets-store-csi-driver/","text":"secrets-store-csi-driver \u00b6 Secrets Store CSI Driver for Kubernetes secrets - Integrates secrets stores with Kubernetes via a Container Storage Interface (CSI) volume. The Secrets Store CSI Driver secrets-store.csi.k8s.io allows Kubernetes to mount multiple secrets, keys, and certs stored in enterprise-grade external secrets stores into their pods as a volume. Once the Volume is attached, the data in it is mounted into the container\u2019s file system. For more details, refer Secrets Store CSI Driver Usage \u00b6 secrets-store-csi-driver can be deployed by enabling the add-ons via the following. enable_secrets_store_csi_driver = true","title":"secrets-store-csi-driver"},{"location":"add-ons/secrets-store-csi-driver/#secrets-store-csi-driver","text":"Secrets Store CSI Driver for Kubernetes secrets - Integrates secrets stores with Kubernetes via a Container Storage Interface (CSI) volume. The Secrets Store CSI Driver secrets-store.csi.k8s.io allows Kubernetes to mount multiple secrets, keys, and certs stored in enterprise-grade external secrets stores into their pods as a volume. Once the Volume is attached, the data in it is mounted into the container\u2019s file system. For more details, refer Secrets Store CSI Driver","title":"secrets-store-csi-driver"},{"location":"add-ons/secrets-store-csi-driver/#usage","text":"secrets-store-csi-driver can be deployed by enabling the add-ons via the following. enable_secrets_store_csi_driver = true","title":"Usage"},{"location":"add-ons/smb-csi-driver/","text":"SMB CSI Driver Helm Chart \u00b6 SMB CSI Driver allows Kubernetes to access SMB server on both Linux and Windows nodes. The driver requires existing and already configured SMB server, it supports dynamic provisioning of Persistent Volumes via Persistent Volume Claims by creating a new subdirectory under SMB server. SMB CSI Driver docs chart bootstraps SMB CSI Driver infrastructure on a Kubernetes cluster using the Helm package manager. For complete project documentation, please visit the SMB CSI Driver documentation site . Usage \u00b6 SMB CSI Driver can be deployed by enabling the add-on via the following. enable_smb_csi_driver = true Deploy SMB CSI Driver with custom values.yaml # Optional Map value; pass smb-csi-driver-values.yaml from consumer module smb_csi_driver_helm_config = { name = \"csi-driver-smb\" # (Required) Release name. repository = \"https://raw.githubusercontent.com/kubernetes-csi/csi-driver-smb/master/charts\" # (Optional) Repository URL where to locate the requested chart. chart = \"csi-driver-smb\" # (Required) Chart name to be installed. version = \"v1.9.0\" # (Optional) Specify the exact chart version to install. If this is not specified, it defaults to the version set within default_helm_config: https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/main/modules/kubernetes-addons/smb-csi-driver/locals.tf values = [templatefile(\"${path.module}/smb-csi-driver-values.yaml\", {})] } GitOps Configuration \u00b6 The following properties are made available for use when managing the add-on via GitOps. smbCsiDriver = { enable = true }","title":"SMB CSI Driver Helm Chart"},{"location":"add-ons/smb-csi-driver/#smb-csi-driver-helm-chart","text":"SMB CSI Driver allows Kubernetes to access SMB server on both Linux and Windows nodes. The driver requires existing and already configured SMB server, it supports dynamic provisioning of Persistent Volumes via Persistent Volume Claims by creating a new subdirectory under SMB server. SMB CSI Driver docs chart bootstraps SMB CSI Driver infrastructure on a Kubernetes cluster using the Helm package manager. For complete project documentation, please visit the SMB CSI Driver documentation site .","title":"SMB CSI Driver Helm Chart"},{"location":"add-ons/smb-csi-driver/#usage","text":"SMB CSI Driver can be deployed by enabling the add-on via the following. enable_smb_csi_driver = true Deploy SMB CSI Driver with custom values.yaml # Optional Map value; pass smb-csi-driver-values.yaml from consumer module smb_csi_driver_helm_config = { name = \"csi-driver-smb\" # (Required) Release name. repository = \"https://raw.githubusercontent.com/kubernetes-csi/csi-driver-smb/master/charts\" # (Optional) Repository URL where to locate the requested chart. chart = \"csi-driver-smb\" # (Required) Chart name to be installed. version = \"v1.9.0\" # (Optional) Specify the exact chart version to install. If this is not specified, it defaults to the version set within default_helm_config: https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/main/modules/kubernetes-addons/smb-csi-driver/locals.tf values = [templatefile(\"${path.module}/smb-csi-driver-values.yaml\", {})] }","title":"Usage"},{"location":"add-ons/smb-csi-driver/#gitops-configuration","text":"The following properties are made available for use when managing the add-on via GitOps. smbCsiDriver = { enable = true }","title":"GitOps Configuration"},{"location":"add-ons/spark-history-server/","text":"Spark History Server \u00b6 Spark Web UI can be enabled by this Add-on. This Add-on deploys Spark History Server and fetches the Spark Event logs stored in S3. Spark Web UI can be exposed via Ingress and LoadBalancer with values.yaml . Alternatively, you can port-forward on spark-history-server service. e.g., kubectl port-forward services/spark-history-server 18085:80 -n spark-history-server Usage \u00b6 Spark History Server can be deployed by enabling the add-on via the following. Basic Example \u00b6 enable_spark_history_server = true spark_history_server_s3a_path = \"s3a://<ENTER_S3_BUCKET_NAME>/<PREFIX_FOR_SPARK_EVENT_LOGS>/\" Advanced Example \u00b6 enable_spark_history_server = true # IAM policy used by IRSA role. It's recommended to create a dedicated IAM policy to access your s3 bucket spark_history_server_irsa_policies = [\"<IRSA_POLICY_ARN>\"] # NOTE: This block requires passing the helm values.yaml # spark_history_server_s3a_path won't be used when you pass custom `values.yaml`. s3a path is passed via `sparkHistoryOpts` in `values.yaml` spark_history_server_helm_config = { name = \"spark-history-server\" chart = \"spark-history-server\" repository = \"https://hyper-mesh.github.io/spark-history-server\" version = \"1.0.0\" namespace = \"spark-history-server\" timeout = \"300\" values = [ <<-EOT serviceAccount: create: false # Enter S3 bucket with Spark Event logs location. # Ensure IRSA roles has permissions to read the files for the given S3 bucket sparkHistoryOpts: \"-Dspark.history.fs.logDirectory=s3a://<ENTER_S3_BUCKET_NAME>/<PREFIX_FOR_SPARK_EVENT_LOGS>/\" # Update spark conf according to your needs sparkConf: |- spark.hadoop.fs.s3a.aws.credentials.provider=com.amazonaws.auth.WebIdentityTokenCredentialsProvider spark.history.fs.eventLog.rolling.maxFilesToRetain=5 spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem spark.eventLog.enabled=true spark.history.ui.port=18080 resources: limits: cpu: 200m memory: 2G requests: cpu: 100m memory: 1G EOT ] } GitOps Configuration \u00b6 The following properties are made available for use when managing the add-on via GitOps sparkHistoryServer = { enable = true }","title":"Spark History Server"},{"location":"add-ons/spark-history-server/#spark-history-server","text":"Spark Web UI can be enabled by this Add-on. This Add-on deploys Spark History Server and fetches the Spark Event logs stored in S3. Spark Web UI can be exposed via Ingress and LoadBalancer with values.yaml . Alternatively, you can port-forward on spark-history-server service. e.g., kubectl port-forward services/spark-history-server 18085:80 -n spark-history-server","title":"Spark History Server"},{"location":"add-ons/spark-history-server/#usage","text":"Spark History Server can be deployed by enabling the add-on via the following.","title":"Usage"},{"location":"add-ons/spark-history-server/#basic-example","text":"enable_spark_history_server = true spark_history_server_s3a_path = \"s3a://<ENTER_S3_BUCKET_NAME>/<PREFIX_FOR_SPARK_EVENT_LOGS>/\"","title":"Basic Example"},{"location":"add-ons/spark-history-server/#advanced-example","text":"enable_spark_history_server = true # IAM policy used by IRSA role. It's recommended to create a dedicated IAM policy to access your s3 bucket spark_history_server_irsa_policies = [\"<IRSA_POLICY_ARN>\"] # NOTE: This block requires passing the helm values.yaml # spark_history_server_s3a_path won't be used when you pass custom `values.yaml`. s3a path is passed via `sparkHistoryOpts` in `values.yaml` spark_history_server_helm_config = { name = \"spark-history-server\" chart = \"spark-history-server\" repository = \"https://hyper-mesh.github.io/spark-history-server\" version = \"1.0.0\" namespace = \"spark-history-server\" timeout = \"300\" values = [ <<-EOT serviceAccount: create: false # Enter S3 bucket with Spark Event logs location. # Ensure IRSA roles has permissions to read the files for the given S3 bucket sparkHistoryOpts: \"-Dspark.history.fs.logDirectory=s3a://<ENTER_S3_BUCKET_NAME>/<PREFIX_FOR_SPARK_EVENT_LOGS>/\" # Update spark conf according to your needs sparkConf: |- spark.hadoop.fs.s3a.aws.credentials.provider=com.amazonaws.auth.WebIdentityTokenCredentialsProvider spark.history.fs.eventLog.rolling.maxFilesToRetain=5 spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem spark.eventLog.enabled=true spark.history.ui.port=18080 resources: limits: cpu: 200m memory: 2G requests: cpu: 100m memory: 1G EOT ] }","title":"Advanced Example"},{"location":"add-ons/spark-history-server/#gitops-configuration","text":"The following properties are made available for use when managing the add-on via GitOps sparkHistoryServer = { enable = true }","title":"GitOps Configuration"},{"location":"add-ons/spark-on-k8s-operator/","text":"Spark K8S Operator \u00b6 The Kubernetes Operator for Apache Spark aims to make specifying and running Spark applications as easy and idiomatic as running other workloads on Kubernetes. It uses Kubernetes custom resources for specifying, running, and surfacing status of Spark applications. For a complete reference of the custom resource definitions, please refer to the API Definition. For details on its design, please refer to the design doc. It requires Spark 2.3 and above that supports Kubernetes as a native scheduler backend. For complete project documentation, please visit the Spark K8S Operator documentation site . Usage \u00b6 Spark K8S Operator can be deployed by enabling the add-on via the following. Basic Example \u00b6 enable_spark_k8s_operator = true Advanced Example \u00b6 enable_spark_k8s_operator = true # Optional Map value # NOTE: This block requires passing the helm values.yaml spark_k8s_operator_helm_config = { name = \"spark-operator\" chart = \"spark-operator\" repository = \"https://googlecloudplatform.github.io/spark-on-k8s-operator\" version = \"1.1.19\" namespace = \"spark-k8s-operator\" timeout = \"1200\" create_namespace = true values = [templatefile(\"${path.module}/values.yaml\", {})] } GitOps Configuration \u00b6 The following properties are made available for use when managing the add-on via GitOps sparkK8sOperator = { enable = true }","title":"Spark K8S Operator"},{"location":"add-ons/spark-on-k8s-operator/#spark-k8s-operator","text":"The Kubernetes Operator for Apache Spark aims to make specifying and running Spark applications as easy and idiomatic as running other workloads on Kubernetes. It uses Kubernetes custom resources for specifying, running, and surfacing status of Spark applications. For a complete reference of the custom resource definitions, please refer to the API Definition. For details on its design, please refer to the design doc. It requires Spark 2.3 and above that supports Kubernetes as a native scheduler backend. For complete project documentation, please visit the Spark K8S Operator documentation site .","title":"Spark K8S Operator"},{"location":"add-ons/spark-on-k8s-operator/#usage","text":"Spark K8S Operator can be deployed by enabling the add-on via the following.","title":"Usage"},{"location":"add-ons/spark-on-k8s-operator/#basic-example","text":"enable_spark_k8s_operator = true","title":"Basic Example"},{"location":"add-ons/spark-on-k8s-operator/#advanced-example","text":"enable_spark_k8s_operator = true # Optional Map value # NOTE: This block requires passing the helm values.yaml spark_k8s_operator_helm_config = { name = \"spark-operator\" chart = \"spark-operator\" repository = \"https://googlecloudplatform.github.io/spark-on-k8s-operator\" version = \"1.1.19\" namespace = \"spark-k8s-operator\" timeout = \"1200\" create_namespace = true values = [templatefile(\"${path.module}/values.yaml\", {})] }","title":"Advanced Example"},{"location":"add-ons/spark-on-k8s-operator/#gitops-configuration","text":"The following properties are made available for use when managing the add-on via GitOps sparkK8sOperator = { enable = true }","title":"GitOps Configuration"},{"location":"add-ons/tetrate-istio/","text":"Tetrate Istio Distro \u00b6 Tetrate Istio Distro is simple, safe enterprise-grade Istio distro. This add-on is implemented as an external add-on. For detailed documentation and usage of the add-on please refer to the add-on repository . Example \u00b6 Checkout the full example . Usage \u00b6 This step deploys the Tetrate Istio Distro with default Helm Chart config enable_tetrate_istio = true Alternatively, you can override the helm values by using the code snippet below enable_tetrate_istio = true # Optional fine-grained configuration tetrate_istio_distribution = \"TID\" # (default, Tetrate Istio Distro) tetrate_istio_version = \"1.12.2\" tetrate_istio_install_base = \"true\" # (default, Istio `base` Helm Chart) tetrate_istio_install_cni = \"true\" # (default, Istio `cni` Helm Chart) tetrate_istio_install_istiod = \"true\" # (default, Istio `istiod` Helm Chart) tetrate_istio_install_gateway = \"true\" # (default, Istio `gateway` Helm Chart) # Istio `base` Helm Chart config tetrate_istio_base_helm_config = { name = \"istio-base\" # (default) Release name. repository = \"https://istio-release.storage.googleapis.com/charts\" # (default) Repository URL where to locate the requested chart. chart = \"base\" # (default) Chart name to be installed. version = \"1.12.2\" # (default) The exact chart version to install. values = [] } # Istio `cni` Helm Chart config tetrate_istio_cni_helm_config = { name = \"istio-cni\" # (default) Release name. repository = \"https://istio-release.storage.googleapis.com/charts\" # (default) Repository URL where to locate the requested chart. chart = \"cni\" # (default) Chart name to be installed. version = \"1.12.2\" # (default) The exact chart version to install. values = [yamlencode({ \"global\" : { \"hub\" : \"containers.istio.tetratelabs.com\", \"tag\" : \"1.12.2-tetratefips-v0\", } })] } # Istio `istiod` Helm Chart config tetrate_istio_istiod_helm_config = { name = \"istio-istiod\" # (default) Release name. repository = \"https://istio-release.storage.googleapis.com/charts\" # (default) Repository URL where to locate the requested chart. chart = \"istiod\" # (default) Chart name to be installed. version = \"1.12.2\" # (default) The exact chart version to install. values = [yamlencode({ \"global\" : { \"hub\" : \"containers.istio.tetratelabs.com\", \"tag\" : \"1.12.2-tetratefips-v0\", } })] } # Istio `gateway` Helm Chart config tetrate_istio_gateway_helm_config = { name = \"istio-ingress\" # (default) Release name. repository = \"https://istio-release.storage.googleapis.com/charts\" # (default) Repository URL where to locate the requested chart. chart = \"gateway\" # (default) Chart name to be installed. version = \"1.12.2\" # (default) The exact chart version to install. values = [] } GitOps Configuration \u00b6 The following properties are made available for use when managing the add-on via GitOps tetrateIstio = { enable = true } GitOps with ArgoCD Add-on repo is located here","title":"Tetrate Istio Distro"},{"location":"add-ons/tetrate-istio/#tetrate-istio-distro","text":"Tetrate Istio Distro is simple, safe enterprise-grade Istio distro. This add-on is implemented as an external add-on. For detailed documentation and usage of the add-on please refer to the add-on repository .","title":"Tetrate Istio Distro"},{"location":"add-ons/tetrate-istio/#example","text":"Checkout the full example .","title":"Example"},{"location":"add-ons/tetrate-istio/#usage","text":"This step deploys the Tetrate Istio Distro with default Helm Chart config enable_tetrate_istio = true Alternatively, you can override the helm values by using the code snippet below enable_tetrate_istio = true # Optional fine-grained configuration tetrate_istio_distribution = \"TID\" # (default, Tetrate Istio Distro) tetrate_istio_version = \"1.12.2\" tetrate_istio_install_base = \"true\" # (default, Istio `base` Helm Chart) tetrate_istio_install_cni = \"true\" # (default, Istio `cni` Helm Chart) tetrate_istio_install_istiod = \"true\" # (default, Istio `istiod` Helm Chart) tetrate_istio_install_gateway = \"true\" # (default, Istio `gateway` Helm Chart) # Istio `base` Helm Chart config tetrate_istio_base_helm_config = { name = \"istio-base\" # (default) Release name. repository = \"https://istio-release.storage.googleapis.com/charts\" # (default) Repository URL where to locate the requested chart. chart = \"base\" # (default) Chart name to be installed. version = \"1.12.2\" # (default) The exact chart version to install. values = [] } # Istio `cni` Helm Chart config tetrate_istio_cni_helm_config = { name = \"istio-cni\" # (default) Release name. repository = \"https://istio-release.storage.googleapis.com/charts\" # (default) Repository URL where to locate the requested chart. chart = \"cni\" # (default) Chart name to be installed. version = \"1.12.2\" # (default) The exact chart version to install. values = [yamlencode({ \"global\" : { \"hub\" : \"containers.istio.tetratelabs.com\", \"tag\" : \"1.12.2-tetratefips-v0\", } })] } # Istio `istiod` Helm Chart config tetrate_istio_istiod_helm_config = { name = \"istio-istiod\" # (default) Release name. repository = \"https://istio-release.storage.googleapis.com/charts\" # (default) Repository URL where to locate the requested chart. chart = \"istiod\" # (default) Chart name to be installed. version = \"1.12.2\" # (default) The exact chart version to install. values = [yamlencode({ \"global\" : { \"hub\" : \"containers.istio.tetratelabs.com\", \"tag\" : \"1.12.2-tetratefips-v0\", } })] } # Istio `gateway` Helm Chart config tetrate_istio_gateway_helm_config = { name = \"istio-ingress\" # (default) Release name. repository = \"https://istio-release.storage.googleapis.com/charts\" # (default) Repository URL where to locate the requested chart. chart = \"gateway\" # (default) Chart name to be installed. version = \"1.12.2\" # (default) The exact chart version to install. values = [] }","title":"Usage"},{"location":"add-ons/tetrate-istio/#gitops-configuration","text":"The following properties are made available for use when managing the add-on via GitOps tetrateIstio = { enable = true } GitOps with ArgoCD Add-on repo is located here","title":"GitOps Configuration"},{"location":"add-ons/traefik/","text":"Traefik \u00b6 Traefik is an open-source Edge Router that makes publishing your services a fun and easy experience. It receives requests on behalf of your system and finds out which components are responsible for handling them. For complete project documentation, please visit the Traefik documentation site . Usage \u00b6 Traefik can be deployed by enabling the add-on via the following. enable_traefik = true How to test Traefik Web UI \u00b6 Once the Traefik deployment is successful, run the following command from your a local machine which have access to an EKS cluster using kubectl. $ kubectl port-forward svc/traefik -n kube-system 9000:9000 Now open the browser from your machine and enter the below URL to access Traefik Web UI. http://127.0.0.1:9000/dashboard/ AWS Service annotations for Traefik Ingress Controller \u00b6 Here is the link to get the AWS ELB service annotations for Traefik Ingress controller GitOps Configuration \u00b6 The following properties are made available for use when managing the add-on via GitOps traefik = { enable = true }","title":"Traefik"},{"location":"add-ons/traefik/#traefik","text":"Traefik is an open-source Edge Router that makes publishing your services a fun and easy experience. It receives requests on behalf of your system and finds out which components are responsible for handling them. For complete project documentation, please visit the Traefik documentation site .","title":"Traefik"},{"location":"add-ons/traefik/#usage","text":"Traefik can be deployed by enabling the add-on via the following. enable_traefik = true","title":"Usage"},{"location":"add-ons/traefik/#how-to-test-traefik-web-ui","text":"Once the Traefik deployment is successful, run the following command from your a local machine which have access to an EKS cluster using kubectl. $ kubectl port-forward svc/traefik -n kube-system 9000:9000 Now open the browser from your machine and enter the below URL to access Traefik Web UI. http://127.0.0.1:9000/dashboard/","title":"How to test Traefik Web UI"},{"location":"add-ons/traefik/#aws-service-annotations-for-traefik-ingress-controller","text":"Here is the link to get the AWS ELB service annotations for Traefik Ingress controller","title":"AWS Service annotations for Traefik Ingress Controller"},{"location":"add-ons/traefik/#gitops-configuration","text":"The following properties are made available for use when managing the add-on via GitOps traefik = { enable = true }","title":"GitOps Configuration"},{"location":"add-ons/vault/","text":"HashiCorp Vault \u00b6 HashiCorp Vault brokers and deeply integrates with trusted identities to automate access to secrets, data, and systems. This add-on is implemented as an external add-on. For detailed documentation and usage of the add-on please refer to the add-on repository . Example \u00b6 Checkout the full example . Usage \u00b6 This step deploys the HashiCorp Vault with default Helm Chart config enable_vault = true Alternatively, you can override the Helm Values by setting the vault_helm_config object, like shown in the code snippet below: enable_vault = true vault_helm_config = { name = \"vault\" # (Required) Release name. chart = \"vault\" # (Required) Chart name to be installed. repository = \"https://helm.releases.hashicorp.com\" # (Optional) Repository URL where to locate the requested chart. version = \"v0.19.0\" # (Optional) Specify the exact chart version to install. # ... } This snippet does not contain all available options that can be set as part of vault_helm_config . For the complete listing, see the hashicorp-vault-eks-blueprints-addon repository .","title":"HashiCorp Vault"},{"location":"add-ons/vault/#hashicorp-vault","text":"HashiCorp Vault brokers and deeply integrates with trusted identities to automate access to secrets, data, and systems. This add-on is implemented as an external add-on. For detailed documentation and usage of the add-on please refer to the add-on repository .","title":"HashiCorp Vault"},{"location":"add-ons/vault/#example","text":"Checkout the full example .","title":"Example"},{"location":"add-ons/vault/#usage","text":"This step deploys the HashiCorp Vault with default Helm Chart config enable_vault = true Alternatively, you can override the Helm Values by setting the vault_helm_config object, like shown in the code snippet below: enable_vault = true vault_helm_config = { name = \"vault\" # (Required) Release name. chart = \"vault\" # (Required) Chart name to be installed. repository = \"https://helm.releases.hashicorp.com\" # (Optional) Repository URL where to locate the requested chart. version = \"v0.19.0\" # (Optional) Specify the exact chart version to install. # ... } This snippet does not contain all available options that can be set as part of vault_helm_config . For the complete listing, see the hashicorp-vault-eks-blueprints-addon repository .","title":"Usage"},{"location":"add-ons/velero/","text":"Velero \u00b6 Velero is an open source tool to safely backup and restore, perform disaster recovery, and migrate Kubernetes cluster resources and persistent volumes. Helm chart Plugin for AWS Usage \u00b6 Velero can be deployed by enabling the add-on via the following. enable_velero = true velero_backup_s3_bucket = \"<YOUR_BUCKET_NAME>\" You can also customize the Helm chart that deploys velero via the following configuration: enable_velero = true velero_helm_config = { name = \"velero\" description = \"A Helm chart for velero\" chart = \"velero\" version = \"2.30.0\" repository = \"https://vmware-tanzu.github.io/helm-charts/\" namespace = \"velero\" values = [templatefile(\"${path.module}/values.yaml\", { bucket = \"<YOUR_BUCKET_NAME>\", region = \"<YOUR_BUCKET_REGION>\" })] } To see a working example, see the stateful example blueprint. Validate \u00b6 Run update-kubeconfig command: aws eks --region <REGION> update-kubeconfig --name <CLUSTER_NAME> Test by listing velero resources provisioned: kubectl get all -n velero # Output should look similar to below NAME READY STATUS RESTARTS AGE pod/velero-b4d8fd5c7-5smp6 1 /1 Running 0 112s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/velero ClusterIP 172 .20.217.203 <none> 8085 /TCP 114s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/velero 1 /1 1 1 114s NAME DESIRED CURRENT READY AGE replicaset.apps/velero-b4d8fd5c7 1 1 1 114s Get backup location using velero CLI velero backup-location get # Output should look similar to below NAME PROVIDER BUCKET/PREFIX PHASE LAST VALIDATED ACCESS MODE DEFAULT default aws velero-ssqwm44hvofzb32d Available 2022 -05-22 10 :53:26 -0400 EDT ReadWrite true To demonstrate creating a backup and restoring, create a new namespace and run nginx using below commands: kubectl create namespace backupdemo kubectl run nginx --image = nginx -n backupdemo Create backup of this namespace using velero velero backup create backup1 --include-namespaces backupdemo # Output should look similar to below Backup request \"backup1\" submitted successfully. Run ` velero backup describe backup1 ` or ` velero backup logs backup1 ` for more details. Describe the backup to check the backup status velero backup describe backup1 # Output should look similar to below Name: backup1 Namespace: velero Labels: velero.io/storage-location = default Annotations: velero.io/source-cluster-k8s-gitversion = v1.21.9-eks-14c7a48 velero.io/source-cluster-k8s-major-version = 1 velero.io/source-cluster-k8s-minor-version = 21 + Phase: Completed Errors: 0 Warnings: 0 Namespaces: Included: backupdemo Excluded: <none> Resources: Included: * Excluded: <none> Cluster-scoped: auto Label selector: <none> Storage Location: default Velero-Native Snapshot PVs: auto TTL: 720h0m0s Hooks: <none> Backup Format Version: 1 .1.0 Started: 2022 -05-22 10 :54:32 -0400 EDT Completed: 2022 -05-22 10 :54:35 -0400 EDT Expiration: 2022 -06-21 10 :54:32 -0400 EDT Total items to be backed up: 10 Items backed up: 10 Velero-Native Snapshots: <none included> Delete the namespace - this will be restored using the backup created kubectl delete namespace backupdemo Restore the namespace from your backup velero restore create --from-backup backup1 Verify that the namespace is restored kubectl get all -n backupdemo # Output should look similar to below NAME READY STATUS RESTARTS AGE pod/nginx 1 /1 Running 0 21s","title":"Velero"},{"location":"add-ons/velero/#velero","text":"Velero is an open source tool to safely backup and restore, perform disaster recovery, and migrate Kubernetes cluster resources and persistent volumes. Helm chart Plugin for AWS","title":"Velero"},{"location":"add-ons/velero/#usage","text":"Velero can be deployed by enabling the add-on via the following. enable_velero = true velero_backup_s3_bucket = \"<YOUR_BUCKET_NAME>\" You can also customize the Helm chart that deploys velero via the following configuration: enable_velero = true velero_helm_config = { name = \"velero\" description = \"A Helm chart for velero\" chart = \"velero\" version = \"2.30.0\" repository = \"https://vmware-tanzu.github.io/helm-charts/\" namespace = \"velero\" values = [templatefile(\"${path.module}/values.yaml\", { bucket = \"<YOUR_BUCKET_NAME>\", region = \"<YOUR_BUCKET_REGION>\" })] } To see a working example, see the stateful example blueprint.","title":"Usage"},{"location":"add-ons/velero/#validate","text":"Run update-kubeconfig command: aws eks --region <REGION> update-kubeconfig --name <CLUSTER_NAME> Test by listing velero resources provisioned: kubectl get all -n velero # Output should look similar to below NAME READY STATUS RESTARTS AGE pod/velero-b4d8fd5c7-5smp6 1 /1 Running 0 112s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE service/velero ClusterIP 172 .20.217.203 <none> 8085 /TCP 114s NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/velero 1 /1 1 1 114s NAME DESIRED CURRENT READY AGE replicaset.apps/velero-b4d8fd5c7 1 1 1 114s Get backup location using velero CLI velero backup-location get # Output should look similar to below NAME PROVIDER BUCKET/PREFIX PHASE LAST VALIDATED ACCESS MODE DEFAULT default aws velero-ssqwm44hvofzb32d Available 2022 -05-22 10 :53:26 -0400 EDT ReadWrite true To demonstrate creating a backup and restoring, create a new namespace and run nginx using below commands: kubectl create namespace backupdemo kubectl run nginx --image = nginx -n backupdemo Create backup of this namespace using velero velero backup create backup1 --include-namespaces backupdemo # Output should look similar to below Backup request \"backup1\" submitted successfully. Run ` velero backup describe backup1 ` or ` velero backup logs backup1 ` for more details. Describe the backup to check the backup status velero backup describe backup1 # Output should look similar to below Name: backup1 Namespace: velero Labels: velero.io/storage-location = default Annotations: velero.io/source-cluster-k8s-gitversion = v1.21.9-eks-14c7a48 velero.io/source-cluster-k8s-major-version = 1 velero.io/source-cluster-k8s-minor-version = 21 + Phase: Completed Errors: 0 Warnings: 0 Namespaces: Included: backupdemo Excluded: <none> Resources: Included: * Excluded: <none> Cluster-scoped: auto Label selector: <none> Storage Location: default Velero-Native Snapshot PVs: auto TTL: 720h0m0s Hooks: <none> Backup Format Version: 1 .1.0 Started: 2022 -05-22 10 :54:32 -0400 EDT Completed: 2022 -05-22 10 :54:35 -0400 EDT Expiration: 2022 -06-21 10 :54:32 -0400 EDT Total items to be backed up: 10 Items backed up: 10 Velero-Native Snapshots: <none included> Delete the namespace - this will be restored using the backup created kubectl delete namespace backupdemo Restore the namespace from your backup velero restore create --from-backup backup1 Verify that the namespace is restored kubectl get all -n backupdemo # Output should look similar to below NAME READY STATUS RESTARTS AGE pod/nginx 1 /1 Running 0 21s","title":"Validate"},{"location":"add-ons/vpa/","text":"Vertical Pod Autoscaler \u00b6 VPA Vertical Pod Autoscaler (VPA) automatically adjusts the CPU and memory reservations for your pods to help \"right size\" your applications. When configured, it will automatically request the necessary reservations based on usage and thus allow proper scheduling onto nodes so that the appropriate resource amount is available for each pod. It will also maintain ratios between limits and requests that were specified in initial container configuration. NOTE: Metrics Server add-on is a dependency for this addon Usage \u00b6 This step deploys the Vertical Pod Autoscaler with default Helm Chart config enable_vpa = true Alternatively, you can override the helm values by using the code snippet below vpa_enable = true vpa_helm_config = { name = \"vpa\" # (Required) Release name. repository = \"https://charts.fairwinds.com/stable\" # (Optional) Repository URL where to locate the requested chart. chart = \"vpa\" # (Required) Chart name to be installed. version = \"1.0.0\" # (Optional) Specify the exact chart version to install. If this is not specified, it defaults to the version set within default_helm_config: https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/main/modules/kubernetes-addons/vpa/locals.tf namespace = \"vpa\" # (Optional) The namespace to install the release into. values = [templatefile(\"${path.module}/values.yaml\", {})] }","title":"Vertical Pod Autoscaler"},{"location":"add-ons/vpa/#vertical-pod-autoscaler","text":"VPA Vertical Pod Autoscaler (VPA) automatically adjusts the CPU and memory reservations for your pods to help \"right size\" your applications. When configured, it will automatically request the necessary reservations based on usage and thus allow proper scheduling onto nodes so that the appropriate resource amount is available for each pod. It will also maintain ratios between limits and requests that were specified in initial container configuration. NOTE: Metrics Server add-on is a dependency for this addon","title":"Vertical Pod Autoscaler"},{"location":"add-ons/vpa/#usage","text":"This step deploys the Vertical Pod Autoscaler with default Helm Chart config enable_vpa = true Alternatively, you can override the helm values by using the code snippet below vpa_enable = true vpa_helm_config = { name = \"vpa\" # (Required) Release name. repository = \"https://charts.fairwinds.com/stable\" # (Optional) Repository URL where to locate the requested chart. chart = \"vpa\" # (Required) Chart name to be installed. version = \"1.0.0\" # (Optional) Specify the exact chart version to install. If this is not specified, it defaults to the version set within default_helm_config: https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/main/modules/kubernetes-addons/vpa/locals.tf namespace = \"vpa\" # (Optional) The namespace to install the release into. values = [templatefile(\"${path.module}/values.yaml\", {})] }","title":"Usage"},{"location":"add-ons/yunikorn/","text":"Apache YuniKorn \u00b6 YuniKorn YuniKorn is a light-weight, universal resource scheduler for container orchestrator systems. Apache YuniKorn (Incubating) is a new Apache incubator project that offers rich scheduling capabilities on Kubernetes. It fills the scheduling gap while running Big Data workloads on Kubernetes, with a ton of useful features such as hierarchical queues, elastic queue quotas, resource fairness, and job ordering You can define batchScheduler: \"yunikorn\" when you are running Spark Applications using SparkK8sOperator Usage \u00b6 This step deploys the Apache YuniKorn K8s schedular with default Helm Chart config enable_yunikorn = true Alternatively, you can override the helm values by using the code snippet below enable_yunikorn = true yunikorn_helm_config = { name = \"yunikorn\" # (Required) Release name. repository = \"https://apache.github.io/yunikorn-release\" # (Optional) Repository URL where to locate the requested chart. chart = \"yunikorn\" # (Required) Chart name to be installed. version = \"0.12.2\" # (Optional) Specify the exact chart version to install. If this is not specified, it defaults to the version set within default_helm_config: https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/main/modules/kubernetes-addons/yunikorn/locals.tf values = [templatefile(\"${path.module}/values.yaml\", {})] }","title":"Apache YuniKorn"},{"location":"add-ons/yunikorn/#apache-yunikorn","text":"YuniKorn YuniKorn is a light-weight, universal resource scheduler for container orchestrator systems. Apache YuniKorn (Incubating) is a new Apache incubator project that offers rich scheduling capabilities on Kubernetes. It fills the scheduling gap while running Big Data workloads on Kubernetes, with a ton of useful features such as hierarchical queues, elastic queue quotas, resource fairness, and job ordering You can define batchScheduler: \"yunikorn\" when you are running Spark Applications using SparkK8sOperator","title":"Apache YuniKorn"},{"location":"add-ons/yunikorn/#usage","text":"This step deploys the Apache YuniKorn K8s schedular with default Helm Chart config enable_yunikorn = true Alternatively, you can override the helm values by using the code snippet below enable_yunikorn = true yunikorn_helm_config = { name = \"yunikorn\" # (Required) Release name. repository = \"https://apache.github.io/yunikorn-release\" # (Optional) Repository URL where to locate the requested chart. chart = \"yunikorn\" # (Required) Chart name to be installed. version = \"0.12.2\" # (Optional) Specify the exact chart version to install. If this is not specified, it defaults to the version set within default_helm_config: https://github.com/aws-ia/terraform-aws-eks-blueprints/blob/main/modules/kubernetes-addons/yunikorn/locals.tf values = [templatefile(\"${path.module}/values.yaml\", {})] }","title":"Usage"},{"location":"advanced/bottlerocket/","text":"Bottlerocket OS \u00b6 Bottlerocket is an open source operating system specifically designed for running containers. Bottlerocket build system is based on Rust. It's a container host OS and doesn't have additional software's or package managers other than what is needed for running containers hence its very light weight and secure. Container optimized operating systems are ideal when you need to run applications in Kubernetes with minimal setup and do not want to worry about security or updates, or want OS support from cloud provider. Container operating systems does updates transactionally. Bottlerocket has two containers runtimes running. Control container on by default used for AWS Systems manager and remote API access. Admin container off by default for deep debugging and exploration. Bottlerocket Launch templates userdata uses the TOML format with Key-value pairs. Remote API access API via SSM agent. You can launch trouble shooting container via user data [settings.host-containers.admin] enabled = true . Features \u00b6 Secure - Opinionated, specialized and highly secured Flexible - Multi cloud and multi orchestrator Transactional - Image based upgraded and rollbacks Isolated - Separate container Runtimes Updates \u00b6 Bottlerocket can be updated automatically via Kubernetes Operator kubectl apply -f Bottlerocket_k8s.csv.yaml kubectl get ClusterServiceVersion Bottlerocket_k8s | jq. 'status'","title":"Bottlerocket"},{"location":"advanced/bottlerocket/#bottlerocket-os","text":"Bottlerocket is an open source operating system specifically designed for running containers. Bottlerocket build system is based on Rust. It's a container host OS and doesn't have additional software's or package managers other than what is needed for running containers hence its very light weight and secure. Container optimized operating systems are ideal when you need to run applications in Kubernetes with minimal setup and do not want to worry about security or updates, or want OS support from cloud provider. Container operating systems does updates transactionally. Bottlerocket has two containers runtimes running. Control container on by default used for AWS Systems manager and remote API access. Admin container off by default for deep debugging and exploration. Bottlerocket Launch templates userdata uses the TOML format with Key-value pairs. Remote API access API via SSM agent. You can launch trouble shooting container via user data [settings.host-containers.admin] enabled = true .","title":"Bottlerocket OS"},{"location":"advanced/bottlerocket/#features","text":"Secure - Opinionated, specialized and highly secured Flexible - Multi cloud and multi orchestrator Transactional - Image based upgraded and rollbacks Isolated - Separate container Runtimes","title":"Features"},{"location":"advanced/bottlerocket/#updates","text":"Bottlerocket can be updated automatically via Kubernetes Operator kubectl apply -f Bottlerocket_k8s.csv.yaml kubectl get ClusterServiceVersion Bottlerocket_k8s | jq. 'status'","title":"Updates"},{"location":"advanced/cluster-upgrades/","text":"EKS Upgrade Documentation \u00b6 Objective: \u00b6 The purpose of this document is to provide an overview of the steps for upgrading the EKS Cluster from one version to another. Please note that EKS upgrade documentation gets published by AWS every year. The current version of the upgrade documentation while writing this README Prerequisites: \u00b6 1. Download the latest upgrade docs from AWS sites (https://docs.aws.amazon.com/eks/latest/userguide/update-cluster.html) 2. Always upgrade one increment at a time (E.g., 1.20 to 1.21). AWS doesn't support upgrades from 1.20 to 1.22 directly Steps to Upgrade EKS cluster: \u00b6 Change the version in Terraform to the desired Kubernetes cluster version. See the example below cluster_version = \"1.21\" If you are specifying a version for EKS managed addons, you will need to ensure the version used is compatible with the new cluster version, or use a data source to pull the appropriate version. If you are not specifying a version for EKS managed addons, no changes are required since the EKS service will update the default addon version based on the cluster version specified. To ensure the correct addon version is used, it is recommended to use the addon version data source which will pull the appropriate version for a given cluster version: data \"aws_eks_addon_version\" \"default\" { for_each = toset([\"coredns\", \"aws-ebs-csi-driver\", \"kube-proxy\", \"vpc-cni\"]) addon_name = each.value kubernetes_version = \"1.21\" # ensure this matches whats set on the cluster most_recent = false # can also set to `true` to use latest version for the specified cluster version } module \"eks_blueprints_kubernetes_addons\" { # Essential inputs are not shown for brevity enable_amazon_eks_coredns = true amazon_eks_coredns_config = { addon_version = data.aws_eks_addon_version.default[\"coredns\"].version resolve_conflicts = \"OVERWRITE\" service_account_role_arn = \"\" additional_iam_policies = [] tags = {} } enable_amazon_eks_aws_ebs_csi_driver = true amazon_eks_aws_ebs_csi_driver_config = { addon_version = data.aws_eks_addon_version.default[\"aws-ebs-csi-driver\"].version resolve_conflicts = \"OVERWRITE\" additional_iam_policies = [] service_account_role_arn = \"\" tags = {} } enable_amazon_eks_kube_proxy = true amazon_eks_kube_proxy_config = { addon_version = data.aws_eks_addon_version.default[\"kube-proxy\"].version resolve_conflicts = \"OVERWRITE\" additional_iam_policies = [] service_account_role_arn = \"\" tags = {} } enable_amazon_eks_vpc_cni = true amazon_eks_vpc_cni_config = { addon_version = data.aws_eks_addon_version.default[\"vpc-cni\"].version resolve_conflicts = \"OVERWRITE\" additional_iam_policies = [] service_account_role_arn = \"\" tags = {} } } Apply the changes to the cluster with Terraform. This will: Upgrade the Control Plane to the version specified Update the Data Plane to ensure the compute resources are utilizing the corresponding AMI for the given cluster version Update addons to reflect the respective versions Important Note \u00b6 Please note that you may need to update other Kubernetes Addons deployed through Helm Charts to match with new Kubernetes upgrade version","title":"Cluster Upgrades"},{"location":"advanced/cluster-upgrades/#eks-upgrade-documentation","text":"","title":"EKS Upgrade Documentation"},{"location":"advanced/cluster-upgrades/#objective","text":"The purpose of this document is to provide an overview of the steps for upgrading the EKS Cluster from one version to another. Please note that EKS upgrade documentation gets published by AWS every year. The current version of the upgrade documentation while writing this README","title":"Objective:"},{"location":"advanced/cluster-upgrades/#prerequisites","text":"1. Download the latest upgrade docs from AWS sites (https://docs.aws.amazon.com/eks/latest/userguide/update-cluster.html) 2. Always upgrade one increment at a time (E.g., 1.20 to 1.21). AWS doesn't support upgrades from 1.20 to 1.22 directly","title":"Prerequisites:"},{"location":"advanced/cluster-upgrades/#steps-to-upgrade-eks-cluster","text":"Change the version in Terraform to the desired Kubernetes cluster version. See the example below cluster_version = \"1.21\" If you are specifying a version for EKS managed addons, you will need to ensure the version used is compatible with the new cluster version, or use a data source to pull the appropriate version. If you are not specifying a version for EKS managed addons, no changes are required since the EKS service will update the default addon version based on the cluster version specified. To ensure the correct addon version is used, it is recommended to use the addon version data source which will pull the appropriate version for a given cluster version: data \"aws_eks_addon_version\" \"default\" { for_each = toset([\"coredns\", \"aws-ebs-csi-driver\", \"kube-proxy\", \"vpc-cni\"]) addon_name = each.value kubernetes_version = \"1.21\" # ensure this matches whats set on the cluster most_recent = false # can also set to `true` to use latest version for the specified cluster version } module \"eks_blueprints_kubernetes_addons\" { # Essential inputs are not shown for brevity enable_amazon_eks_coredns = true amazon_eks_coredns_config = { addon_version = data.aws_eks_addon_version.default[\"coredns\"].version resolve_conflicts = \"OVERWRITE\" service_account_role_arn = \"\" additional_iam_policies = [] tags = {} } enable_amazon_eks_aws_ebs_csi_driver = true amazon_eks_aws_ebs_csi_driver_config = { addon_version = data.aws_eks_addon_version.default[\"aws-ebs-csi-driver\"].version resolve_conflicts = \"OVERWRITE\" additional_iam_policies = [] service_account_role_arn = \"\" tags = {} } enable_amazon_eks_kube_proxy = true amazon_eks_kube_proxy_config = { addon_version = data.aws_eks_addon_version.default[\"kube-proxy\"].version resolve_conflicts = \"OVERWRITE\" additional_iam_policies = [] service_account_role_arn = \"\" tags = {} } enable_amazon_eks_vpc_cni = true amazon_eks_vpc_cni_config = { addon_version = data.aws_eks_addon_version.default[\"vpc-cni\"].version resolve_conflicts = \"OVERWRITE\" additional_iam_policies = [] service_account_role_arn = \"\" tags = {} } } Apply the changes to the cluster with Terraform. This will: Upgrade the Control Plane to the version specified Update the Data Plane to ensure the compute resources are utilizing the corresponding AMI for the given cluster version Update addons to reflect the respective versions","title":"Steps to Upgrade EKS cluster:"},{"location":"advanced/cluster-upgrades/#important-note","text":"Please note that you may need to update other Kubernetes Addons deployed through Helm Charts to match with new Kubernetes upgrade version","title":"Important Note"},{"location":"advanced/ecr-instructions/","text":"Docker upload to Elastic Container Registry \u00b6 Download the docker image to your local Mac/Laptop $ docker pull <image name>:<image tag> Retrieve an authentication token and authenticate your Docker client to your registry. Use the AWS CLI: $ aws ecr get-login-password --region <aws region> | docker login --username AWS --password-stdin <account id>.dkr.ecr.<aws region>.amazonaws.com Create an ECR repo for your image. $ aws ecr create-repository --repository-name <image name> --image-scanning-configuration scanOnPush=true After the repo is created in ECR, tag your image so, you can push the image to this repository: $ docker tag <image name>:<image tag> <account id>.dkr.ecr.<aws region.amazonaws.com/<image name>:<image tag> Step 6: Run the following command to push this image to your newly created AWS repository: $ docker push <account id>.dkr.ecr.<aws region.amazonaws.com/<image name>:<image tag>","title":"ECR Instructions"},{"location":"advanced/ecr-instructions/#docker-upload-to-elastic-container-registry","text":"Download the docker image to your local Mac/Laptop $ docker pull <image name>:<image tag> Retrieve an authentication token and authenticate your Docker client to your registry. Use the AWS CLI: $ aws ecr get-login-password --region <aws region> | docker login --username AWS --password-stdin <account id>.dkr.ecr.<aws region>.amazonaws.com Create an ECR repo for your image. $ aws ecr create-repository --repository-name <image name> --image-scanning-configuration scanOnPush=true After the repo is created in ECR, tag your image so, you can push the image to this repository: $ docker tag <image name>:<image tag> <account id>.dkr.ecr.<aws region.amazonaws.com/<image name>:<image tag> Step 6: Run the following command to push this image to your newly created AWS repository: $ docker push <account id>.dkr.ecr.<aws region.amazonaws.com/<image name>:<image tag>","title":"Docker upload to Elastic Container Registry"},{"location":"advanced/gitops-with-flux/","text":"Manage your cluster(s) configuration with Flux \u00b6 Once you have deployed your EKS cluster(s) with Terraform, you can leverage Flux to manage your cluster's configuration with GitOps , including the deployment of add-ons, cluster configuration (e.g. cluster policies) and applications. Using GitOps practices to manage your clusters configuration will simplfy management, scaling the number of clusters you run and be able to easily recreate your clusters, treating them as ephemeral resources. Recreating your cluster is as simple as deploying a new cluster with Terraform and bootstraping it with Flux pointing to the repository containing the configuration. The aws-samples/flux-eks-gitops-config repository provides a sample configuration blueprint for configuring multiple Amazon EKS clusters belonging to different stages ( test and production ) using GitOps with Flux v2 . This repository installs a set of commonly used Kubernetes add-ons to perform policy enforcement, restrict network traffic with network policies, cluster monitoring, extend Kubernetes deployment capabilities enabling progressive Canary deployments for your applications... You can use the above sample repository to experiment with the predefined cluster configurations and use it as a baseline to adjust it to your own needs. This sample installs the following Kubernetes add-ons: metrics-server : Aggregator of resource usage data in your cluster, commonly used by other Kubernetes add ons, such us Horizontal Pod Autoscaler or Kubernetes Dashboard . Calico : Project Calico is a network policy engine for Kubernetes. Calico network policy enforcement allows you to implement network segmentation and tenant isolation. For more information check the Amazon EKS documentation . Kyverno : Kubernetes Policy Management Engine. Kyverno allows cluster administrators to manage environment specific configurations independently of workload configurations and enforce configuration best practices for their clusters. Kyverno can be used to scan existing workloads for best practices, or can be used to enforce best practices by blocking or mutating API requests. Prometheus : Defacto standard open-source systems monitoring and alerting toolkit for Kubernetes. This repository installs kube-prometheus-stack . Flagger : Progressive delivery operator for Flux. Flagger can run automated application analysis, testing, promotion and rollback for the following deployment strategies: Canary, A/B Testing and Blue/Green. For more details, check the Flagger documentation . nginx-ingress-controller : Ingress controller to expose apps and enable canary deployments and A/B testing with Flagger . NOTE: The add-ons on the sample are not configured for a production-ready cluster (e.g. Prometheus would need to be configured for long term metric storage, nginx would need HPA and any custom settings you need...). There're also a set of Kyverno cluster policies deployed to audit (test) or enforce (production) security settings on your workloads, as well as podinfo as a sample application, configured with Flagger to perform progressive deployments. For further information, visit the aws-samples/flux-eks-gitops-config repository documentation. Bootstrap your cluster with Flux \u00b6 The below instructions assume you have created a cluster with eks-blueprints with no add-ons other than aws-load-balancer-controller. If you're installing additonal add-ons via terraform, the configuration may clash with the one on the sample repository. If you plan to leverage Flux, we recommend that you use Terraform to install and manage only add-ons that require additional AWS resources to be created (like IAM roles for Service accounts), and then use Flux to manage the rest. Prerequisites \u00b6 The add-ons and configurations of this repository require Kubernetes 1.21 or higher (this is required by the version of kube-prometheus-stack that is installed, you can use 1.19+ installing previous versions of kube-prometheus-stack). You'll also need the following: Install flux CLI on your computer following the instructions here . This repository has been tested with flux 0.22. A GitHub account and a personal access token that can create repositories. Bootstrap your cluster \u00b6 Fork the aws-samples/flux-eks-gitops-config repository on your personal GitHub account and export your GitHub access token, username and repo name: export GITHUB_TOKEN = <your-token> export GITHUB_USER = <your-username> export GITHUB_REPO = <repository-name> Define whether you want to bootstrap your cluster with the TEST or the PRODUCTION configuration: # TEST configuration export CLUSTER_ENVIRONMENT = test # PRODUCTION configuration export CLUSTER_ENVIRONMENT = production Verify that your stagging cluster satisfies the prerequisites with: flux check --pre You can now bootstrap your cluster with Flux CLI. flux bootstrap github --owner = ${ GITHUB_USER } --repository = ${ GITHUB_REPO } --branch = main --path = clusters/ ${ CLUSTER_ENVIRONMENT } --personal The bootstrap command commits the manifests for the Flux components in clusters/${CLUSTER_ENVIRONMENT}/flux-system directory and creates a deploy key with read-only access on GitHub, so it can pull changes inside the cluster. Confirm that Flux has finished applying the configuration to your cluster (it will take 3 or 4 minutes to sync everything): $ flux get kustomization NAME READY MESSAGE REVISION SUSPENDED apps True Applied revision: main/b7d10ca21be7cac0dcdd14c80353012ccfedd4fe main/b7d10ca21be7cac0dcdd14c80353012ccfedd4fe False calico-installation True Applied revision: master/00a2f33ea55f2018819434175c09c8bd8f20741a master/00a2f33ea55f2018819434175c09c8bd8f20741a False calico-operator True Applied revision: master/00a2f33ea55f2018819434175c09c8bd8f20741a master/00a2f33ea55f2018819434175c09c8bd8f20741a False config True Applied revision: main/8fd33f531df71002f2da7bc9619ee75281a9ead0 main/8fd33f531df71002f2da7bc9619ee75281a9ead0 False flux-system True Applied revision: main/b7d10ca21be7cac0dcdd14c80353012ccfedd4fe main/b7d10ca21be7cac0dcdd14c80353012ccfedd4fe False infrastructure True Applied revision: main/b7d10ca21be7cac0dcdd14c80353012ccfedd4fe main/b7d10ca21be7cac0dcdd14c80353012ccfedd4fe False Get the URL for the nginx ingress controller that has been deployed in your cluster (you will see two ingresses, since Flagger will create a canary ingress): $ kubectl get ingress -n podinfo NAME CLASS HOSTS ADDRESS PORTS AGE podinfo nginx podinfo.test k8s-xxxxxx.elb.us-west-2.amazonaws.com 80 23h podinfo-canary nginx podinfo.test k8s-xxxxxx.elb.us-west-2.amazonaws.com 80 23h Confirm that podinfo can be correctly accessed via ingress: $ curl -H \"Host: podinfo.test\" k8s-xxxxxx.elb.us-west-2.amazonaws.com { \"hostname\" : \"podinfo-primary-65584c8f4f-d7v4t\" , \"version\" : \"6.0.0\" , \"revision\" : \"\" , \"color\" : \"#34577c\" , \"logo\" : \"https://raw.githubusercontent.com/stefanprodan/podinfo/gh-pages/cuddle_clap.gif\" , \"message\" : \"greetings from podinfo v6.0.0\" , \"goos\" : \"linux\" , \"goarch\" : \"amd64\" , \"runtime\" : \"go1.16.5\" , \"num_goroutine\" : \"10\" , \"num_cpu\" : \"2\" } Congratulations! Your cluster has sync'ed all the configuration defined on the repository. Continue exploring the deployed configuration following these docs: Review the repository structure to understand the applied configuration Test the cluster policies configured with Kyverno Test progressive deployments with Flux, Flagger and nginx controller","title":"GitOps with Flux"},{"location":"advanced/gitops-with-flux/#manage-your-clusters-configuration-with-flux","text":"Once you have deployed your EKS cluster(s) with Terraform, you can leverage Flux to manage your cluster's configuration with GitOps , including the deployment of add-ons, cluster configuration (e.g. cluster policies) and applications. Using GitOps practices to manage your clusters configuration will simplfy management, scaling the number of clusters you run and be able to easily recreate your clusters, treating them as ephemeral resources. Recreating your cluster is as simple as deploying a new cluster with Terraform and bootstraping it with Flux pointing to the repository containing the configuration. The aws-samples/flux-eks-gitops-config repository provides a sample configuration blueprint for configuring multiple Amazon EKS clusters belonging to different stages ( test and production ) using GitOps with Flux v2 . This repository installs a set of commonly used Kubernetes add-ons to perform policy enforcement, restrict network traffic with network policies, cluster monitoring, extend Kubernetes deployment capabilities enabling progressive Canary deployments for your applications... You can use the above sample repository to experiment with the predefined cluster configurations and use it as a baseline to adjust it to your own needs. This sample installs the following Kubernetes add-ons: metrics-server : Aggregator of resource usage data in your cluster, commonly used by other Kubernetes add ons, such us Horizontal Pod Autoscaler or Kubernetes Dashboard . Calico : Project Calico is a network policy engine for Kubernetes. Calico network policy enforcement allows you to implement network segmentation and tenant isolation. For more information check the Amazon EKS documentation . Kyverno : Kubernetes Policy Management Engine. Kyverno allows cluster administrators to manage environment specific configurations independently of workload configurations and enforce configuration best practices for their clusters. Kyverno can be used to scan existing workloads for best practices, or can be used to enforce best practices by blocking or mutating API requests. Prometheus : Defacto standard open-source systems monitoring and alerting toolkit for Kubernetes. This repository installs kube-prometheus-stack . Flagger : Progressive delivery operator for Flux. Flagger can run automated application analysis, testing, promotion and rollback for the following deployment strategies: Canary, A/B Testing and Blue/Green. For more details, check the Flagger documentation . nginx-ingress-controller : Ingress controller to expose apps and enable canary deployments and A/B testing with Flagger . NOTE: The add-ons on the sample are not configured for a production-ready cluster (e.g. Prometheus would need to be configured for long term metric storage, nginx would need HPA and any custom settings you need...). There're also a set of Kyverno cluster policies deployed to audit (test) or enforce (production) security settings on your workloads, as well as podinfo as a sample application, configured with Flagger to perform progressive deployments. For further information, visit the aws-samples/flux-eks-gitops-config repository documentation.","title":"Manage your cluster(s) configuration with Flux"},{"location":"advanced/gitops-with-flux/#bootstrap-your-cluster-with-flux","text":"The below instructions assume you have created a cluster with eks-blueprints with no add-ons other than aws-load-balancer-controller. If you're installing additonal add-ons via terraform, the configuration may clash with the one on the sample repository. If you plan to leverage Flux, we recommend that you use Terraform to install and manage only add-ons that require additional AWS resources to be created (like IAM roles for Service accounts), and then use Flux to manage the rest.","title":"Bootstrap your cluster with Flux"},{"location":"advanced/gitops-with-flux/#prerequisites","text":"The add-ons and configurations of this repository require Kubernetes 1.21 or higher (this is required by the version of kube-prometheus-stack that is installed, you can use 1.19+ installing previous versions of kube-prometheus-stack). You'll also need the following: Install flux CLI on your computer following the instructions here . This repository has been tested with flux 0.22. A GitHub account and a personal access token that can create repositories.","title":"Prerequisites"},{"location":"advanced/gitops-with-flux/#bootstrap-your-cluster","text":"Fork the aws-samples/flux-eks-gitops-config repository on your personal GitHub account and export your GitHub access token, username and repo name: export GITHUB_TOKEN = <your-token> export GITHUB_USER = <your-username> export GITHUB_REPO = <repository-name> Define whether you want to bootstrap your cluster with the TEST or the PRODUCTION configuration: # TEST configuration export CLUSTER_ENVIRONMENT = test # PRODUCTION configuration export CLUSTER_ENVIRONMENT = production Verify that your stagging cluster satisfies the prerequisites with: flux check --pre You can now bootstrap your cluster with Flux CLI. flux bootstrap github --owner = ${ GITHUB_USER } --repository = ${ GITHUB_REPO } --branch = main --path = clusters/ ${ CLUSTER_ENVIRONMENT } --personal The bootstrap command commits the manifests for the Flux components in clusters/${CLUSTER_ENVIRONMENT}/flux-system directory and creates a deploy key with read-only access on GitHub, so it can pull changes inside the cluster. Confirm that Flux has finished applying the configuration to your cluster (it will take 3 or 4 minutes to sync everything): $ flux get kustomization NAME READY MESSAGE REVISION SUSPENDED apps True Applied revision: main/b7d10ca21be7cac0dcdd14c80353012ccfedd4fe main/b7d10ca21be7cac0dcdd14c80353012ccfedd4fe False calico-installation True Applied revision: master/00a2f33ea55f2018819434175c09c8bd8f20741a master/00a2f33ea55f2018819434175c09c8bd8f20741a False calico-operator True Applied revision: master/00a2f33ea55f2018819434175c09c8bd8f20741a master/00a2f33ea55f2018819434175c09c8bd8f20741a False config True Applied revision: main/8fd33f531df71002f2da7bc9619ee75281a9ead0 main/8fd33f531df71002f2da7bc9619ee75281a9ead0 False flux-system True Applied revision: main/b7d10ca21be7cac0dcdd14c80353012ccfedd4fe main/b7d10ca21be7cac0dcdd14c80353012ccfedd4fe False infrastructure True Applied revision: main/b7d10ca21be7cac0dcdd14c80353012ccfedd4fe main/b7d10ca21be7cac0dcdd14c80353012ccfedd4fe False Get the URL for the nginx ingress controller that has been deployed in your cluster (you will see two ingresses, since Flagger will create a canary ingress): $ kubectl get ingress -n podinfo NAME CLASS HOSTS ADDRESS PORTS AGE podinfo nginx podinfo.test k8s-xxxxxx.elb.us-west-2.amazonaws.com 80 23h podinfo-canary nginx podinfo.test k8s-xxxxxx.elb.us-west-2.amazonaws.com 80 23h Confirm that podinfo can be correctly accessed via ingress: $ curl -H \"Host: podinfo.test\" k8s-xxxxxx.elb.us-west-2.amazonaws.com { \"hostname\" : \"podinfo-primary-65584c8f4f-d7v4t\" , \"version\" : \"6.0.0\" , \"revision\" : \"\" , \"color\" : \"#34577c\" , \"logo\" : \"https://raw.githubusercontent.com/stefanprodan/podinfo/gh-pages/cuddle_clap.gif\" , \"message\" : \"greetings from podinfo v6.0.0\" , \"goos\" : \"linux\" , \"goarch\" : \"amd64\" , \"runtime\" : \"go1.16.5\" , \"num_goroutine\" : \"10\" , \"num_cpu\" : \"2\" } Congratulations! Your cluster has sync'ed all the configuration defined on the repository. Continue exploring the deployed configuration following these docs: Review the repository structure to understand the applied configuration Test the cluster policies configured with Kyverno Test progressive deployments with Flux, Flagger and nginx controller","title":"Bootstrap your cluster"},{"location":"advanced/multi-cluster/","text":"Advanced Deployment Folder Structure \u00b6 This example shows how to structure folders in your repo when you want to deploy multiple EKS Clusters across multiple regions and accounts. The top-level examples\\advanced folder provides an example of how you can structure your folders and files to define multiple EKS Cluster environments and consume this Blueprints module. This approach is suitable for large projects, with clearly defined sub directory and file structure. Each folder under live/<region>/application represents an EKS cluster environment(e.g., dev, test, load etc.). Each folder contains a backend.conf and <env>.tfvars , used to create a unique Terraform state for each cluster environment. Terraform backend configuration can be updated in backend.conf and cluster common configuration variables in <env>.tfvars e.g. folder/file structure for defining multiple clusters \u251c\u2500\u2500 examples\\advanced \u2502 \u2514\u2500\u2500 live \u2502 \u2514\u2500\u2500 preprod \u2502 \u2514\u2500\u2500 eu-west-1 \u2502 \u2514\u2500\u2500 application \u2502 \u2514\u2500\u2500 dev \u2502 \u2514\u2500\u2500 backend.conf \u2502 \u2514\u2500\u2500 dev.tfvars \u2502 \u2514\u2500\u2500 main.tf \u2502 \u2514\u2500\u2500 variables.tf \u2502 \u2514\u2500\u2500 outputs.tf \u2502 \u2514\u2500\u2500 test \u2502 \u2514\u2500\u2500 backend.conf \u2502 \u2514\u2500\u2500 test.tfvars \u2502 \u2514\u2500\u2500 prod \u2502 \u2514\u2500\u2500 eu-west-1 \u2502 \u2514\u2500\u2500 application \u2502 \u2514\u2500\u2500 prod \u2502 \u2514\u2500\u2500 backend.conf \u2502 \u2514\u2500\u2500 prod.tfvars \u2502 \u2514\u2500\u2500 main.tf \u2502 \u2514\u2500\u2500 variables.tf \u2502 \u2514\u2500\u2500 outputs.tf Important Note \u00b6 If you are using an existing VPC, you need to ensure that the following tags are added to the VPC and subnet resources Add Tags to VPC Key = \"Kubernetes.io/cluster/${local.cluster_id}\" Value = \"Shared\" Add Tags to Public Subnets tagging requirement public_subnet_tags = { \"Kubernetes.io/cluster/${local.cluster_id}\" = \"shared\" \"Kubernetes.io/role/elb\" = \"1\" } Add Tags to Private Subnets tagging requirement private_subnet_tags = { \"Kubernetes.io/cluster/${local.cluster_id}\" = \"shared\" \"Kubernetes.io/role/internal-elb\" = \"1\" }","title":"Multi-cluster"},{"location":"advanced/multi-cluster/#advanced-deployment-folder-structure","text":"This example shows how to structure folders in your repo when you want to deploy multiple EKS Clusters across multiple regions and accounts. The top-level examples\\advanced folder provides an example of how you can structure your folders and files to define multiple EKS Cluster environments and consume this Blueprints module. This approach is suitable for large projects, with clearly defined sub directory and file structure. Each folder under live/<region>/application represents an EKS cluster environment(e.g., dev, test, load etc.). Each folder contains a backend.conf and <env>.tfvars , used to create a unique Terraform state for each cluster environment. Terraform backend configuration can be updated in backend.conf and cluster common configuration variables in <env>.tfvars e.g. folder/file structure for defining multiple clusters \u251c\u2500\u2500 examples\\advanced \u2502 \u2514\u2500\u2500 live \u2502 \u2514\u2500\u2500 preprod \u2502 \u2514\u2500\u2500 eu-west-1 \u2502 \u2514\u2500\u2500 application \u2502 \u2514\u2500\u2500 dev \u2502 \u2514\u2500\u2500 backend.conf \u2502 \u2514\u2500\u2500 dev.tfvars \u2502 \u2514\u2500\u2500 main.tf \u2502 \u2514\u2500\u2500 variables.tf \u2502 \u2514\u2500\u2500 outputs.tf \u2502 \u2514\u2500\u2500 test \u2502 \u2514\u2500\u2500 backend.conf \u2502 \u2514\u2500\u2500 test.tfvars \u2502 \u2514\u2500\u2500 prod \u2502 \u2514\u2500\u2500 eu-west-1 \u2502 \u2514\u2500\u2500 application \u2502 \u2514\u2500\u2500 prod \u2502 \u2514\u2500\u2500 backend.conf \u2502 \u2514\u2500\u2500 prod.tfvars \u2502 \u2514\u2500\u2500 main.tf \u2502 \u2514\u2500\u2500 variables.tf \u2502 \u2514\u2500\u2500 outputs.tf","title":"Advanced Deployment Folder Structure"},{"location":"advanced/multi-cluster/#important-note","text":"If you are using an existing VPC, you need to ensure that the following tags are added to the VPC and subnet resources Add Tags to VPC Key = \"Kubernetes.io/cluster/${local.cluster_id}\" Value = \"Shared\" Add Tags to Public Subnets tagging requirement public_subnet_tags = { \"Kubernetes.io/cluster/${local.cluster_id}\" = \"shared\" \"Kubernetes.io/role/elb\" = \"1\" } Add Tags to Private Subnets tagging requirement private_subnet_tags = { \"Kubernetes.io/cluster/${local.cluster_id}\" = \"shared\" \"Kubernetes.io/role/internal-elb\" = \"1\" }","title":"Important Note"},{"location":"advanced/private-clusters/","text":"Private Clusters \u00b6 For fully Private EKS clusters requires the following VPC endpoints to be created to communicate with AWS services. This module will create these endpoints if you choose to create VPC. If you are using an existing VPC then you may need to ensure these endpoints are created. com.amazonaws.region.aps-workspaces - For AWS Managed Prometheus Workspace com.amazonaws.region.ssm - Secrets Management com.amazonaws.region.ec2 com.amazonaws.region.ecr.api com.amazonaws.region.ecr.dkr com.amazonaws.region.logs \u2013 For CloudWatch Logs com.amazonaws.region.sts \u2013 If using AWS Fargate or IAM roles for service accounts com.amazonaws.region.elasticloadbalancing \u2013 If using Application Load Balancers com.amazonaws.region.autoscaling \u2013 If using Cluster Autoscaler com.amazonaws.region.s3 \u2013 Creates S3 gateway","title":"Private Clusters"},{"location":"advanced/private-clusters/#private-clusters","text":"For fully Private EKS clusters requires the following VPC endpoints to be created to communicate with AWS services. This module will create these endpoints if you choose to create VPC. If you are using an existing VPC then you may need to ensure these endpoints are created. com.amazonaws.region.aps-workspaces - For AWS Managed Prometheus Workspace com.amazonaws.region.ssm - Secrets Management com.amazonaws.region.ec2 com.amazonaws.region.ecr.api com.amazonaws.region.ecr.dkr com.amazonaws.region.logs \u2013 For CloudWatch Logs com.amazonaws.region.sts \u2013 If using AWS Fargate or IAM roles for service accounts com.amazonaws.region.elasticloadbalancing \u2013 If using Application Load Balancers com.amazonaws.region.autoscaling \u2013 If using Cluster Autoscaler com.amazonaws.region.s3 \u2013 Creates S3 gateway","title":"Private Clusters"},{"location":"iam/minimum-iam-policy/","text":"Minimum IAM policy \u00b6 This document describes the minimum IAM policy required to run core examples that we run in our E2E workflow , mainly focused on the list of IAM actions. Note : The policy resource is set as * to allow all resources, this is not a recommended practice. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"aps:CreateAlertManagerDefinition\" , \"aps:CreateWorkspace\" , \"aps:DeleteAlertManagerDefinition\" , \"aps:DeleteWorkspace\" , \"aps:DescribeAlertManagerDefinition\" , \"aps:DescribeWorkspace\" , \"aps:ListTagsForResource\" , \"autoscaling:CreateAutoScalingGroup\" , \"autoscaling:CreateOrUpdateTags\" , \"autoscaling:DeleteAutoScalingGroup\" , \"autoscaling:DeleteLifecycleHook\" , \"autoscaling:DeleteTags\" , \"autoscaling:DescribeAutoScalingGroups\" , \"autoscaling:DescribeLifecycleHooks\" , \"autoscaling:DescribeTags\" , \"autoscaling:PutLifecycleHook\" , \"autoscaling:SetInstanceProtection\" , \"autoscaling:UpdateAutoScalingGroup\" , \"ec2:AllocateAddress\" , \"ec2:AssociateRouteTable\" , \"ec2:AttachInternetGateway\" , \"ec2:AuthorizeSecurityGroupEgress\" , \"ec2:AuthorizeSecurityGroupIngress\" , \"ec2:CreateEgressOnlyInternetGateway\" , \"ec2:CreateInternetGateway\" , \"ec2:CreateLaunchTemplate\" , \"ec2:CreateNatGateway\" , \"ec2:CreateNetworkAclEntry\" , \"ec2:CreateRoute\" , \"ec2:CreateRouteTable\" , \"ec2:CreateSecurityGroup\" , \"ec2:CreateSubnet\" , \"ec2:CreateTags\" , \"ec2:CreateVpc\" , \"ec2:DeleteEgressOnlyInternetGateway\" , \"ec2:DeleteInternetGateway\" , \"ec2:DeleteLaunchTemplate\" , \"ec2:DeleteNatGateway\" , \"ec2:DeleteNetworkAclEntry\" , \"ec2:DeleteRoute\" , \"ec2:DeleteRouteTable\" , \"ec2:DeleteSecurityGroup\" , \"ec2:DeleteSubnet\" , \"ec2:DeleteTags\" , \"ec2:DeleteVpc\" , \"ec2:DescribeAccountAttributes\" , \"ec2:DescribeAddresses\" , \"ec2:DescribeAvailabilityZones\" , \"ec2:DescribeEgressOnlyInternetGateways\" , \"ec2:DescribeImages\" , \"ec2:DescribeInternetGateways\" , \"ec2:DescribeLaunchTemplateVersions\" , \"ec2:DescribeLaunchTemplates\" , \"ec2:DescribeNatGateways\" , \"ec2:DescribeNetworkAcls\" , \"ec2:DescribeNetworkInterfaces\" , \"ec2:DescribeRouteTables\" , \"ec2:DescribeSecurityGroups\" , \"ec2:DescribeSubnets\" , \"ec2:DescribeTags\" , \"ec2:DescribeVpcAttribute\" , \"ec2:DescribeVpcClassicLink\" , \"ec2:DescribeVpcClassicLinkDnsSupport\" , \"ec2:DescribeVpcs\" , \"ec2:DetachInternetGateway\" , \"ec2:DisassociateRouteTable\" , \"ec2:ModifySubnetAttribute\" , \"ec2:ModifyVpcAttribute\" , \"ec2:ReleaseAddress\" , \"ec2:RevokeSecurityGroupEgress\" , \"ec2:RevokeSecurityGroupIngress\" , \"eks:CreateAddon\" , \"eks:CreateCluster\" , \"eks:CreateFargateProfile\" , \"eks:CreateNodegroup\" , \"eks:DeleteAddon\" , \"eks:DeleteCluster\" , \"eks:DeleteFargateProfile\" , \"eks:DeleteNodegroup\" , \"eks:DescribeAddon\" , \"eks:DescribeAddonVersions\" , \"eks:DescribeCluster\" , \"eks:DescribeFargateProfile\" , \"eks:DescribeNodegroup\" , \"elasticfilesystem:CreateFileSystem\" , \"elasticfilesystem:CreateMountTarget\" , \"elasticfilesystem:DeleteFileSystem\" , \"elasticfilesystem:DeleteMountTarget\" , \"elasticfilesystem:DescribeFileSystems\" , \"elasticfilesystem:DescribeLifecycleConfiguration\" , \"elasticfilesystem:DescribeMountTargetSecurityGroups\" , \"elasticfilesystem:DescribeMountTargets\" , \"emr-containers:CreateVirtualCluster\" , \"emr-containers:DeleteVirtualCluster\" , \"emr-containers:DescribeVirtualCluster\" , \"events:DeleteRule\" , \"events:DescribeRule\" , \"events:ListTagsForResource\" , \"events:ListTargetsByRule\" , \"events:PutRule\" , \"events:PutTargets\" , \"events:RemoveTargets\" , \"iam:AddRoleToInstanceProfile\" , \"iam:AttachRolePolicy\" , \"iam:CreateInstanceProfile\" , \"iam:CreateOpenIDConnectProvider\" , \"iam:CreatePolicy\" , \"iam:CreateRole\" , \"iam:CreateServiceLinkedRole\" , \"iam:DeleteInstanceProfile\" , \"iam:DeleteOpenIDConnectProvider\" , \"iam:DeletePolicy\" , \"iam:DeleteRole\" , \"iam:DetachRolePolicy\" , \"iam:GetInstanceProfile\" , \"iam:GetOpenIDConnectProvider\" , \"iam:GetPolicy\" , \"iam:GetPolicyVersion\" , \"iam:GetRole\" , \"iam:ListAttachedRolePolicies\" , \"iam:ListInstanceProfilesForRole\" , \"iam:ListPolicyVersions\" , \"iam:ListRolePolicies\" , \"iam:PassRole\" , \"iam:RemoveRoleFromInstanceProfile\" , \"iam:TagInstanceProfile\" , \"iam:UpdateAssumeRolePolicy\" , \"kms:CreateAlias\" , \"kms:CreateKey\" , \"kms:DeleteAlias\" , \"kms:DescribeKey\" , \"kms:EnableKeyRotation\" , \"kms:GetKeyPolicy\" , \"kms:GetKeyRotationStatus\" , \"kms:ListAliases\" , \"kms:ListResourceTags\" , \"kms:PutKeyPolicy\" , \"kms:ScheduleKeyDeletion\" , \"kms:TagResource\" , \"logs:CreateLogGroup\" , \"logs:DeleteLogGroup\" , \"logs:DescribeLogGroups\" , \"logs:ListTagsLogGroup\" , \"logs:PutRetentionPolicy\" , \"s3:CreateBucket\" , \"s3:DeleteBucket\" , \"s3:DeleteBucketOwnershipControls\" , \"s3:DeleteBucketPolicy\" , \"s3:DeleteObject\" , \"s3:GetAccelerateConfiguration\" , \"s3:GetBucketAcl\" , \"s3:GetBucketCORS\" , \"s3:GetBucketLogging\" , \"s3:GetBucketObjectLockConfiguration\" , \"s3:GetBucketOwnershipControls\" , \"s3:GetBucketPolicy\" , \"s3:GetBucketPublicAccessBlock\" , \"s3:GetBucketRequestPayment\" , \"s3:GetBucketTagging\" , \"s3:GetBucketVersioning\" , \"s3:GetBucketWebsite\" , \"s3:GetEncryptionConfiguration\" , \"s3:GetLifecycleConfiguration\" , \"s3:GetObject\" , \"s3:GetObjectTagging\" , \"s3:GetObjectVersion\" , \"s3:GetReplicationConfiguration\" , \"s3:ListAllMyBuckets\" , \"s3:ListBucket\" , \"s3:PutBucketAcl\" , \"s3:PutBucketOwnershipControls\" , \"s3:PutBucketPolicy\" , \"s3:PutBucketPublicAccessBlock\" , \"s3:PutBucketTagging\" , \"s3:PutBucketVersioning\" , \"s3:PutEncryptionConfiguration\" , \"s3:PutObject\" , \"secretsmanager:CreateSecret\" , \"secretsmanager:DeleteSecret\" , \"secretsmanager:DescribeSecret\" , \"secretsmanager:GetResourcePolicy\" , \"secretsmanager:GetSecretValue\" , \"secretsmanager:PutSecretValue\" , \"sqs:CreateQueue\" , \"sqs:DeleteQueue\" , \"sqs:GetQueueAttributes\" , \"sqs:ListQueueTags\" , \"sqs:SetQueueAttributes\" , \"sqs:TagQueue\" , \"sts:GetCallerIdentity\" ], \"Resource\" : \"*\" } ] } How this policy was generated? \u00b6 For each example we run in the E2E workflow, we run iamlive in the background in CSM mode to help generate the policy. After generating the policy for each example, we merge the generated policies into a single policy shown above. To learn more about the implementation you can review the GitHub workflow itself","title":"Minimum IAM policy"},{"location":"iam/minimum-iam-policy/#minimum-iam-policy","text":"This document describes the minimum IAM policy required to run core examples that we run in our E2E workflow , mainly focused on the list of IAM actions. Note : The policy resource is set as * to allow all resources, this is not a recommended practice. { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Effect\" : \"Allow\" , \"Action\" : [ \"aps:CreateAlertManagerDefinition\" , \"aps:CreateWorkspace\" , \"aps:DeleteAlertManagerDefinition\" , \"aps:DeleteWorkspace\" , \"aps:DescribeAlertManagerDefinition\" , \"aps:DescribeWorkspace\" , \"aps:ListTagsForResource\" , \"autoscaling:CreateAutoScalingGroup\" , \"autoscaling:CreateOrUpdateTags\" , \"autoscaling:DeleteAutoScalingGroup\" , \"autoscaling:DeleteLifecycleHook\" , \"autoscaling:DeleteTags\" , \"autoscaling:DescribeAutoScalingGroups\" , \"autoscaling:DescribeLifecycleHooks\" , \"autoscaling:DescribeTags\" , \"autoscaling:PutLifecycleHook\" , \"autoscaling:SetInstanceProtection\" , \"autoscaling:UpdateAutoScalingGroup\" , \"ec2:AllocateAddress\" , \"ec2:AssociateRouteTable\" , \"ec2:AttachInternetGateway\" , \"ec2:AuthorizeSecurityGroupEgress\" , \"ec2:AuthorizeSecurityGroupIngress\" , \"ec2:CreateEgressOnlyInternetGateway\" , \"ec2:CreateInternetGateway\" , \"ec2:CreateLaunchTemplate\" , \"ec2:CreateNatGateway\" , \"ec2:CreateNetworkAclEntry\" , \"ec2:CreateRoute\" , \"ec2:CreateRouteTable\" , \"ec2:CreateSecurityGroup\" , \"ec2:CreateSubnet\" , \"ec2:CreateTags\" , \"ec2:CreateVpc\" , \"ec2:DeleteEgressOnlyInternetGateway\" , \"ec2:DeleteInternetGateway\" , \"ec2:DeleteLaunchTemplate\" , \"ec2:DeleteNatGateway\" , \"ec2:DeleteNetworkAclEntry\" , \"ec2:DeleteRoute\" , \"ec2:DeleteRouteTable\" , \"ec2:DeleteSecurityGroup\" , \"ec2:DeleteSubnet\" , \"ec2:DeleteTags\" , \"ec2:DeleteVpc\" , \"ec2:DescribeAccountAttributes\" , \"ec2:DescribeAddresses\" , \"ec2:DescribeAvailabilityZones\" , \"ec2:DescribeEgressOnlyInternetGateways\" , \"ec2:DescribeImages\" , \"ec2:DescribeInternetGateways\" , \"ec2:DescribeLaunchTemplateVersions\" , \"ec2:DescribeLaunchTemplates\" , \"ec2:DescribeNatGateways\" , \"ec2:DescribeNetworkAcls\" , \"ec2:DescribeNetworkInterfaces\" , \"ec2:DescribeRouteTables\" , \"ec2:DescribeSecurityGroups\" , \"ec2:DescribeSubnets\" , \"ec2:DescribeTags\" , \"ec2:DescribeVpcAttribute\" , \"ec2:DescribeVpcClassicLink\" , \"ec2:DescribeVpcClassicLinkDnsSupport\" , \"ec2:DescribeVpcs\" , \"ec2:DetachInternetGateway\" , \"ec2:DisassociateRouteTable\" , \"ec2:ModifySubnetAttribute\" , \"ec2:ModifyVpcAttribute\" , \"ec2:ReleaseAddress\" , \"ec2:RevokeSecurityGroupEgress\" , \"ec2:RevokeSecurityGroupIngress\" , \"eks:CreateAddon\" , \"eks:CreateCluster\" , \"eks:CreateFargateProfile\" , \"eks:CreateNodegroup\" , \"eks:DeleteAddon\" , \"eks:DeleteCluster\" , \"eks:DeleteFargateProfile\" , \"eks:DeleteNodegroup\" , \"eks:DescribeAddon\" , \"eks:DescribeAddonVersions\" , \"eks:DescribeCluster\" , \"eks:DescribeFargateProfile\" , \"eks:DescribeNodegroup\" , \"elasticfilesystem:CreateFileSystem\" , \"elasticfilesystem:CreateMountTarget\" , \"elasticfilesystem:DeleteFileSystem\" , \"elasticfilesystem:DeleteMountTarget\" , \"elasticfilesystem:DescribeFileSystems\" , \"elasticfilesystem:DescribeLifecycleConfiguration\" , \"elasticfilesystem:DescribeMountTargetSecurityGroups\" , \"elasticfilesystem:DescribeMountTargets\" , \"emr-containers:CreateVirtualCluster\" , \"emr-containers:DeleteVirtualCluster\" , \"emr-containers:DescribeVirtualCluster\" , \"events:DeleteRule\" , \"events:DescribeRule\" , \"events:ListTagsForResource\" , \"events:ListTargetsByRule\" , \"events:PutRule\" , \"events:PutTargets\" , \"events:RemoveTargets\" , \"iam:AddRoleToInstanceProfile\" , \"iam:AttachRolePolicy\" , \"iam:CreateInstanceProfile\" , \"iam:CreateOpenIDConnectProvider\" , \"iam:CreatePolicy\" , \"iam:CreateRole\" , \"iam:CreateServiceLinkedRole\" , \"iam:DeleteInstanceProfile\" , \"iam:DeleteOpenIDConnectProvider\" , \"iam:DeletePolicy\" , \"iam:DeleteRole\" , \"iam:DetachRolePolicy\" , \"iam:GetInstanceProfile\" , \"iam:GetOpenIDConnectProvider\" , \"iam:GetPolicy\" , \"iam:GetPolicyVersion\" , \"iam:GetRole\" , \"iam:ListAttachedRolePolicies\" , \"iam:ListInstanceProfilesForRole\" , \"iam:ListPolicyVersions\" , \"iam:ListRolePolicies\" , \"iam:PassRole\" , \"iam:RemoveRoleFromInstanceProfile\" , \"iam:TagInstanceProfile\" , \"iam:UpdateAssumeRolePolicy\" , \"kms:CreateAlias\" , \"kms:CreateKey\" , \"kms:DeleteAlias\" , \"kms:DescribeKey\" , \"kms:EnableKeyRotation\" , \"kms:GetKeyPolicy\" , \"kms:GetKeyRotationStatus\" , \"kms:ListAliases\" , \"kms:ListResourceTags\" , \"kms:PutKeyPolicy\" , \"kms:ScheduleKeyDeletion\" , \"kms:TagResource\" , \"logs:CreateLogGroup\" , \"logs:DeleteLogGroup\" , \"logs:DescribeLogGroups\" , \"logs:ListTagsLogGroup\" , \"logs:PutRetentionPolicy\" , \"s3:CreateBucket\" , \"s3:DeleteBucket\" , \"s3:DeleteBucketOwnershipControls\" , \"s3:DeleteBucketPolicy\" , \"s3:DeleteObject\" , \"s3:GetAccelerateConfiguration\" , \"s3:GetBucketAcl\" , \"s3:GetBucketCORS\" , \"s3:GetBucketLogging\" , \"s3:GetBucketObjectLockConfiguration\" , \"s3:GetBucketOwnershipControls\" , \"s3:GetBucketPolicy\" , \"s3:GetBucketPublicAccessBlock\" , \"s3:GetBucketRequestPayment\" , \"s3:GetBucketTagging\" , \"s3:GetBucketVersioning\" , \"s3:GetBucketWebsite\" , \"s3:GetEncryptionConfiguration\" , \"s3:GetLifecycleConfiguration\" , \"s3:GetObject\" , \"s3:GetObjectTagging\" , \"s3:GetObjectVersion\" , \"s3:GetReplicationConfiguration\" , \"s3:ListAllMyBuckets\" , \"s3:ListBucket\" , \"s3:PutBucketAcl\" , \"s3:PutBucketOwnershipControls\" , \"s3:PutBucketPolicy\" , \"s3:PutBucketPublicAccessBlock\" , \"s3:PutBucketTagging\" , \"s3:PutBucketVersioning\" , \"s3:PutEncryptionConfiguration\" , \"s3:PutObject\" , \"secretsmanager:CreateSecret\" , \"secretsmanager:DeleteSecret\" , \"secretsmanager:DescribeSecret\" , \"secretsmanager:GetResourcePolicy\" , \"secretsmanager:GetSecretValue\" , \"secretsmanager:PutSecretValue\" , \"sqs:CreateQueue\" , \"sqs:DeleteQueue\" , \"sqs:GetQueueAttributes\" , \"sqs:ListQueueTags\" , \"sqs:SetQueueAttributes\" , \"sqs:TagQueue\" , \"sts:GetCallerIdentity\" ], \"Resource\" : \"*\" } ] }","title":"Minimum IAM policy"},{"location":"iam/minimum-iam-policy/#how-this-policy-was-generated","text":"For each example we run in the E2E workflow, we run iamlive in the background in CSM mode to help generate the policy. After generating the policy for each example, we merge the generated policies into a single policy shown above. To learn more about the implementation you can review the GitHub workflow itself","title":"How this policy was generated?"},{"location":"internal/ci/","text":"E2E tests \u00b6 We use GitHub Actions to run an end-to-end tests to verify all PRs. The GitHub Actions used are a combination of aws-actions/configure-aws-credentials and hashicorp/setup-terraform@v1 . See the complete action definition here . Setup \u00b6 Use the following CloudFormation template to setup a new IAM role. Parameters : GitHubOrg : Type : String RepositoryName : Type : String OIDCProviderArn : Description : Arn for the GitHub OIDC Provider. Default : \"\" Type : String Conditions : CreateOIDCProvider : !Equals - !Ref OIDCProviderArn - \"\" Resources : Role : Type : AWS::IAM::Role Properties : AssumeRolePolicyDocument : Statement : - Effect : Allow Action : sts:AssumeRoleWithWebIdentity Principal : Federated : !If - CreateOIDCProvider - !Ref GithubOidc - !Ref OIDCProviderArn Condition : StringLike : token.actions.githubusercontent.com:sub : !Sub repo:${GitHubOrg}/${RepositoryName}:* GithubOidc : Type : AWS::IAM::OIDCProvider Condition : CreateOIDCProvider Properties : Url : https://token.actions.githubusercontent.com ClientIdList : - sts.amazonaws.com ThumbprintList : - a031c46782e6e6c662c2c87c76da9aa62ccabd8e Outputs : Role : Value : !GetAtt Role.Arn Add a permissible IAM Policy to the above create role. For our purpose AdministratorAccess works the best. Setup a GitHub repo secret called ROLE_TO_ASSUME and set it to ARN of the role created in 1. We use an S3 backend to test the canonical example . This allows us to recover from any failures during the apply stage. If you are setting up your own CI pipeline change the s3 bucket name in backend configuration of the example.","title":"E2E tests"},{"location":"internal/ci/#e2e-tests","text":"We use GitHub Actions to run an end-to-end tests to verify all PRs. The GitHub Actions used are a combination of aws-actions/configure-aws-credentials and hashicorp/setup-terraform@v1 . See the complete action definition here .","title":"E2E tests"},{"location":"internal/ci/#setup","text":"Use the following CloudFormation template to setup a new IAM role. Parameters : GitHubOrg : Type : String RepositoryName : Type : String OIDCProviderArn : Description : Arn for the GitHub OIDC Provider. Default : \"\" Type : String Conditions : CreateOIDCProvider : !Equals - !Ref OIDCProviderArn - \"\" Resources : Role : Type : AWS::IAM::Role Properties : AssumeRolePolicyDocument : Statement : - Effect : Allow Action : sts:AssumeRoleWithWebIdentity Principal : Federated : !If - CreateOIDCProvider - !Ref GithubOidc - !Ref OIDCProviderArn Condition : StringLike : token.actions.githubusercontent.com:sub : !Sub repo:${GitHubOrg}/${RepositoryName}:* GithubOidc : Type : AWS::IAM::OIDCProvider Condition : CreateOIDCProvider Properties : Url : https://token.actions.githubusercontent.com ClientIdList : - sts.amazonaws.com ThumbprintList : - a031c46782e6e6c662c2c87c76da9aa62ccabd8e Outputs : Role : Value : !GetAtt Role.Arn Add a permissible IAM Policy to the above create role. For our purpose AdministratorAccess works the best. Setup a GitHub repo secret called ROLE_TO_ASSUME and set it to ARN of the role created in 1. We use an S3 backend to test the canonical example . This allows us to recover from any failures during the apply stage. If you are setting up your own CI pipeline change the s3 bucket name in backend configuration of the example.","title":"Setup"},{"location":"modules/emr-on-eks/","text":"EMR on EKS \u00b6 EMR on EKS is a deployment option in EMR that allows you to automate the provisioning and management of open-source big data frameworks on EKS. This module deploys the necessary resources to run EMR Spark Jobs on EKS Cluster. Create a new Namespace to run Spark workloads Create K8s Role and Role Binding to allow the username emr-containers on a given namespace( spark ) Create RBAC permissions and adding EMR on EKS service-linked role into aws-auth configmap Enables IAM Roles for Service Account (IRSA) Update trust relationship for job execution role Usage \u00b6 EMR on EKS can be deployed by enabling the module via the following. Checkout this Blog to setup Observability for EMR on EKS Spark Jobs #--------------------------------------- # ENABLE EMR ON EKS #--------------------------------------- enable_emr_on_eks = true emr_on_eks_teams = { emr-team-a = { namespace = \"emr-data-team-a\" job_execution_role = \"emr-eks-data-team-a\" additional_iam_policies = [\"<ENTER-IAM-POLICY-ARN>\"] } emr-team-b = { namespace = \"emr-data-team-b\" job_execution_role = \"emr-eks-data-team-b\" additional_iam_policies = [\"<ENTER-IAM-POLICY-ARN>\"] } } Once deployed, you can create Virtual EMR Cluster and execute Spark jobs. See the document below for more details.","title":"EMR on EKS"},{"location":"modules/emr-on-eks/#emr-on-eks","text":"EMR on EKS is a deployment option in EMR that allows you to automate the provisioning and management of open-source big data frameworks on EKS. This module deploys the necessary resources to run EMR Spark Jobs on EKS Cluster. Create a new Namespace to run Spark workloads Create K8s Role and Role Binding to allow the username emr-containers on a given namespace( spark ) Create RBAC permissions and adding EMR on EKS service-linked role into aws-auth configmap Enables IAM Roles for Service Account (IRSA) Update trust relationship for job execution role","title":"EMR on EKS"},{"location":"modules/emr-on-eks/#usage","text":"EMR on EKS can be deployed by enabling the module via the following. Checkout this Blog to setup Observability for EMR on EKS Spark Jobs #--------------------------------------- # ENABLE EMR ON EKS #--------------------------------------- enable_emr_on_eks = true emr_on_eks_teams = { emr-team-a = { namespace = \"emr-data-team-a\" job_execution_role = \"emr-eks-data-team-a\" additional_iam_policies = [\"<ENTER-IAM-POLICY-ARN>\"] } emr-team-b = { namespace = \"emr-data-team-b\" job_execution_role = \"emr-eks-data-team-b\" additional_iam_policies = [\"<ENTER-IAM-POLICY-ARN>\"] } } Once deployed, you can create Virtual EMR Cluster and execute Spark jobs. See the document below for more details.","title":"Usage"}]}