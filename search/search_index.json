{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#amazon-eks-blueprints-for-terraform","title":"Amazon EKS Blueprints for Terraform","text":"<p>Welcome to Amazon EKS Blueprints for Terraform!</p> <p>This project contains a collection of Amazon EKS cluster patterns implemented in Terraform that demonstrate how fast and easy it is for customers to adopt Amazon EKS. The patterns can be used by AWS customers, partners, and internal AWS teams to configure and manage complete EKS clusters that are fully bootstrapped with the operational software that is needed to deploy and operate workloads.</p>"},{"location":"#motivation","title":"Motivation","text":"<p>Kubernetes is a powerful and extensible container orchestration technology that allows you to deploy and manage containerized applications at scale. The extensible nature of Kubernetes also allows you to use a wide range of popular open-source tools in Kubernetes clusters. However, With the wide array of tooling and design choices available, configuring an EKS cluster that meets your organization\u2019s specific needs can take a significant amount of time. It involves integrating a wide range of open-source tools and AWS services as well as expertise in AWS and Kubernetes.</p> <p>AWS customers have asked for patterns that demonstrate how to integrate the landscape of Kubernetes tools and make it easy for them to provision complete, opinionated EKS clusters that meet specific application requirements. Customers can utilize EKS Blueprints to configure and deploy purpose built EKS clusters, and start onboarding workloads in days, rather than months.</p>"},{"location":"#consumption","title":"Consumption","text":"<p>EKS Blueprints for Terraform has been designed to be consumed in the following manners:</p> <ol> <li>Reference: Users can refer to the patterns and snippets provided to help guide them to their desired solution. Users will typically view how the pattern or snippet is configured to achieve the desired end result and then replicate that in their environment.</li> <li>Copy &amp; Paste: Users can copy and paste the patterns and snippets into their own environment, using EKS Blueprints as the starting point for their implementation. Users can then adapt the initial pattern to customize it to their specific needs.</li> </ol> <p>EKS Blueprints for Terraform are not intended to be consumed as-is directly from this project. In \"Terraform speak\" - the patterns and snippets provided in this repository are not designed to be consumed as a Terraform module. Therefore, the patterns provided only contain <code>variables</code> when certain information is required to deploy the pattern (i.e. - a Route53 hosted zone ID, or ACM certificate ARN) and generally use local variables. If you wish to deploy the patterns into a different region or with other changes, it is recommended that you make those modifications locally before applying the pattern. EKS Blueprints for Terraform will not expose variables and outputs in the same manner that Terraform modules follow in order to avoid confusion around the consumption model.</p> <p>However, we do have a number of Terraform modules that were created to support EKS Blueprints in addition to the community hosted modules. Please see the respective projects for more details on the modules constructed to support EKS Blueprints for Terraform; those projects are listed below.</p> <ul> <li><code>terraform-aws-eks-blueprint-addon</code> - (Note the singular form) Terraform module which can provision an addon using the Terraform <code>helm_release</code> resource in addition to an IAM role for service account (IRSA).</li> <li><code>terraform-aws-eks-blueprint-addons</code> - (Note the plural form) Terraform module which can provision multiple addons; both EKS addons using the <code>aws_eks_addon</code> resource as well as Helm chart based addons using the <code>terraform-aws-eks-blueprint-addon</code> module.</li> <li><code>terraform-aws-eks-blueprints-teams</code> - Terraform module that creates Kubernetes multi-tenancy resources and configurations, allowing both administrators and application developers to access only the resources which they are responsible for.</li> </ul>"},{"location":"#related-projects","title":"Related Projects","text":"<p>In addition to the supporting EKS Blueprints Terraform modules listed above, there are a number of related projects that users should be aware of:</p> <ol> <li> <p>GitOps</p> <ul> <li><code>terraform-aws-eks-ack-addons</code> -   Terraform module to deploy ACK controllers onto EKS clusters</li> <li><code>crossplane-on-eks</code> - Crossplane Blueprints is an open source repo to bootstrap Amazon EKS clusters and provision AWS resources using a library of Crossplane Compositions (XRs) with Composite Resource Definitions (XRDs).</li> </ul> </li> <li> <p>Data on EKS</p> <ul> <li><code>data-on-eks</code> - A collection of blueprints intended for data workloads on Amazon EKS.</li> <li><code>terraform-aws-eks-data-addons</code> - Terraform module to deploy multiple addons that are specific to data workloads on EKS clusters.</li> </ul> </li> <li> <p>Observability Accelerator</p> <ul> <li><code>terraform-aws-observability-accelerator</code> - A set of opinionated modules to help you set up observability for your AWS environments with AWS-managed observability services such as Amazon Managed Service for Prometheus, Amazon Managed Grafana, AWS Distro for OpenTelemetry (ADOT) and Amazon CloudWatch</li> </ul> </li> <li> <p>Karpenter Blueprints</p> </li> <li><code>karpenter-blueprints</code> - includes a list of common workload scenarios,    some of them go in depth with the explanation of why configuring Karpenter and Kubernetes objects in such a way is important.</li> </ol>"},{"location":"#terraform-caveats","title":"Terraform Caveats","text":"<p>EKS Blueprints for Terraform does not intend to teach users the recommended practices for Terraform nor does it offer guidance on how users should structure their Terraform projects. The patterns provided are intended to show users how they can achieve a defined architecture or configuration in a way that they can quickly and easily get up and running to start interacting with that pattern. Therefore, there are a few caveats users should be aware of when using EKS Blueprints for Terraform:</p> <ol> <li> <p>We recognize that most users will already have an existing VPC in a separate Terraform workspace. However, the patterns provided come complete with a VPC to ensure a stable, deployable example that has been tested and validated.</p> </li> <li> <p>Hashicorp does not recommend providing computed values in provider blocks , which means that the cluster configuration should be defined in a workspace separate from the resources deployed onto the cluster (i.e. - addons). However, to simplify the pattern experience, we have defined everything in one workspace and provided instructions to provision the patterns using a targeted apply approach. Users are encouraged to investigate a Terraform project structure that suites their needs; EKS Blueprints for Terraform does not have an opinion in this matter and will defer to Hashicorp's guidance.</p> </li> <li> <p>Patterns are not intended to be consumed in-place in the same manner that one would consume a module. Therefore, we do not provide variables and outputs to expose various levels of configuration for the examples. Users can modify the pattern locally after cloning to suite their requirements.</p> </li> <li> <p>Please see the FAQ section on authenticating Kubernetes based providers (<code>kubernetes</code>, <code>helm</code>, <code>kubectl</code>) to Amazon EKS clusters regarding the use of static tokens versus dynamic tokens using the <code>awscli</code>.</p> </li> </ol>"},{"location":"#support-feedback","title":"Support &amp; Feedback","text":"<p>EKS Blueprints for Terraform is maintained by AWS Solution Architects. It is not part of an AWS service and support is provided as a best-effort by the EKS Blueprints community. To provide feedback, please use the issues templates provided. If you are interested in contributing to EKS Blueprints, see the Contribution guide.</p>"},{"location":"#security","title":"Security","text":"<p>See CONTRIBUTING for more information.</p>"},{"location":"#license","title":"License","text":"<p>Apache-2.0 Licensed. See LICENSE.</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#timeouts-on-destroy","title":"Timeouts on destroy","text":"<p>Customers who are deleting their environments using <code>terraform destroy</code> may see timeout errors when VPCs are being deleted. This is due to a known issue in the vpc-cni</p> <p>Customers may face a situation where ENIs that were attached to EKS managed nodes (same may apply to self-managed nodes) are not being deleted by the VPC CNI as expected which leads to IaC tool failures, such as:</p> <ul> <li>ENIs are left on subnets</li> <li>EKS managed security group which is attached to the ENI can\u2019t be deleted by EKS</li> </ul> <p>The current recommendation is to execute cleanup in the following order:</p> <ol> <li>delete all pods that have been created in the cluster.</li> <li>add delay/ wait</li> <li>delete VPC CNI</li> <li>delete nodes</li> <li>delete cluster</li> </ol>"},{"location":"faq/#leaked-cloudwatch-logs-group","title":"Leaked CloudWatch Logs Group","text":"<p>Sometimes, customers may see the CloudWatch Log Group for EKS cluster being created is left behind after their blueprint has been destroyed using <code>terraform destroy</code>. This happens because even after terraform deletes the CW log group, there\u2019s still logs being processed behind the scene by AWS EKS and service continues to write logs after recreating the log group using the EKS service IAM role which users don't have control over. This results in a terraform failure when the same blueprint is being recreated due to the existing log group left behind.</p> <p>There are two options here:</p> <ol> <li> <p>During cluster creation set <code>var.create_cloudwatch_log_group</code> to <code>false</code>. This will tell the EKS module to not create the log group, but instead let the EKS service create the log group. This means that upon cluster deletion the log group will be left behind but there will not be Terraform failures if you re-create the same cluster as Terraform does not manage the log group creation/deletion anymore.</p> </li> <li> <p>During cluster creation set <code>var.create_cloudwatch_log_group</code> to <code>true</code>. This will tell the EKS module to create the log group via Terraform. The EKS service will detect the log group and will start forwarding the logs for the log types enabled. Upon deletion terraform will delete the log group but depending upon any un-forwarded logs, the EKS service may recreate log group using the service role. This will result in terraform errors if the same blueprint is recreated. To proceed, manually delete the log group using the console or cli rerun the <code>terraform destroy</code>.</p> </li> </ol>"},{"location":"faq/#provider-authentication","title":"Provider Authentication","text":"<p>The chain of events when provisioning an example is typically in the stages of VPC -&gt; EKS cluster -&gt; addons and manifests. Per Terraform's recommendation, it is not recommended to pass an unknown value into provider configurations. However, for the sake of simplicity and ease of use, Blueprints does specify the AWS provider along with the Kubernetes, Helm, and Kubectl providers in order to show the full configuration required for provisioning example. Note - this is the configuration required to provision the example, not necessarily the shape of how the configuration should be structured; users are encouraged to split up EKS cluster creation from addon and manifest provisioning to align with Terraform's recommendations.</p> <p>With that said, the examples here are combining the providers and users can sometimes encounter various issues with the provider authentication methods. There are primarily two methods for authenticating the Kubernetes, Helm, and Kubectl providers to the EKS cluster created:</p> <ol> <li>Using a static token which has a lifetime of 15 minutes per the EKS service documentation.</li> <li>Using the <code>exec()</code> method which will fetch a token at the time of Terraform invocation.</li> </ol> <p>The Kubernetes and Helm providers recommend the <code>exec()</code> method, however this has the caveat that it requires the awscli to be installed on the machine running Terraform AND of at least a minimum version to support the API spec used by the provider (i.e. - <code>\"client.authentication.k8s.io/v1alpha1\"</code>, <code>\"client.authentication.k8s.io/v1beta1\"</code>, etc.). Selecting the appropriate provider authentication method is left up to users, and the examples used in this project will default to using the static token method for ease of use.</p> <p>Users of the static token method should be aware that if they receive a <code>401 Unauthorized</code> message, they might have a token that has expired and will need to run <code>terraform refresh</code> to get a new token. Users of the <code>exec()</code> method should be aware that the <code>exec()</code> method is reliant on the awscli and the associated authentication API version; the awscli version may need to be updated to support a later API version required by the Kubernetes version in use.</p> <p>The following examples demonstrate either method that users can utilize - please refer to the associated provider's documentation for further details on configuration.</p>"},{"location":"faq/#static-token-example","title":"Static Token Example","text":"<pre><code>provider \"kubernetes\" {\n  host                   = module.eks.cluster_endpoint\n  cluster_ca_certificate = base64decode(module.eks.cluster_certificate_authority_data)\n  token                  = data.aws_eks_cluster_auth.this.token\n}\n\nprovider \"helm\" {\n  kubernetes {\n    host                   = module.eks.cluster_endpoint\n    cluster_ca_certificate = base64decode(module.eks.cluster_certificate_authority_data)\n    token                  = data.aws_eks_cluster_auth.this.token\n  }\n}\n\nprovider \"kubectl\" {\n  apply_retry_count      = 10\n  host                   = module.eks.cluster_endpoint\n  cluster_ca_certificate = base64decode(module.eks.cluster_certificate_authority_data)\n  load_config_file       = false\n  token                  = data.aws_eks_cluster_auth.this.token\n}\n\ndata \"aws_eks_cluster_auth\" \"this\" {\n  name = module.eks.cluster_name\n}\n</code></pre>"},{"location":"faq/#exec-example","title":"<code>exec()</code> Example","text":"<p>Usage of exec plugin for AWS credentials</p> <p>Links to References related to this issue</p> <ul> <li>https://github.com/hashicorp/terraform/issues/29182</li> <li>https://github.com/aws/aws-cli/pull/6476</li> </ul> <pre><code>provider \"kubernetes\" {\n  host                   = module.eks.cluster_endpoint\n  cluster_ca_certificate = base64decode(module.eks.cluster_certificate_authority_data)\n\n  exec {\n    api_version = \"client.authentication.k8s.io/v1beta1\"\n    command     = \"aws\"\n    # This requires the awscli to be installed locally where Terraform is executed\n    args = [\"eks\", \"get-token\", \"--cluster-name\", module.eks.cluster_name]\n  }\n}\n\nprovider \"helm\" {\n  kubernetes {\n    host                   = module.eks.cluster_endpoint\n    cluster_ca_certificate = base64decode(module.eks.cluster_certificate_authority_data)\n\n    exec {\n      api_version = \"client.authentication.k8s.io/v1beta1\"\n      command     = \"aws\"\n      # This requires the awscli to be installed locally where Terraform is executed\n      args = [\"eks\", \"get-token\", \"--cluster-name\", module.eks.cluster_name]\n    }\n  }\n}\n\nprovider \"kubectl\" {\n  apply_retry_count      = 5\n  host                   = module.eks.cluster_endpoint\n  cluster_ca_certificate = base64decode(module.eks.cluster_certificate_authority_data)\n  load_config_file       = false\n\n  exec {\n    api_version = \"client.authentication.k8s.io/v1beta1\"\n    command     = \"aws\"\n    # This requires the awscli to be installed locally where Terraform is executed\n    args = [\"eks\", \"get-token\", \"--cluster-name\", module.eks.cluster_name]\n  }\n}\n</code></pre>"},{"location":"faq/#unable-to-destroy-namespace-created-by-terraform","title":"Unable to destroy namespace created by Terraform","text":"<p>In some cases, when you try to run terraform destroy on kubernetes resources created by Terraform such as namespace, you may end up seeing failures such as timeout and context deadline exceeded failures. Namespace one of those resources we've seen before, the main reason this happens is because orphaned resources created through CRDs of addons (such as ArgoCD, AWS LBC and more) are left behind after the addons are being deleted, this is case by case scenario. For example, with namespaces:</p> <ol> <li> <p>Confirm the namespace is hanging in status <code>Terminating</code></p> <pre><code>kubectl get namespaces\n</code></pre> </li> <li> <p>Check for any orphaned resources in the namespace, make sure to replace  with your namespace: <pre><code>kubectl api-resources --verbs=list --namespaced -o name   | xargs -n 1 kubectl get  \\\n--show-kind --ignore-not-found -n &lt;namespace_name&gt;\n</code></pre> <li> <p>For any of the above output, patch the resource finalize:</p> <pre><code>kubectl patch RESOURCE NAME -p '{\"metadata\":{\"finalizers\":[]}}' --type=merge\n</code></pre> </li> <li> <p>Check the status of the namespace, if needed you may need to patch the namespace finalizers as-well</p> <pre><code>kubectl patch ns &lt;ns-name&gt; -p '{\"spec\":{\"finalizers\":null}}'\n</code></pre> </li>"},{"location":"getting-started/","title":"Getting Started","text":"<p>This getting started guide will help you deploy your first pattern using EKS Blueprints.</p>"},{"location":"getting-started/#prerequisites","title":"Prerequisites","text":"<p>Ensure that you have installed the following tools locally:</p> <ul> <li>awscli</li> <li>kubectl</li> <li>terraform</li> </ul>"},{"location":"getting-started/#deploy","title":"Deploy","text":"<ol> <li> <p>For consuming EKS Blueprints, please see the Consumption section. For exploring and trying out the patterns provided, please clone the project locally to quickly get up and running with a pattern. After cloning the project locally, <code>cd</code> into the pattern directory of your choice.</p> </li> <li> <p>To provision the pattern, the typical steps of execution are as follows:</p> <pre><code>terraform init\nterraform apply -target=\"module.vpc\" -auto-approve\nterraform apply -target=\"module.eks\" -auto-approve\nterraform apply -auto-approve\n</code></pre> <p>For patterns that deviate from this general flow, see the pattern's respective <code>README.md</code> for more details.</p> <p>Terraform targeted apply</p> <p>Please see the Terraform Caveats section for details on the use of targeted Terraform apply's</p> </li> <li> <p>Once all of the resources have successfully been provisioned, the following command can be used to update the <code>kubeconfig</code> on your local machine and allow you to interact with your EKS Cluster using <code>kubectl</code>.</p> <pre><code>aws eks --region &lt;REGION&gt; update-kubeconfig --name &lt;CLUSTER_NAME&gt; --alias &lt;CLUSTER_NAME&gt;\n</code></pre> <p>Pattern Terraform outputs</p> <p>Most examples will output the <code>aws eks update-kubeconfig ...</code> command as part of the Terraform apply output to simplify this process for users</p> <p>Private clusters</p> <p>Clusters that do not enable the clusters public endpoint will require users to access the cluster from within the VPC. For these patterns, a sample EC2 or other means are provided to demonstrate how to access those clusters privately</p> <p>and without exposing the public endpoint. Please see the respective pattern's <code>README.md</code> for more details.</p> </li> <li> <p>Once you have updated your <code>kubeconfig</code>, you can verify that you are able to interact with your cluster by running the following command:</p> <pre><code>kubectl get nodes\n</code></pre> <p>This should return a list of the node(s) running in the cluster created. If any errors are encountered, please re-trace the steps above and consult the pattern's <code>README.md</code> for more details on any additional/specific steps that may be required.</p> </li> </ol>"},{"location":"getting-started/#destroy","title":"Destroy","text":"<p>To teardown and remove the resources created in the pattern, the typical steps of execution are as follows:</p> <pre><code>terraform destroy -target=\"module.eks_blueprints_addons\" -auto-approve\nterraform destroy -target=\"module.eks\" -auto-approve\nterraform destroy -auto-approve\n</code></pre> <p>Resources created outside of Terraform</p> <p>Depending on the pattern, some resources may have been created that Terraform is not aware of that will cause issues when attempting to clean up the pattern. For example, Karpenter is responsible for creating additional EC2 instances to satisfy the pod scheduling requirements. These instances will not be cleaned up by Terraform and will need to be de-provisioned BEFORE attempting to <code>terraform destroy</code>. This is why it is important that the addons, or any resources provisioned onto the cluster are cleaned up first. Please see the respective pattern's <code>README.md</code> for more details.</p>"},{"location":"_partials/destroy/","title":"Destroy","text":"<pre><code>terraform destroy -target=\"module.eks_blueprints_addons\" -auto-approve\nterraform destroy -target=\"module.eks\" -auto-approve\nterraform destroy -auto-approve\n</code></pre> <p>See here for more details on cleaning up the resources created.</p>"},{"location":"internal/ci/","title":"E2E tests","text":"<p>We use GitHub Actions to run an end-to-end tests to verify all PRs. The GitHub Actions used are a combination of <code>aws-actions/configure-aws-credentials</code> and <code>hashicorp/setup-terraform@v1</code>.</p>"},{"location":"internal/ci/#setup","title":"Setup","text":"<ol> <li>Use the following CloudFormation template to setup a new IAM role.</li> </ol> <pre><code>Parameters:\n  GitHubOrg:\n    Type: String\n  RepositoryName:\n    Type: String\n  OIDCProviderArn:\n    Description: Arn for the GitHub OIDC Provider.\n    Default: \"\"\n    Type: String\n\nConditions:\n  CreateOIDCProvider: !Equals\n    - !Ref OIDCProviderArn\n    - \"\"\n\nResources:\n  Role:\n    Type: AWS::IAM::Role\n    Properties:\n      AssumeRolePolicyDocument:\n        Statement:\n          - Effect: Allow\n            Action: sts:AssumeRoleWithWebIdentity\n            Principal:\n              Federated: !If\n                - CreateOIDCProvider\n                - !Ref GithubOidc\n                - !Ref OIDCProviderArn\n            Condition:\n              StringLike:\n                token.actions.githubusercontent.com:sub: !Sub repo:${GitHubOrg}/${RepositoryName}:*\n\n  GithubOidc:\n    Type: AWS::IAM::OIDCProvider\n    Condition: CreateOIDCProvider\n    Properties:\n      Url: https://token.actions.githubusercontent.com\n      ClientIdList:\n        - sts.amazonaws.com\n      ThumbprintList:\n        - a031c46782e6e6c662c2c87c76da9aa62ccabd8e\n\nOutputs:\n  Role:\n    Value: !GetAtt Role.Arn\n</code></pre> <ol> <li> <p>Add a permissible IAM Policy to the above create role. For our purpose <code>AdministratorAccess</code> works the best.</p> </li> <li> <p>Setup a GitHub repo secret called <code>ROLE_TO_ASSUME</code> and set it to ARN of the role created in 1.</p> </li> <li> <p>We use an S3 backend for the e2e tests. This allows us to recover from any failures during the <code>apply</code> stage. If you are setting up your own CI pipeline change the s3 bucket name in backend configuration of the example.</p> </li> </ol>"},{"location":"patterns/agones-game-controller/","title":"Agones Game Controller","text":""},{"location":"patterns/agones-game-controller/#amazon-eks-deployment-with-agones-gaming-kubernetes-controller","title":"Amazon EKS Deployment with Agones Gaming Kubernetes Controller","text":"<p>This pattern shows how to deploy and run gaming applications on Amazon EKS using the Agones Kubernetes Controller</p> <p>Agones is an open source Kubernetes controller that provisions and manages dedicated game server processes within Kubernetes clusters using standard Kubernetes tooling and APIs. This model also allows any matchmaker to interact directly with Agones via the Kubernetes API to provision a dedicated game server</p> <p>Amazon GameLift enables developers to deploy, operate, and scale dedicated, low-cost servers in the cloud for session-based, multiplayer games. Built on AWS global computing infrastructure, GameLift helps deliver high-performance, high-reliability, low-cost game servers while dynamically scaling your resource usage to meet worldwide player demand. See below for more information on how GameLift FleetIQ can be integrated with Agones deployed on Amazon EKS.</p> <p>Amazon GameLift FleetIQ optimizes the use of low-cost Spot Instances for cloud-based game hosting with Amazon EC2. With GameLift FleetIQ, you can work directly with your hosting resources in Amazon EC2 and Auto Scaling while taking advantage of GameLift optimizations to deliver inexpensive, resilient game hosting for your players and makes the use of low-cost Spot Instances viable for game hosting</p> <p>This blog walks through the details of deploying EKS Cluster using eksctl and deploy Agones with GameLift FleetIQ.</p>"},{"location":"patterns/agones-game-controller/#deploy","title":"Deploy","text":"<p>See here for the prerequisites and steps to deploy this pattern.</p>"},{"location":"patterns/agones-game-controller/#validate","title":"Validate","text":"<ol> <li> <p>Deploy the sample game server</p> <pre><code>kubectl create -f https://raw.githubusercontent.com/googleforgames/agones/release-1.32.0/examples/simple-game-server/gameserver.yaml\nkubectl get gs\n</code></pre> <pre><code>NAME                       STATE   ADDRESS         PORT   NODE                                        AGE\nsimple-game-server-7r6jr   Ready   34.243.345.22   7902   ip-10-1-23-233.eu-west-1.compute.internal   11h\n</code></pre> </li> <li> <p>Test the sample game server using <code>netcat</code></p> <pre><code>echo -n \"UDP test - Hello EKS Blueprints!\" | nc -u 34.243.345.22 7902\n</code></pre> <pre><code>Hello EKS Blueprints!\nACK: Hello EKS Blueprints!\nEXIT\nACK: EXIT\n</code></pre> </li> </ol>"},{"location":"patterns/agones-game-controller/#destroy","title":"Destroy","text":"<p>Delete the resources created by the sample game server first:</p> <pre><code>kubectl -n default delete gs --all || true\n</code></pre> <pre><code>terraform destroy -target=\"module.eks_blueprints_addons\" -auto-approve\nterraform destroy -target=\"module.eks\" -auto-approve\nterraform destroy -auto-approve\n</code></pre> <p>See here for more details on cleaning up the resources created.</p>"},{"location":"patterns/blue-green-upgrade/","title":"Blue/Green Upgrade","text":""},{"location":"patterns/blue-green-upgrade/#bluegreen-migration","title":"Blue/Green Migration","text":"<p>This directory provides a solution based on EKS Blueprint for Terraform that shows how to leverage blue/green or canary application workload migration between EKS clusters, using Amazon Route 53 weighted routing feature. The workloads will be dynamically exposed using AWS LoadBalancer Controller and External DNS add-on.</p> <p>We are leveraging the existing EKS Blueprints Workloads GitHub repository sample to deploy our GitOps ArgoCD applications, which are defined as helm charts. We are leveraging ArgoCD Apps of apps pattern where an ArgoCD Application can also reference other Helm charts to deploy.</p> <p>You can also find more informations in the associated blog post</p>"},{"location":"patterns/blue-green-upgrade/#table-of-content","title":"Table of content","text":"<ul> <li>Blue/Green Migration</li> <li>Table of content</li> <li>Project structure</li> <li>Prerequisites</li> <li>Quick Start<ul> <li>Configure the Stacks</li> <li>Create the environment stack</li> <li>Create the Blue cluster</li> <li>Create the Green cluster</li> </ul> </li> <li>How this work<ul> <li>Watch our Workload: we focus on team-burnham namespace.</li> <li>Using AWS Route53 and External DNS</li> <li>Configure Ingress resources with weighted records</li> </ul> </li> <li>Automate the migration from Terraform</li> <li>Delete the Stack<ul> <li>Delete the EKS Cluster(s)</li> <li>TL;DR</li> </ul> </li> <li>Troubleshoot<ul> <li>External DNS Ownership</li> <li>Check Route 53 Record status</li> <li>Check current resolution and TTL value</li> <li>Get ArgoCD UI Password</li> </ul> </li> </ul>"},{"location":"patterns/blue-green-upgrade/#project-structure","title":"Project structure","text":"<p>See the Architecture of what we are building</p> <p> </p> <p>Our sample is composed of four main directory:</p> <ul> <li>environment \u2192 this stack will create the common VPC and its dependencies used by our EKS clusters: create a Route53 sub domain hosted zone for our sample, a wildcard certificate on Certificate Manager for our applications TLS endpoints, and a SecretManager password for the ArgoCD UIs.</li> <li>modules/eks_cluster \u2192 local module defining the EKS blueprint cluster with ArgoCD add-on which will automatically deploy additional add-ons and our demo workloads</li> <li>eks-blue \u2192 an instance of the eks_cluster module to create blue cluster</li> <li>eks-green \u2192 an instance of the eks_cluster module to create green cluster</li> </ul> <p>So we are going to create 2 EKS clusters, sharing the same VPC, and each one of them will install locally our workloads from the central GitOps repository leveraging ArgoCD add-on. In the GitOps workload repository, we have configured our applications deployments to leverage AWS Load Balancers Controllers annotations, so that applications will be exposed on AWS Load Balancers, created from our Kubernetes manifests. We will have 1 load balancer per cluster for each of our applications.</p> <p>We have configured ExternalDNS add-ons in our two clusters to share the same Route53 Hosted Zone. The workloads in both clusters also share the same Route 53 DNS records, we rely on AWS Route53 weighted records to allow us to configure canary workload migration between our two EKS clusters.</p> <p>We are leveraging the gitops-bridge-argocd-bootstrap terraform module that allow us to dynamically provide metadatas from Terraform to ArgoCD deployed in the cluster. For doing this, the module will extract all metadatas from the terraform-aws-eks-blueprints-addons module, configured to create all resources except installing the addon's Helm chart. The addon Installation will be delegate to ArgoCD Itself using the eks-blueprints-add-ons git repository containing ArgoCD ApplicaitonSets for each supported Addons.</p> <p>The gitops-bridge will create a secret in the EKS cluster containing all metadatas that will be dynamically used by ArgoCD ApplicationSets at deployment time, so that we can adapt their configuration to our EKS cluster context.</p> <p></p> <p>Our objective here is to show you how Application teams and Platform teams can configure their infrastructure and workloads so that application teams are able to deploy autonomously their workloads to the EKS clusters thanks to ArgoCD, and platform team can keep the control of migrating production workloads from one cluster to another without having to synchronized operations with applications teams, or asking them to build a complicated CD pipeline.</p> <p>In this example we show how you can seamlessly migrate your stateless workloads between the 2 clusters for a blue/green or Canary migration, but you can also leverage the same architecture to have your workloads for example separated in different accounts or regions, for either High Availability or Lower latency Access from your customers.</p>"},{"location":"patterns/blue-green-upgrade/#prerequisites","title":"Prerequisites","text":"<ul> <li>Terraform (tested version v1.3.5 on linux)</li> <li>Git</li> <li>AWS CLI</li> <li>AWS test account with administrator role access</li> <li>For working with this repository, you will need an existing Amazon Route 53 Hosted Zone that will be used to create our project hosted zone. It will be provided via the Terraform variable <code>hosted_zone_name</code> defined in terraform.tfvars.example.</li> <li>Before moving to the next step, you will need to register a parent domain with AWS Route 53 (https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/domain-register.html) in case you don\u2019t have one created yet.</li> <li>Accessing GitOps git repositories with SSH access requiring an SSH key for authentication. In this example our workloads repositories are stored in GitHub, you can see in GitHub documentation on how to connect with SSH.</li> <li>Your GitHub private ssh key value is supposed to be stored in plain text in AWS Secret Manager in a secret named <code>github-blueprint-ssh-key</code>, but you can change it using the terraform variable <code>workload_repo_secret</code> in terraform.tfvars.example.</li> <li></li> </ul>"},{"location":"patterns/blue-green-upgrade/#quick-start","title":"Quick Start","text":""},{"location":"patterns/blue-green-upgrade/#configure-the-stacks","title":"Configure the Stacks","text":"<ol> <li>Clone the repository</li> </ol> <pre><code>git clone https://github.com/aws-ia/terraform-aws-eks-blueprints.git\ncd patterns/blue-green-upgrade/\n</code></pre> <ol> <li>Copy the <code>terraform.tfvars.example</code> to <code>terraform.tfvars</code> and symlink it on each <code>environment</code>, <code>eks-blue</code> and <code>eks-green</code> folders, and change region, hosted_zone_name, eks_admin_role_name according to your needs.</li> </ol> <pre><code>cp terraform.tfvars.example terraform.tfvars\nln -s ../terraform.tfvars environment/terraform.tfvars\nln -s ../terraform.tfvars eks-blue/terraform.tfvars\nln -s ../terraform.tfvars eks-green/terraform.tfvars\n</code></pre> <ul> <li>You will need to provide the <code>hosted_zone_name</code> for example <code>my-example.com</code>. Terraform will create a new hosted zone for the project with name: <code>${environment}.${hosted_zone_name}</code> so in our example <code>eks-blueprint.my-example.com</code>.</li> <li>You need to provide a valid IAM role in <code>eks_admin_role_name</code> to have EKS cluster admin rights, generally the one uses in the EKS console.</li> </ul>"},{"location":"patterns/blue-green-upgrade/#create-the-environment-stack","title":"Create the environment stack","text":"<p>More info in the environment Readme</p> <pre><code>cd environment\nterraform init\nterraform apply\n</code></pre> <p>There can be some Warnings due to not declare variables. This is normal and you can ignore them as we share the same <code>terraform.tfvars</code> for the 3 projects by using symlinks for a unique file, and we declare some variables used for the eks-blue and eks-green directory</p>"},{"location":"patterns/blue-green-upgrade/#create-the-blue-cluster","title":"Create the Blue cluster","text":"<p>More info in the eks-blue Readme, you can also see the detailed step in the local module Readme</p> <pre><code>cd eks-blue\nterraform init\nterraform apply\n</code></pre> <p>This can take 8mn for EKS cluster, 15mn</p>"},{"location":"patterns/blue-green-upgrade/#create-the-green-cluster","title":"Create the Green cluster","text":"<pre><code>cd eks-green\nterraform init\nterraform apply\n</code></pre> <p>By default the only differences in the 2 clusters are the values defined in main.tf. We will change those values to upgrade Kubernetes version of new cluster, and to migrate our stateless workloads between clusters.</p>"},{"location":"patterns/blue-green-upgrade/#how-this-work","title":"How this work","text":""},{"location":"patterns/blue-green-upgrade/#watch-our-workload-we-focus-on-team-burnham-namespace","title":"Watch our Workload: we focus on team-burnham namespace.","text":"<p>Our clusters are configured with existing ArgoCD Github repository that is synchronized into each of the clusters:</p> <ul> <li>EKS Blueprints Add-ons repository</li> <li>Workloads repository</li> </ul> <p> </p> <p>We are going to look after one of the application deployed from the workload repository as example to demonstrate our migration automation: the <code>Burnham</code> workload in the team-burnham namespace. We have set up a simple go application than can respond in it's body the name of the cluster it is running on. With this it will be easy to see the current migration on our workload.</p> <pre><code>&lt;head&gt;\n  &lt;title&gt;Hello EKS Blueprint&lt;/title&gt;\n&lt;/head&gt;\n&lt;div class=\"info\"&gt;\n  &lt;h&gt;Hello EKS Blueprint Version 1.4&lt;/h&gt;\n  &lt;p&gt;&lt;span&gt;Server&amp;nbsp;address:&lt;/span&gt; &lt;span&gt;10.0.2.201:34120&lt;/span&gt;&lt;/p&gt;\n  &lt;p&gt;&lt;span&gt;Server&amp;nbsp;name:&lt;/span&gt; &lt;span&gt;burnham-9d686dc7b-dw45m&lt;/span&gt;&lt;/p&gt;\n  &lt;p class=\"smaller\"&gt;&lt;span&gt;Date:&lt;/span&gt; &lt;span&gt;2022.10.13 07:27:28&lt;/span&gt;&lt;/p&gt;\n  &lt;p class=\"smaller\"&gt;&lt;span&gt;URI:&lt;/span&gt; &lt;span&gt;/&lt;/span&gt;&lt;/p&gt;\n  &lt;p class=\"smaller\"&gt;&lt;span&gt;HOST:&lt;/span&gt; &lt;span&gt;burnham.eks-blueprint.mon-domain.com&lt;/span&gt;&lt;/p&gt;\n  &lt;p class=\"smaller\"&gt;&lt;span&gt;CLUSTER_NAME:&lt;/span&gt; &lt;span&gt;eks-blueprint-blue&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n</code></pre> <p>The application is deployed from our  workload repository manifest <p>Connect to the cluster: Execute one of the EKS cluster login commands from the <code>terraform output</code> command, depending on the IAM role you can assume to access to the cluster. If you want EKS Admin cluster, you can execute the command associated to the eks_blueprints_admin_team_configure_kubectl output. It should be something similar to:</p> <pre><code>aws eks --region eu-west-3 update-kubeconfig --name eks-blueprint-blue  --role-arn arn:aws:iam::0123456789:role/admin-team-20230505075455219300000002\n</code></pre> <p>Note it will allow the role associated to the parameter eks_admin_role_name to assume the role.</p> <p>You can also connect with the user who created the EKS cluster without specifying the <code>--role-arn</code> parameter</p> <p>Next, you can interact with the cluster and see the deployment</p> <pre><code>$ kubectl get deployment -n team-burnham -l app=burnham\nNAME      READY   UP-TO-DATE   AVAILABLE   AGE\nburnham   3/3     3            3           3d18h\n</code></pre> <p>See the pods</p> <pre><code>$ kubectl get pods -n team-burnham -l app=burnham\nNAME                       READY   STATUS    RESTARTS   AGE\nburnham-7db4c6fdbb-82hxn   1/1     Running   0          3d18h\nburnham-7db4c6fdbb-dl59v   1/1     Running   0          3d18h\nburnham-7db4c6fdbb-hpq6h   1/1     Running   0          3d18h\n</code></pre> <p>See the logs:</p> <pre><code>$ kubectl logs -n team-burnham -l app=burnham\n2022/10/10 12:35:40 {url: / }, cluster: eks-blueprint-blue }\n2022/10/10 12:35:49 {url: / }, cluster: eks-blueprint-blue }\n</code></pre> <p>You can make a request to the service, and filter the output to know on which cluster it runs:</p> <pre><code>$ URL=$(echo -n \"https://\" ; kubectl get ing -n team-burnham burnham-ingress -o json | jq \".spec.rules[0].host\" -r)\n$ curl -s $URL | grep CLUSTER_NAME | awk -F \"&lt;span&gt;|&lt;/span&gt;\" '{print $4}'\neks-blueprint-blue\n</code></pre>"},{"location":"patterns/blue-green-upgrade/#using-aws-route53-and-external-dns","title":"Using AWS Route53 and External DNS","text":"<p>We have configured both our clusters to configure the same Amazon Route 53 Hosted Zones. This is done by having the same configuration of ExternalDNS add-on in <code>main.tf</code>:</p> <p>This is the Terraform configuration to configure the ExternalDNS Add-on which is deployed by the Blueprint using ArgoCD. we specify the Route53 zone that external-dns needs to monitor.</p> <pre><code>  enable_external_dns = true\n  external_dns_route53_zone_arns      = [data.aws_route53_zone.sub.arn]\n</code></pre> <p>we also configure the addons_metadata to provide more configurations to external-dns:</p> <pre><code>addons_metadata = merge(\n  ...\n  external_dns_policy        = \"sync\"\n</code></pre> <ul> <li>We use ExternalDNS in <code>sync</code> mode so that the controller can create but also remove DNS records accordingly to service or ingress objects creation.</li> <li>We also configured the <code>txtOwnerId</code> with the name of the cluster, so that each controller will be able to create/update/delete records but only for records which are associated to the proper OwnerId.</li> <li>Each Route53 record will be also associated with a <code>txt</code> record. This record is used to specify the owner of the associated record and is in the form of:</li> </ul> <pre><code>\"heritage=external-dns,external-dns/owner=eks-blueprint-blue,external-dns/resource=ingress/team-burnham/burnham-ingress\"\n</code></pre> <p>So in this example the Owner of the record is the external-dns controller, from the eks-blueprint-blue EKS cluster, and correspond to the Kubernetes ingress resource names burnham-ingress in the team-burnham namespace.</p> <p>Using this feature, and relying on weighted records, we will be able to do blue/green or canary migration by changing the weight of ingress resources defined in each cluster.</p>"},{"location":"patterns/blue-green-upgrade/#configure-ingress-resources-with-weighted-records","title":"Configure Ingress resources with weighted records","text":"<p>Since we have configured ExternalDNS add-on, we can now defined specific annotation in our <code>ingress</code> object. You may already know that our workload are synchronized using ArgoCD from our workload repository sample.</p> <p>We are focusing on the burnham deployment, which is defined here where we configure the <code>burnham-ingress</code> ingress object with:</p> <pre><code>    external-dns.alpha.kubernetes.io/set-identifier: {{ .Values.spec.clusterName }}\n    external-dns.alpha.kubernetes.io/aws-weight: '{{ .Values.spec.ingress.route53_weight }}'\n</code></pre> <p>We rely on two external-dns annotation to configure how the record will be created. the <code>set-identifier</code> annotation will contain the name of the cluster we want to create, which must match the one define in the external-dns <code>txtOwnerId</code> configuration.</p> <p>The <code>aws-weight</code> will be used to configure the value of the weighted record, and it will be deployed from Helm values, that will be injected by Terraform in our example, so that our platform team will be able to control autonomously how and when they want to migrate workloads between the EKS clusters.</p> <p>Amazon Route 53 weighted records works like this:</p> <ul> <li>If we specify a value of 100 in eks-blue cluster and 0 in eks-green cluster, then Route 53 will route all requests to eks-blue cluster.</li> <li>If we specify a value of 0 in eks-blue cluster and 0 in eks-green cluster, then Route 53 will route all requests to eks-green cluster.</li> <li>we can also define any intermediate values like 100 in eks-blue cluster and 100 in eks-green cluster, so we will have 50% on eks-blue and 50% on eks-green.</li> </ul>"},{"location":"patterns/blue-green-upgrade/#automate-the-migration-from-terraform","title":"Automate the migration from Terraform","text":"<p>Now that we have setup our 2 clusters, deployed with ArgoCD and that the weighed records from <code>values.yaml</code> are injected from Terraform, let's see how our Platform team can trigger the workload migration.</p> <ol> <li>At first, 100% of burnham traffic is set to the eks-blue cluster, this is controlled from the <code>eks-blue/main.tf</code> &amp; <code>eks-green/main.tf</code> files with the parameter <code>route53_weight = \"100\"</code>. The same parameter is set to 0 in cluster eks-green.</li> </ol> <p> </p> <p>Which correspond to :</p> <p> </p> <p>All requests to our endpoint should response with <code>eks-blueprint-blue</code> we can test it with the following command:</p> <pre><code>URL=$(echo -n \"https://\" ; kubectl get ing -n team-burnham burnham-ingress -o json | jq \".spec.rules[0].host\" -r)\ncurl -s $URL | grep CLUSTER_NAME | awk -F \"&lt;span&gt;|&lt;/span&gt;\" '{print $4}'\n</code></pre> <p>you should see:</p> <pre><code>eks-blueprint-blue\n</code></pre> <ol> <li>Let's change traffic to 50% eks-blue and 50% eks-green by activating also value 100 in eks-green locals.tf (<code>route53_weight = \"100\"</code>) and let's <code>terraform apply</code> to let terraform update the configuration</li> </ol> <p> </p> <p>Which correspond to :</p> <p> </p> <p>All records have weight of 100, so we will have 50% requests on each clusters.</p> <p>We can check the ratio of requests resolution between both clusters</p> <pre><code>URL=$(echo -n \"https://\" ; kubectl get ing -n team-burnham burnham-ingress -o json | jq \".spec.rules[0].host\" -r)\nrepeat 10 curl -s $URL | grep CLUSTER_NAME | awk -F \"&lt;span&gt;|&lt;/span&gt;\" '{print $4}' &amp;&amp; sleep 60\n</code></pre> <p>Result should be similar to:</p> <pre><code>eks-blueprint-blue\neks-blueprint-blue\neks-blueprint-blue\neks-blueprint-blue\neks-blueprint-green\neks-blueprint-green\neks-blueprint-blue\neks-blueprint-green\neks-blueprint-blue\neks-blueprint-green\n</code></pre> <p>The default TTL is for 60 seconds, and you have 50% chance to have blue or green cluster, then you may need to replay the previous command several times to have an idea of the repartition, which theoretically is 50%</p> <ol> <li>Now that we see that our green cluster is taking requests correctly, we can update the eks-blue cluster configuration to have the weight to 0 and apply again. after a few moment, your route53 records should look like the below screenshot, and all requests should now reach eks-green cluster.</li> </ol> <p> </p> <p>Which correspond to :</p> <p> </p> <p>At this step, once all DNS TTL will be up to date, all the traffic will be coming on the eks-green cluster. You can either, delete the eks-blue cluster, or decide to make upgrades on the blue cluster and send back traffic on eks-blue afterward, or simply keep it as a possibility for rollback if needed.</p> <p>In this sample, we uses a simple terraform variable to control the weight for all applications, we can also choose to have several parameters, let's say one per application, so you can finer control your migration strategy application by application.</p>"},{"location":"patterns/blue-green-upgrade/#delete-the-stack","title":"Delete the Stack","text":""},{"location":"patterns/blue-green-upgrade/#delete-the-eks-clusters","title":"Delete the EKS Cluster(s)","text":"<p>This section, can be executed in either eks-blue or eks-green folders, or in both if you want to delete both clusters.</p> <p>In order to properly destroy the Cluster, we need first to remove the ArgoCD workloads, while keeping the ArgoCD addons. We will also need to remove our Karpenter provisioners, and any other objects you created outside of Terraform that needs to be cleaned before destroying the terraform stack.</p> <p>Why doing this? When we remove an ingress object, we want the associated Kubernetes add-ons like aws load balancer controller and External DNS to correctly free the associated AWS resources. If we directly ask terraform to destroy everything, it can remove first theses controllers without allowing them the time to remove associated aws resources that will still existing in AWS, preventing us to completely delete our cluster.</p>"},{"location":"patterns/blue-green-upgrade/#tldr","title":"TL;DR","text":"<pre><code>../tear-down.sh\n</code></pre>"},{"location":"patterns/blue-green-upgrade/#troubleshoot","title":"Troubleshoot","text":""},{"location":"patterns/blue-green-upgrade/#external-dns-ownership","title":"External DNS Ownership","text":"<p>The Amazon Route 53 records association are controlled by ExternalDNS controller. You can see the logs from the controller to understand what is happening by executing the following command in each cluster:</p> <pre><code>kubectl logs  -n external-dns -l app.kubernetes.io/name=external-dns -f\n</code></pre> <p>In eks-blue cluster, you can see logs like the following, which showcase that the eks-blueprint-blue controller won't make any change in records owned by eks-blueprint-green cluster, the reverse is also true.</p> <pre><code>time=\"2022-10-10T15:46:54Z\" level=debug msg=\"Skipping endpoint skiapp.eks-blueprint.sallaman.people.aws.dev 300 IN CNAME eks-blueprint-green k8s-riker-68438cd99f-893407990.eu-west-1.elb.amazonaws.com [{aws/evaluate-target-health true} {alias true} {aws/weight 100}] because owner id does not match, found: \\\"eks-blueprint-green\\\", required: \\\"eks-blueprint-blue\\\"\"\ntime=\"2022-10-10T15:46:54Z\" level=debug msg=\"Refreshing zones list cache\"\n</code></pre>"},{"location":"patterns/blue-green-upgrade/#check-route-53-record-status","title":"Check Route 53 Record status","text":"<p>We can also use the CLI to see our current Route 53 configuration:</p> <pre><code>export ROOT_DOMAIN=&lt;your-domain-name&gt; # the value you put in hosted_zone_name\nZONE_ID=$(aws route53 list-hosted-zones-by-name --output json --dns-name \"eks-blueprint.${ROOT_DOMAIN}.\" --query \"HostedZones[0].Id\" --out text)\necho $ZONE_ID\naws route53 list-resource-record-sets \\\n  --output json \\\n  --hosted-zone-id $ZONE_ID \\\n  --query \"ResourceRecordSets[?Name == 'burnham.eks-blueprint.$ROOT_DOMAIN.']|[?Type == 'A']\"\n\naws route53 list-resource-record-sets \\\n  --output json \\\n  --hosted-zone-id $ZONE_ID \\\n  --query \"ResourceRecordSets[?Name == 'burnham.eks-blueprint.$ROOT_DOMAIN.']|[?Type == 'TXT']\"\n</code></pre>"},{"location":"patterns/blue-green-upgrade/#check-current-resolution-and-ttl-value","title":"Check current resolution and TTL value","text":"<p>As DNS migration is dependent of DNS caching, normally relying on the TTL, you can use dig to see the current value of the TTL used locally</p> <pre><code>export ROOT_DOMAIN=&lt;your-domain-name&gt; # the value you put for hosted_zone_name\ndig +noauthority +noquestion +noadditional +nostats +ttlunits +ttlid A burnham.eks-blueprint.$ROOT_DOMAIN\n</code></pre>"},{"location":"patterns/blue-green-upgrade/#get-argocd-ui-password","title":"Get ArgoCD UI Password","text":"<p>You can connect to the ArgoCD UI using the service :</p> <pre><code>kubectl get svc -n argocd argo-cd-argocd-server -o json | jq '.status.loadBalancer.ingress[0].hostname' -r\n</code></pre> <p>Then login with admin and get the password from AWS Secret Manager:</p> <pre><code>aws secretsmanager get-secret-value \\\n  --secret-id argocd-admin-secret.eks-blueprint \\\n  --query SecretString \\\n  --output text --region $AWS_REGION\n</code></pre>"},{"location":"patterns/bottlerocket/","title":"Bottlerocket","text":""},{"location":"patterns/bottlerocket/#bottlerocket-with-bottlerocket-update-operator","title":"Bottlerocket with Bottlerocket Update Operator","text":"<p>This pattern demostrates how to deploy Amazon EKS Clusters using Bottlerocket OS for Managed Node Groups (MNG) and Karpenter, using Bottlerocket Update Operator (BRUPOP) to manage CVE patches automatically at the Node OS level. The BROPUP doesn't work with minor or major upgrades, just patch level.</p>"},{"location":"patterns/bottlerocket/#deploy","title":"Deploy","text":"<p>See here for the prerequisites and steps to deploy this pattern.</p>"},{"location":"patterns/bottlerocket/#validate","title":"Validate","text":"<ol> <li>List all Nodes in the cluster. You should see three Nodes that belongs to the defined MNG, and should be in the <code>v1.30.0-eks-fff26e3</code> version since we are using a specific AMI ID to test the BRUPOP.</li> </ol> <pre><code>$ kubectl get nodes\nNAME                                        STATUS   ROLES    AGE     VERSION\nip-10-0-2-29.us-west-2.compute.internal     Ready    &lt;none&gt;   7m24s   v1.30.0-eks-fff26e3\nip-10-0-26-48.us-west-2.compute.internal    Ready    &lt;none&gt;   7m23s   v1.30.0-eks-fff26e3\nip-10-0-43-187.us-west-2.compute.internal   Ready    &lt;none&gt;   7m19s   v1.30.0-eks-fff26e3\n</code></pre> <ol> <li>Check for the Label <code>\"bottlerocket.aws/updater-interface-version\"=\"2.0.0\"</code> that is set to all the Nodes in the MNG. This Label is responsible to mark the Nodes that will have updates managed by BRUPOP.</li> </ol> <pre><code>$ kubectl get nodes -L bottlerocket.aws/updater-interface-version\nNAME                                        STATUS   ROLES    AGE   VERSION               UPDATER-INTERFACE-VERSION\nip-10-0-2-29.us-west-2.compute.internal     Ready    &lt;none&gt;   79m   v1.30.0-eks-fff26e3   2.0.0\nip-10-0-26-48.us-west-2.compute.internal    Ready    &lt;none&gt;   79m   v1.30.0-eks-fff26e3   2.0.0\nip-10-0-43-187.us-west-2.compute.internal   Ready    &lt;none&gt;   79m   v1.30.0-eks-fff26e3   2.0.0\n</code></pre> <ol> <li>Validate if all the Pods are in Running status, and Ready.</li> </ol> <pre><code>$ kubectl get pods -A\nNAMESPACE                 NAME                                            READY   STATUS    RESTARTS        AGE\nbrupop-bottlerocket-aws   brupop-agent-2msn5                              1/1     Running   0               3m20s\nbrupop-bottlerocket-aws   brupop-agent-7kvx5                              1/1     Running   0               3m20s\nbrupop-bottlerocket-aws   brupop-agent-8d8n8                              1/1     Running   0               3m20s\nbrupop-bottlerocket-aws   brupop-apiserver-7b45c5546f-dzwqz               1/1     Running   0               3m20s\nbrupop-bottlerocket-aws   brupop-apiserver-7b45c5546f-lvnt4               1/1     Running   0               3m20s\nbrupop-bottlerocket-aws   brupop-apiserver-7b45c5546f-xmvx2               1/1     Running   0               3m20s\nbrupop-bottlerocket-aws   brupop-controller-deployment-7fcfc69978-rwkml   1/1     Running   0               3m20s\ncert-manager              cert-manager-5b44f85959-zc5zc                   1/1     Running   0               4m2s\ncert-manager              cert-manager-cainjector-7f97f54fd-kjnq5         1/1     Running   0               4m3s\ncert-manager              cert-manager-webhook-c59f66876-jkwhj            1/1     Running   0               4m3s\nkarpenter                 karpenter-7b7958bbf5-647n2                      1/1     Running   0               11m\nkarpenter                 karpenter-7b7958bbf5-s8475                      1/1     Running   0               11m\nkube-system               aws-node-5n496                                  2/2     Running   0               10m\nkube-system               aws-node-krz6q                                  2/2     Running   0               10m\nkube-system               aws-node-tx76l                                  2/2     Running   0               10m\nkube-system               coredns-544fd9dfb5-6l6nt                        1/1     Running   0               9m27s\nkube-system               coredns-544fd9dfb5-hcq84                        1/1     Running   0               9m27s\nkube-system               kube-proxy-9sh2s                                1/1     Running   0               9m19s\nkube-system               kube-proxy-gl5g9                                1/1     Running   0               9m24s\nkube-system               kube-proxy-jwcqp                                1/1     Running   0               9m15s\n</code></pre> <ol> <li>Test the Bottlerocket Update Operator. By default in this pattern, it's set to check for updates every hour.</li> </ol> <pre><code>  set = [{\n    name  = \"scheduler_cron_expression\"\n    value = \"0 * * * * * *\" # Default Unix Cron syntax, set to check every hour. Example \"0 0 23 * * Sat *\" Perform update checks every Saturday at 23H / 11PM\n    }]\n</code></pre> <p>Describe any Node with the <code>v1.30.0-eks-fff26e3</code> version.</p> <pre><code>$ kubectl describe node ip-10-0-43-187.us-west-2.compute.internal | grep Image\n  OS Image:                   Bottlerocket OS 1.20.0 (aws-k8s-1.30)\n</code></pre> <ol> <li>Wait a couple of minutes and check that one or more Nodes were updated to a newer version, in this example, <code>v1.30.1-eks-e564799</code>.</li> </ol> <pre><code>$ kubectl get nodes\nNAME                                        STATUS   ROLES    AGE   VERSION\nip-10-0-2-29.us-west-2.compute.internal     Ready    &lt;none&gt;   83m   v1.30.1-eks-e564799\nip-10-0-26-48.us-west-2.compute.internal    Ready    &lt;none&gt;   83m   v1.30.0-eks-fff26e3\nip-10-0-43-187.us-west-2.compute.internal   Ready    &lt;none&gt;   83m   v1.30.0-eks-fff26e3\n</code></pre> <ol> <li>Describe the Node with the <code>v1.30.1-eks-e564799</code> version.</li> </ol> <pre><code>$ kubectl describe node ip-10-0-2-29.us-west-2.compute.internal | grep Image\n  OS Image:                   Bottlerocket OS 1.20.4 (aws-k8s-1.30)\n</code></pre> <ol> <li>In the Karpenter's EC2NodeClass configuration, the default OS is also set to Bottlerocket, but in it's latest version, and the label to perform automated updates is not set, since Karpenter is configured to expire the Nodes every 24 hours.</li> </ol> <pre><code>kubectl describe ec2nodeclasses.karpenter.k8s.aws default | grep Status -A50 | egrep 'Amis|Id|Name'\n  Amis:\n    Id:    ami-079c06d95540e8c92\n    Name:  bottlerocket-aws-k8s-1.30-nvidia-x86_64-v1.20.4-b6163b2a\n    Id:          ami-079c06d95540e8c92\n    Name:        bottlerocket-aws-k8s-1.30-nvidia-x86_64-v1.20.4-b6163b2a\n    Id:          ami-05206fcf92965fc27\n    Name:        bottlerocket-aws-k8s-1.30-nvidia-aarch64-v1.20.4-b6163b2a\n    Id:          ami-05206fcf92965fc27\n    Name:        bottlerocket-aws-k8s-1.30-nvidia-aarch64-v1.20.4-b6163b2a\n    Id:          ami-03b7010797ca2bf91\n    Name:        bottlerocket-aws-k8s-1.30-x86_64-v1.20.4-b6163b2a\n    Id:          ami-0c37339cac90815d6\n    Name:        bottlerocket-aws-k8s-1.30-aarch64-v1.20.4-b6163b2a\n</code></pre> <ol> <li>To validate that, use the <code>kubectl</code> command to create an example deployment, and scale it to any desired amount of replicas. Karpenter should provision a new Node in with the latest available version for Bottlerocket.</li> </ol> <pre><code>$ kubectl scale deployment inflate --replicas 10\ndeployment.apps/inflate scaled\n\n$ kubectl get pods -o wide\nNAME                       READY   STATUS    RESTARTS   AGE   IP            NODE                                       NOMINATED NODE   READINESS GATES\ninflate-7849c696cd-2668t   1/1     Running   0          49s   10.0.34.254   ip-10-0-45-41.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\ninflate-7849c696cd-5wffm   1/1     Running   0          49s   10.0.46.13    ip-10-0-45-41.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\ninflate-7849c696cd-8x5ws   1/1     Running   0          49s   10.0.35.190   ip-10-0-45-41.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\ninflate-7849c696cd-9nhvr   1/1     Running   0          49s   10.0.42.99    ip-10-0-45-41.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\ninflate-7849c696cd-cbr5q   1/1     Running   0          49s   10.0.35.195   ip-10-0-45-41.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\ninflate-7849c696cd-jcr7r   1/1     Running   0          49s   10.0.33.41    ip-10-0-45-41.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\ninflate-7849c696cd-nhjt4   1/1     Running   0          49s   10.0.35.213   ip-10-0-45-41.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\ninflate-7849c696cd-p9j7x   1/1     Running   0          49s   10.0.43.102   ip-10-0-45-41.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\ninflate-7849c696cd-qr7th   1/1     Running   0          49s   10.0.37.221   ip-10-0-45-41.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\ninflate-7849c696cd-rzjzr   1/1     Running   0          49s   10.0.33.210   ip-10-0-45-41.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\n\n$ kubect get nodes\nNAME                                        STATUS                     ROLES    AGE   VERSION\nip-10-0-2-29.us-west-2.compute.internal     Ready                      &lt;none&gt;   90m   v1.30.1-eks-e564799\nip-10-0-26-48.us-west-2.compute.internal    Ready                      &lt;none&gt;   90m   v1.30.0-eks-fff26e3\nip-10-0-43-187.us-west-2.compute.internal   Ready                      &lt;none&gt;   90m   v1.30.0-eks-fff26e3\nip-10-0-45-41.us-west-2.compute.internal    Ready                      &lt;none&gt;   60s   v1.30.1-eks-e564799\n\n$ kubectl describe node ip-10-0-45-41.us-west-2.compute.internal | grep Image\n  OS Image:                   Bottlerocket OS 1.20.4 (aws-k8s-1.30)\n</code></pre>"},{"location":"patterns/bottlerocket/#destroy","title":"Destroy","text":"<p>Scale down the example application to de-provision Karpenter Nodes.</p> <pre><code>$ kubectl delete -f example.yaml\ndeployment.apps \"inflate\" deleted\n</code></pre> <pre><code>terraform destroy -target=\"module.eks_blueprints_addons\" -auto-approve\nterraform destroy -target=\"module.eks\" -auto-approve\nterraform destroy -auto-approve\n</code></pre> <p>See here for more details on cleaning up the resources created.</p>"},{"location":"patterns/ecr-pull-through-cache/","title":"ECR Pull Through Cache","text":""},{"location":"patterns/ecr-pull-through-cache/#ecr-pull-through-cache","title":"ECR Pull Through Cache","text":"<p>This pattern demonstrates how to set up ECR cache pull-through for public images. The Terraform code creates four cache pull-through rules for public image repositories: Docker, Kubernetes, Quay, and ECR. It also configures basic scanning on push for all repositories and includes a creation template. Additionally, it configures the EC2 node role with permissions to pull through images. The setup then installs ALB Controller, Metrics Server, Gatekeeper, ArgoCD, and Prometheus Operator, with their respective Helm charts configured in the values files to pull images through the pull-through cache.</p>"},{"location":"patterns/ecr-pull-through-cache/#deploy","title":"Deploy","text":"<p>Follow the instructions here for the prerequisites and steps to deploy this pattern.</p> <pre><code>terraform init\nterraform apply -var='docker_secret={\"username\":\"your-docker-username\", \"accessToken\":\"your-docker-password\"}'\n</code></pre>"},{"location":"patterns/ecr-pull-through-cache/#validate","title":"Validate","text":"<p>Validate the pull trough cache rules connectivity:</p> <pre><code>for i in docker-hub ecr k8s quay ; do aws ecr validate-pull-through-cache-rule --ecr-repository-prefix $i --region us-east-1; done\n</code></pre> <p>Expected output:</p> <pre><code>{\n    \"ecrRepositoryPrefix\": \"docker-hub\",\n    \"registryId\": \"111122223333\",\n    \"upstreamRegistryUrl\": \"registry-1.docker.io\",\n    \"credentialArn\": \"arn:aws:secretsmanager:us-east-1:111122223333:secret:ecr-pullthroughcache/docker-111XXX\",\n    \"isValid\": true,\n    \"failure\": \"\"\n}\n{\n    \"ecrRepositoryPrefix\": \"ecr\",\n    \"registryId\": \"111122223333\",\n    \"upstreamRegistryUrl\": \"public.ecr.aws\",\n    \"isValid\": true,\n    \"failure\": \"\"\n}\n{\n    \"ecrRepositoryPrefix\": \"k8s\",\n    \"registryId\": \"111122223333\",\n    \"upstreamRegistryUrl\": \"registry.k8s.io\",\n    \"isValid\": true,\n    \"failure\": \"\"\n}\n{\n    \"ecrRepositoryPrefix\": \"quay\",\n    \"registryId\": \"111122223333\",\n    \"upstreamRegistryUrl\": \"quay.io\",\n    \"isValid\": true,\n    \"failure\": \"\"\n}\n</code></pre> <p>Validate pods are pulling the images and in Running state:</p> <pre><code>kubectl get pods -A\n</code></pre> <p>Expected output:</p> <pre><code>NAMESPACE               NAME                                                        READY   STATUS      RESTARTS   AGE\nargocd                  argo-cd-argocd-application-controller-0                     1/1     Running     0          2m26s\nargocd                  argo-cd-argocd-applicationset-controller-78ccd75cfb-7zfs5   1/1     Running     0          2m28s\nargocd                  argo-cd-argocd-notifications-controller-8cc5c5578-r7g8s     1/1     Running     0          2m27s\nargocd                  argo-cd-argocd-redis-secret-init-x55l4                      0/1     Completed   0          2m43s\nargocd                  argo-cd-argocd-repo-server-5d64dff78d-wg7xm                 1/1     Running     0          2m27s\nargocd                  argo-cd-argocd-server-7b7974dfbf-dl7z6                      1/1     Running     0          2m27s\nargocd                  argo-cd-redis-ha-haproxy-78456c586d-4jxxh                   1/1     Running     0          2m28s\nargocd                  argo-cd-redis-ha-haproxy-78456c586d-cp7sw                   1/1     Running     0          2m28s\nargocd                  argo-cd-redis-ha-haproxy-78456c586d-hpfrl                   1/1     Running     0          2m28s\nargocd                  argo-cd-redis-ha-server-0                                   3/3     Running     0          2m26s\nargocd                  argo-cd-redis-ha-server-1                                   3/3     Running     0          71s\nargocd                  argo-cd-redis-ha-server-2                                   1/3     Running     0          11s\ngatekeeper-system       gatekeeper-audit-85cc8756cf-zx5xn                           1/1     Running     0          53s\ngatekeeper-system       gatekeeper-controller-manager-568c7544d-6s5hw               1/1     Running     0          53s\ngatekeeper-system       gatekeeper-controller-manager-568c7544d-gtq88               1/1     Running     0          53s\ngatekeeper-system       gatekeeper-controller-manager-568c7544d-nz5g4               1/1     Running     0          53s\nkube-prometheus-stack   alertmanager-kube-prometheus-stack-alertmanager-0           2/2     Running     0          9m33s\nkube-prometheus-stack   kube-prometheus-stack-grafana-76f988cb8c-dbsc5              3/3     Running     0          9m38s\nkube-prometheus-stack   kube-prometheus-stack-kube-state-metrics-57c4f8df9c-td9tp   1/1     Running     0          9m38s\nkube-prometheus-stack   kube-prometheus-stack-operator-77695dc8-z4qm2               1/1     Running     0          9m38s\nkube-prometheus-stack   kube-prometheus-stack-prometheus-node-exporter-dp9nl        1/1     Running     0          9m38s\nkube-prometheus-stack   kube-prometheus-stack-prometheus-node-exporter-gsp24        1/1     Running     0          9m38s\nkube-prometheus-stack   kube-prometheus-stack-prometheus-node-exporter-vgl4r        1/1     Running     0          9m38s\nkube-prometheus-stack   prometheus-kube-prometheus-stack-prometheus-0               2/2     Running     0          9m33s\nkube-system             aws-load-balancer-controller-7cb475c856-7rgwm               1/1     Running     0          10m\nkube-system             aws-load-balancer-controller-7cb475c856-qh2v6               1/1     Running     0          10m\nkube-system             aws-node-4v2c5                                              2/2     Running     0          12m\nkube-system             aws-node-lgcsc                                              2/2     Running     0          12m\nkube-system             aws-node-lprv6                                              2/2     Running     0          12m\nkube-system             coredns-86d5d9b668-gw2c7                                    1/1     Running     0          11m\nkube-system             coredns-86d5d9b668-qtfxm                                    1/1     Running     0          11m\nkube-system             ebs-csi-controller-57547c649b-bm4lv                         6/6     Running     0          11m\nkube-system             ebs-csi-controller-57547c649b-q68b6                         6/6     Running     0          11m\nkube-system             ebs-csi-node-7shn9                                          3/3     Running     0          11m\nkube-system             ebs-csi-node-f25zz                                          3/3     Running     0          11m\nkube-system             ebs-csi-node-rdq6v                                          3/3     Running     0          11m\nkube-system             kube-proxy-7mzgr                                            1/1     Running     0          12m\nkube-system             kube-proxy-ksz6w                                            1/1     Running     0          12m\nkube-system             kube-proxy-w6x2s                                            1/1     Running     0          12m\nkube-system             metrics-server-5d6489d58d-pbrxv                             1/1     Running     0          10m\n</code></pre>"},{"location":"patterns/ecr-pull-through-cache/#destroy","title":"Destroy","text":"<p>ECR repositories are automatically created via pull through cache and can be deleted using the following command.</p> <p>NOTE: This commands deletes all the ECR repositories in the specified region.</p> <pre><code>for REPO in $(aws ecr describe-repositories --query 'repositories[].repositoryName' --output text); do aws ecr delete-repository --repository-name $REPO --force ; done\n</code></pre> <pre><code>terraform destroy -target=\"module.eks_blueprints_addons\" -auto-approve\nterraform destroy -target=\"module.eks\" -auto-approve\nterraform destroy -auto-approve\n</code></pre> <p>See here for more details on cleaning up the resources created.</p>"},{"location":"patterns/external-secrets/","title":"External Secrets","text":""},{"location":"patterns/external-secrets/#amazon-eks-cluster-w-external-secrets-operator","title":"Amazon EKS Cluster w/ External Secrets Operator","text":"<p>This pattern deploys an EKS Cluster with the External Secrets Operator. The cluster is populated with a ClusterSecretStore and SecretStore example using SecretManager and Parameter Store respectively. A secret for each store is also created. Both stores use IRSA to retrieve the secret values from AWS.</p>"},{"location":"patterns/external-secrets/#deploy","title":"Deploy","text":"<p>See here for the prerequisites and steps to deploy this pattern.</p>"},{"location":"patterns/external-secrets/#validate","title":"Validate","text":"<ol> <li> <p>List the secret resources in the <code>external-secrets</code> namespace</p> <pre><code>kubectl get externalsecrets -n external-secrets\nkubectl get secrets -n external-secrets\n</code></pre> </li> </ol>"},{"location":"patterns/external-secrets/#destroy","title":"Destroy","text":"<pre><code>terraform destroy -target=\"module.eks_blueprints_addons\" -auto-approve\nterraform destroy -target=\"module.eks\" -auto-approve\nterraform destroy -auto-approve\n</code></pre> <p>See here for more details on cleaning up the resources created.</p>"},{"location":"patterns/fargate-serverless/","title":"Fargate Serverless","text":""},{"location":"patterns/fargate-serverless/#serverless-amazon-eks-cluster","title":"Serverless Amazon EKS Cluster","text":"<p>This pattern demonstrates an Amazon EKS Cluster that utilizes Fargate profiles for a serverless data plane.</p>"},{"location":"patterns/fargate-serverless/#deploy","title":"Deploy","text":"<p>See here for the prerequisites and steps to deploy this pattern.</p>"},{"location":"patterns/fargate-serverless/#validate","title":"Validate","text":"<ol> <li> <p>List the nodes in in the cluster; you should see Fargate instances:</p> <pre><code>kubectl get nodes\n</code></pre> <pre><code>NAME                                                STATUS   ROLES    AGE   VERSION\nfargate-ip-10-0-17-17.us-west-2.compute.internal    Ready    &lt;none&gt;   25m   v1.26.3-eks-f4dc2c0\nfargate-ip-10-0-20-244.us-west-2.compute.internal   Ready    &lt;none&gt;   71s   v1.26.3-eks-f4dc2c0\nfargate-ip-10-0-41-143.us-west-2.compute.internal   Ready    &lt;none&gt;   25m   v1.26.3-eks-f4dc2c0\nfargate-ip-10-0-44-95.us-west-2.compute.internal    Ready    &lt;none&gt;   25m   v1.26.3-eks-f4dc2c0\nfargate-ip-10-0-45-153.us-west-2.compute.internal   Ready    &lt;none&gt;   77s   v1.26.3-eks-f4dc2c0\nfargate-ip-10-0-47-31.us-west-2.compute.internal    Ready    &lt;none&gt;   75s   v1.26.3-eks-f4dc2c0\nfargate-ip-10-0-6-175.us-west-2.compute.internal    Ready    &lt;none&gt;   25m   v1.26.3-eks-f4dc2c0\n</code></pre> </li> <li> <p>List the pods. All the pods should reach a status of <code>Running</code> after approximately 60 seconds:</p> <pre><code>kubectl get pods -A\n</code></pre> <pre><code>NAMESPACE       NAME                                            READY   STATUS    RESTARTS   AGE\napp-2048        app-2048-65bd744dfb-7g9rx                       1/1     Running   0          2m34s\napp-2048        app-2048-65bd744dfb-n1cbm                       1/1     Running   0          2m34s\napp-2048        app-2048-65bd744dfb-z4b6z                       1/1     Running   0          2m34s\nkube-system     aws-load-balancer-controller-6cbdb58654-f1skt   1/1     Running   0          26m\nkube-system     aws-load-balancer-controller-6cbdb58654-sc7dk   1/1     Running   0          26m\nkube-system     coredns-7b7bddbc85-j1bv6                        1/1     Running   0          26m\nkube-system     coredns-7b7bddbc85-rg2zq                        1/1     Running   0          26m\n</code></pre> </li> <li> <p>Validate the <code>aws-logging</code> configMap for Fargate Fluentbit was created:</p> <pre><code>kubectl -n aws-observability get configmap aws-logging\n</code></pre> <pre><code>NAME               DATA   AGE\naws-logging        4      20m\n</code></pre> <p>You can also validate if the CloudWatch LogGroup was created accordingly, and LogStreams were populated:</p> <pre><code>aws logs describe-log-groups \\\n    --log-group-name-prefix \"/fargate-serverless/fargate-fluentbit\"\n</code></pre> <pre><code>{\n    \"logGroups\": [\n        {\n            \"logGroupName\": \"/fargate-serverless/fargate-fluentbit-logs20230509014113352200000006\",\n            \"creationTime\": 1683580491652,\n            \"retentionInDays\": 90,\n            \"metricFilterCount\": 0,\n            \"arn\": \"arn:aws:logs:us-west-2:111222333444:log-group:/fargate-serverless/fargate-fluentbit-logs20230509014113352200000006:*\",\n            \"storedBytes\": 0\n        }\n    ]\n}\n</code></pre> <pre><code>aws logs describe-log-streams \\\n    --log-group-name \"/fargate-serverless/fargate-fluentbit-logs20230509014113352200000006\" \\\n    --log-stream-name-prefix fargate-logs --query 'logStreams[].logStreamName'\n</code></pre> <pre><code>[\n    \"fargate-logs-flblogs.var.log.fluent-bit.log\",\n    \"fargate-logs-kube.var.log.containers.aws-load-balancer-controller-7f989fc6c-gr1sq_kube-system_aws-load-balancer-controller-feaa22b4cdaa71ecfc8355feb81d4b61ea85598a7bb57aef07667c767c6b98e4.log\",\n    \"fargate-logs-kube.var.log.containers.aws-load-balancer-controller-7f989fc6c-wzr46_kube-system_aws-load-balancer-controller-69075ea9ab3c7474eac2a1696d3a84a848a151420cd783d79aeef960b181567f.log\",\n    \"fargate-logs-kube.var.log.containers.coredns-7b7bddbc85-8cx1q_kube-system_coredns-9e4f3ab435269a566bcbaa606c02c146ad58508e67cef09fa87d5c09e4ac0088.log\",\n    \"fargate-logs-kube.var.log.containers.coredns-7b7bddbc85-g1jwp_kube-system_coredns-11016818361cd68c32bf8f0b1328f3d92a6d7b8cf5879bfe8b301f393cb011cc.log\"\n]\n</code></pre> </li> </ol>"},{"location":"patterns/fargate-serverless/#example","title":"Example","text":"<ol> <li> <p>Create an ingress resource using the AWS load balancer controller deployed, pointing to our application service:</p> <pre><code>kubectl get svc -n app-2048\n</code></pre> <pre><code>NAME       TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\napp-2048   NodePort   172.20.33.217   &lt;none&gt;        80:32568/TCP   2m48s\n</code></pre> <pre><code>kubectl -n app-2048 create ingress app-2048 --class alb --rule=\"/*=app-2048:80\" \\\n--annotation alb.ingress.kubernetes.io/scheme=internet-facing \\\n--annotation alb.ingress.kubernetes.io/target-type=ip\n</code></pre> <pre><code>kubectl -n app-2048 get ingress\n</code></pre> <pre><code>NAME       CLASS   HOSTS   ADDRESS                                                                 PORTS   AGE\napp-2048   alb     *       k8s-app2048-app2048-6d9c5e92d6-1234567890.us-west-2.elb.amazonaws.com   80      4m9s\n</code></pre> </li> <li> <p>Open the browser to access the application via the URL address shown in the last output in the ADDRESS column.</p> <p>In our example: <code>k8s-app2048-app2048-6d9c5e92d6-1234567890.us-west-2.elb.amazonaws.com</code></p> <p>Info</p> <p>You might need to wait a few minutes, and then refresh your browser. If your Ingress isn't created after several minutes, then run this command to view the AWS Load Balancer Controller logs:</p> <pre><code>kubectl logs -n kube-system deployment.apps/aws-load-balancer-controller\n</code></pre> </li> </ol>"},{"location":"patterns/fargate-serverless/#destroy","title":"Destroy","text":"<pre><code>terraform destroy -target=\"module.eks_blueprints_addons\" -auto-approve\nterraform destroy -target=\"module.eks\" -auto-approve\nterraform destroy -auto-approve\n</code></pre> <p>See here for more details on cleaning up the resources created.</p>"},{"location":"patterns/fully-private-cluster/","title":"Fully Private Cluster","text":""},{"location":"patterns/fully-private-cluster/#fully-private-amazon-eks-cluster","title":"Fully Private Amazon EKS Cluster","text":"<p>This pattern demonstrates an Amazon EKS cluster that does not have internet access. The private cluster must pull images from a container registry that is within in your VPC, and also must have endpoint private access enabled. This is required for nodes to register with the cluster endpoint.</p> <p>Please see this document for more details on configuring fully private EKS Clusters.</p> <p>For fully Private EKS clusters requires the following VPC endpoints to be created to communicate with AWS services. This example solution will provide these endpoints if you choose to create VPC. If you are using an existing VPC then you may need to ensure these endpoints are created.</p> <pre><code>com.amazonaws.region.aps-workspaces       - If using AWS Managed Prometheus Workspace\ncom.amazonaws.region.ssm                  - Secrets Management\ncom.amazonaws.region.ec2\ncom.amazonaws.region.ecr.api\ncom.amazonaws.region.ecr.dkr\ncom.amazonaws.region.logs                 \u2013 For CloudWatch Logs\ncom.amazonaws.region.sts                  \u2013 If using AWS Fargate or IAM roles for service accounts\ncom.amazonaws.region.elasticloadbalancing \u2013 If using Application Load Balancers\ncom.amazonaws.region.autoscaling          \u2013 If using Cluster Autoscaler\ncom.amazonaws.region.s3\n</code></pre>"},{"location":"patterns/fully-private-cluster/#deploy","title":"Deploy","text":"<p>See here for the prerequisites and steps to deploy this pattern.</p>"},{"location":"patterns/fully-private-cluster/#validate","title":"Validate","text":"<ol> <li> <p>Test by listing Nodes in in the cluster:</p> <pre><code>kubectl get nodes\n</code></pre> <pre><code>NAME                                        STATUS   ROLES    AGE     VERSION\nip-10-0-19-90.us-west-2.compute.internal    Ready    &lt;none&gt;   8m34s   v1.26.2-eks-a59e1f0\nip-10-0-44-110.us-west-2.compute.internal   Ready    &lt;none&gt;   8m36s   v1.26.2-eks-a59e1f0\nip-10-0-9-147.us-west-2.compute.internal    Ready    &lt;none&gt;   8m35s   v1.26.2-eks-a59e1f0\n</code></pre> </li> <li> <p>Test by listing all the Pods running currently. All the Pods should reach a status of <code>Running</code> after approximately 60 seconds:</p> <pre><code>kubectl get pods -A\n</code></pre> <pre><code>NAMESPACE     NAME                       READY   STATUS    RESTARTS   AGE\nkube-system   aws-node-j2n9x             1/1     Running   0          7m42s\nkube-system   aws-node-m1jlf             1/1     Running   0          7m45s\nkube-system   aws-node-q458h             1/1     Running   0          7m49s\nkube-system   coredns-6c45d94f67-495rr   1/1     Running   0          14m\nkube-system   coredns-6c45d94f67-5c8tc   1/1     Running   0          14m\nkube-system   kube-proxy-47wfh           1/1     Running   0          8m32s\nkube-system   kube-proxy-f6chz           1/1     Running   0          8m30s\nkube-system   kube-proxy-x2fkc           1/1     Running   0          8m31s\n</code></pre> </li> </ol>"},{"location":"patterns/fully-private-cluster/#destroy","title":"Destroy","text":"<pre><code>terraform destroy -target=\"module.eks_blueprints_addons\" -auto-approve\nterraform destroy -target=\"module.eks\" -auto-approve\nterraform destroy -auto-approve\n</code></pre> <p>See here for more details on cleaning up the resources created.</p>"},{"location":"patterns/istio/","title":"Istio","text":""},{"location":"patterns/istio/#amazon-eks-cluster-w-istio","title":"Amazon EKS Cluster w/ Istio","text":"<p>This example shows how to provision an EKS cluster with Istio.</p> <ul> <li>Deploy EKS Cluster with one managed node group in an VPC</li> <li>Add node_security_group rules for port access required for Istio communication</li> <li>Install Istio using Helm resources in Terraform</li> <li>Install Istio Ingress Gateway using Helm resources in Terraform</li> <li>This step deploys a Service of type <code>LoadBalancer</code> that creates an AWS Network Load Balancer.</li> <li>Deploy/Validate Istio communication using sample application</li> </ul> <p>Refer to the documentation on Istio concepts.</p>"},{"location":"patterns/istio/#deploy","title":"Deploy","text":"<p>See here for the prerequisites and run the following command to deploy this pattern.</p> <pre><code>terraform init\nterraform apply --auto-approve\n</code></pre> <p>Once the resources have been provisioned, you will need to replace the <code>istio-ingress</code> pods due to a <code>istiod</code> dependency issue. Use the following command to perform a rolling restart of the <code>istio-ingress</code> pods:</p> <pre><code>kubectl rollout restart deployment istio-ingress -n istio-ingress\n</code></pre>"},{"location":"patterns/istio/#observability-add-ons","title":"Observability Add-ons","text":"<p>Use the following code snippet to add the Istio Observability Add-ons on the EKS cluster with deployed Istio.</p> <pre><code>for ADDON in kiali jaeger prometheus grafana\ndo\n    ADDON_URL=\"https://raw.githubusercontent.com/istio/istio/release-1.20/samples/addons/$ADDON.yaml\"\n    kubectl apply --server-side -f $ADDON_URL\ndone\n</code></pre>"},{"location":"patterns/istio/#validate","title":"Validate","text":"<ol> <li> <p>List out all pods and services in the <code>istio-system</code> namespace:</p> <pre><code>kubectl get pods,svc -n istio-system\nkubectl get pods,svc -n istio-ingress\n</code></pre> <pre><code>NAME                             READY   STATUS    RESTARTS   AGE\npod/grafana-7d4f5589fb-4xj9m     1/1     Running   0          4m14s\npod/istiod-ff577f8b8-c8ssk       1/1     Running   0          4m40s\npod/jaeger-58c79c85cd-n7bkx      1/1     Running   0          4m14s\npod/kiali-749d76d7bb-8kjg7       1/1     Running   0          4m14s\npod/prometheus-5d5d6d6fc-s1txl   2/2     Running   0          4m15s\n\nNAME                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                 AGE\nservice/grafana            ClusterIP   172.20.141.12    &lt;none&gt;        3000/TCP                                4m14s\nservice/istiod             ClusterIP   172.20.172.70    &lt;none&gt;        15010/TCP,15012/TCP,443/TCP,15014/TCP   4m40s\nservice/jaeger-collector   ClusterIP   172.20.223.28    &lt;none&gt;        14268/TCP,14250/TCP,9411/TCP            4m15s\nservice/kiali              ClusterIP   172.20.182.231   &lt;none&gt;        20001/TCP,9090/TCP                      4m15s\nservice/prometheus         ClusterIP   172.20.89.64     &lt;none&gt;        9090/TCP                                4m14s\nservice/tracing            ClusterIP   172.20.253.201   &lt;none&gt;        80/TCP,16685/TCP                        4m14s\nservice/zipkin             ClusterIP   172.20.221.157   &lt;none&gt;        9411/TCP                                4m15s\n\nNAME                                 READY   STATUS    RESTARTS   AGE\npod/istio-ingress-6f7c5dffd8-g1szr   1/1     Running   0          4m28s\n\nNAME                    TYPE           CLUSTER-IP      EXTERNAL-IP                                                                     PORT(S)                                      AGE\nservice/istio-ingress   LoadBalancer   172.20.104.27   k8s-istioing-istioing-844c89b6c2-875b8c9a4b4e9365.elb.us-west-2.amazonaws.com   15021:32760/TCP,80:31496/TCP,443:32534/TCP   4m28s\n</code></pre> </li> <li> <p>Verify all the Helm releases installed in the <code>istio-system</code> and <code>istio-ingress</code> namespaces:</p> <pre><code>helm list -n istio-system\n</code></pre> <pre><code>NAME           NAMESPACE    REVISION UPDATED                              STATUS   CHART          APP VERSION\nistio-base    istio-system 1        2023-07-19 11:05:41.599921 -0700 PDT deployed base-1.18.1    1.18.1\nistiod        istio-system 1        2023-07-19 11:05:48.087616 -0700 PDT deployed istiod-1.18.1  1.18.1\n</code></pre> <pre><code>helm list -n istio-ingress\n</code></pre> <pre><code>NAME           NAMESPACE    REVISION UPDATED                              STATUS   CHART          APP VERSION\nistio-ingress istio-ingress 1        2023-07-19 11:06:03.41609 -0700 PDT  deployed gateway-1.18.1 1.18.1\n</code></pre> </li> </ol>"},{"location":"patterns/istio/#observability-add-ons_1","title":"Observability Add-ons","text":"<p>Validate the setup of the observability add-ons by running the following commands and accessing each of the service endpoints using this URL of the form http://localhost:\\ where <code>&lt;port&gt;</code> is one of the port number for the corresponding service. <pre><code># Visualize Istio Mesh console using Kiali\nkubectl port-forward svc/kiali 20001:20001 -n istio-system\n\n# Get to the Prometheus UI\nkubectl port-forward svc/prometheus 9090:9090 -n istio-system\n\n# Visualize metrics in using Grafana\nkubectl port-forward svc/grafana 3000:3000 -n istio-system\n\n# Visualize application traces via Jaeger\nkubectl port-forward svc/jaeger 16686:16686 -n istio-system\n</code></pre>"},{"location":"patterns/istio/#example","title":"Example","text":"<ol> <li> <p>Create the <code>sample</code> namespace and enable the sidecar injection on it</p> <pre><code>kubectl create namespace sample\nkubectl label namespace sample istio-injection=enabled\n</code></pre> <pre><code>namespace/sample created\nnamespace/sample labeled\n</code></pre> </li> <li> <p>Deploy <code>helloworld</code> app</p> <pre><code>cat &lt;&lt;EOF &gt; helloworld.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: helloworld\n  labels:\n    app: helloworld\n    service: helloworld\nspec:\n  ports:\n  - port: 5000\n    name: http\n  selector:\n    app: helloworld\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: helloworld-v1\n  labels:\n    app: helloworld\n    version: v1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: helloworld\n      version: v1\n  template:\n    metadata:\n      labels:\n        app: helloworld\n        version: v1\n    spec:\n      containers:\n      - name: helloworld\n        image: docker.io/istio/examples-helloworld-v1\n        resources:\n          requests:\n            cpu: \"100m\"\n        imagePullPolicy: IfNotPresent #Always\n        ports:\n        - containerPort: 5000\nEOF\n\nkubectl apply --server-side -f helloworld.yaml -n sample\n</code></pre> <pre><code>service/helloworld created\ndeployment.apps/helloworld-v1 created\n</code></pre> </li> <li> <p>Deploy <code>sleep</code> app that we will use to connect to <code>helloworld</code> app</p> <pre><code>cat &lt;&lt;EOF &gt; sleep.yaml\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: sleep\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: sleep\n  labels:\n    app: sleep\n    service: sleep\nspec:\n  ports:\n  - port: 80\n    name: http\n  selector:\n    app: sleep\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sleep\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sleep\n  template:\n    metadata:\n      labels:\n        app: sleep\n    spec:\n      terminationGracePeriodSeconds: 0\n      serviceAccountName: sleep\n      containers:\n      - name: sleep\n        image: curlimages/curl\n        command: [\"/bin/sleep\", \"infinity\"]\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - mountPath: /etc/sleep/tls\n          name: secret-volume\n      volumes:\n      - name: secret-volume\n        secret:\n          secretName: sleep-secret\n          optional: true\nEOF\n\nkubectl apply --server-side -f sleep.yaml -n sample\n</code></pre> <pre><code>serviceaccount/sleep created\nservice/sleep created\ndeployment.apps/sleep created\n</code></pre> </li> <li> <p>Check all the pods in the <code>sample</code> namespace</p> <pre><code>kubectl get pods -n sample\n</code></pre> <pre><code>NAME                           READY   STATUS    RESTARTS   AGE\nhelloworld-v1-b6c45f55-bx2xk   2/2     Running   0          50s\nsleep-9454cc476-p2zxr          2/2     Running   0          15s\n</code></pre> </li> <li> <p>Connect to <code>helloworld</code> app from <code>sleep</code> app and verify if the connection uses envoy proxy</p> <pre><code>kubectl exec -n sample -c sleep \\\n    \"$(kubectl get pod -n sample -l \\\n    app=sleep -o jsonpath='{.items[0].metadata.name}')\" \\\n    -- curl -v helloworld.sample:5000/hello\n</code></pre> <pre><code>* processing: helloworld.sample:5000/hello\n...\n* Connection #0 to host helloworld.sample left intact\n</code></pre> </li> </ol>"},{"location":"patterns/istio/#destroy","title":"Destroy","text":"<p>The AWS Load Balancer Controller add-on asynchronously reconciles resource deletions. During stack destruction, the istio ingress resource and the load balancer controller add-on are deleted in quick succession, preventing the removal of some of the AWS resources associated with the ingress gateway load balancer like, the frontend and the backend security groups. This causes the final <code>terraform destroy -auto-approve</code> command to timeout and fail with VPC dependency errors like below:</p> <pre><code>\u2502 Error: deleting EC2 VPC (vpc-XXXX): operation error EC2: DeleteVpc, https response error StatusCode: 400, RequestID: XXXXX-XXXX-XXXX-XXXX-XXXXXX, api error DependencyViolation: The vpc 'vpc-XXXX' has dependencies and cannot be deleted.\n</code></pre> <p>A possible workaround is to manually uninstall the <code>istio-ingress</code> helm chart.</p> <pre><code>terraform destroy -target='module.eks_blueprints_addons.helm_release.this[\"istio-ingress\"]' -auto-approve\n</code></pre> <p>Once the chart is uninstalled move on to destroy the stack.</p> <pre><code>terraform destroy -target=\"module.eks_blueprints_addons\" -auto-approve\nterraform destroy -target=\"module.eks\" -auto-approve\nterraform destroy -auto-approve\n</code></pre> <p>See here for more details on cleaning up the resources created.</p>"},{"location":"patterns/karpenter-mng/","title":"Karpenter on EKS MNG","text":""},{"location":"patterns/karpenter-mng/#karpenter-on-eks-mng","title":"Karpenter on EKS MNG","text":"<p>This pattern demonstrates how to provision Karpenter on an EKS managed node group. Deploying onto standard EC2 instances created by an EKS managed node group will allow for daemonsets to run on the nodes created for the Karpenter controller, and therefore better unification of tooling across your data plane. This solution is comprised of the following components:</p> <ol> <li>An EKS managed node group that applies both a taint as well as a label for the Karpenter controller. We want the Karpenter controller to target these nodes via a <code>nodeSelector</code> in order to avoid the controller pods from running on nodes that Karpenter itself creates and manages. In addition, we are applying a taint to keep other pods off of these nodes as they are primarily intended for the controller pods. We apply a toleration to the CoreDNS addon, to allow those pods to run on the controller nodes as well. This is needed so that when a cluster is created, the CoreDNS pods have a place to run in order for the Karpenter controller to be provisioned and start managing the additional compute requirements for the cluster. Without letting CoreDNS run on these nodes, the controllers would fail to deploy and the data plane would be in a \"deadlock\" waiting for resources to deploy but unable to do so.</li> <li>The <code>eks-pod-identity-agent</code> addon has been provisioned to allow the Karpenter controller to utilize EKS Pod Identity for AWS permissions via an IAM role.</li> <li>The VPC subnets and node security group have been tagged with <code>\"karpenter.sh/discovery\" = local.name</code> for discoverability by the controller. The controller will discover these resources and use them to provision EC2 resources for the cluster.</li> <li>An IAM role for the Karpenter controller has been created with a trust policy that trusts the EKS Pod Identity service principal. This allows the EKS Pod Identity service to provide AWS credentials to the Karpenter controller pods in order to call AWS APIs.</li> <li>An IAM role for the nodes that Karpenter will create has been created along with a cluster access entry which allows the nodes to acquire permissions to join the cluster. Karpenter will create and manage the instance profile that utilizes this IAM role.</li> <li>An SQS queue has been created that is subscribed to certain EC2 CloudWatch events. This queue is used by Karpenter, allowing it to respond to certain EC2 lifecycle events and gracefully migrate pods off the instance before it is terminated.</li> </ol>"},{"location":"patterns/karpenter-mng/#code","title":"Code","text":"<p>The areas of significance related to this pattern are highlighted in the code provided below.</p>"},{"location":"patterns/karpenter-mng/#cluster","title":"Cluster","text":"<pre><code>################################################################################\n# Cluster\n################################################################################\n\nmodule \"eks\" {\n  source  = \"terraform-aws-modules/eks/aws\"\n  version = \"~&gt; 20.24\"\n\n  cluster_name    = local.name\n  cluster_version = \"1.30\"\n\n  # Give the Terraform identity admin access to the cluster\n  # which will allow it to deploy resources into the cluster\n  enable_cluster_creator_admin_permissions = true\n  cluster_endpoint_public_access           = true\n\n  cluster_addons = {\n    coredns = {\n      configuration_values = jsonencode({\n        tolerations = [\n          # Allow CoreDNS to run on the same nodes as the Karpenter controller\n          # for use during cluster creation when Karpenter nodes do not yet exist\n          {\n            key    = \"karpenter.sh/controller\"\n            value  = \"true\"\n            effect = \"NoSchedule\"\n          }\n        ]\n      })\n    }\n    eks-pod-identity-agent = {}\n    kube-proxy             = {}\n    vpc-cni                = {}\n  }\n\n  vpc_id     = module.vpc.vpc_id\n  subnet_ids = module.vpc.private_subnets\n\n  eks_managed_node_groups = {\n    karpenter = {\n      ami_type       = \"BOTTLEROCKET_x86_64\"\n      instance_types = [\"m5.large\"]\n\n      min_size     = 2\n      max_size     = 3\n      desired_size = 2\n\n      labels = {\n        # Used to ensure Karpenter runs on nodes that it does not manage\n        \"karpenter.sh/controller\" = \"true\"\n      }\n\n      taints = {\n        # The pods that do not tolerate this taint should run on nodes\n        # created by Karpenter\n        karpenter = {\n          key    = \"karpenter.sh/controller\"\n          value  = \"true\"\n          effect = \"NO_SCHEDULE\"\n        }\n      }\n    }\n  }\n\n  node_security_group_tags = merge(local.tags, {\n    # NOTE - if creating multiple security groups with this module, only tag the\n    # security group that Karpenter should utilize with the following tag\n    # (i.e. - at most, only one security group should have this tag in your account)\n    \"karpenter.sh/discovery\" = local.name\n  })\n\n  tags = local.tags\n}\n\noutput \"configure_kubectl\" {\n  description = \"Configure kubectl: make sure you're logged in with the correct AWS profile and run the following command to update your kubeconfig\"\n  value       = \"aws eks --region ${local.region} update-kubeconfig --name ${module.eks.cluster_name}\"\n}\n</code></pre>"},{"location":"patterns/karpenter-mng/#karpenter-resources","title":"Karpenter Resources","text":"<pre><code>locals {\n  namespace = \"karpenter\"\n}\n\n################################################################################\n# Controller &amp; Node IAM roles, SQS Queue, Eventbridge Rules\n################################################################################\n\nmodule \"karpenter\" {\n  source  = \"terraform-aws-modules/eks/aws//modules/karpenter\"\n  version = \"~&gt; 20.24\"\n\n  cluster_name          = module.eks.cluster_name\n  enable_v1_permissions = true\n  namespace             = local.namespace\n\n  # Name needs to match role name passed to the EC2NodeClass\n  node_iam_role_use_name_prefix   = false\n  node_iam_role_name              = local.name\n  create_pod_identity_association = true\n\n  tags = local.tags\n}\n\n################################################################################\n# Helm charts\n################################################################################\n\nresource \"helm_release\" \"karpenter\" {\n  name                = \"karpenter\"\n  namespace           = local.namespace\n  create_namespace    = true\n  repository          = \"oci://public.ecr.aws/karpenter\"\n  repository_username = data.aws_ecrpublic_authorization_token.token.user_name\n  repository_password = data.aws_ecrpublic_authorization_token.token.password\n  chart               = \"karpenter\"\n  version             = \"1.0.2\"\n  wait                = false\n\n  values = [\n    &lt;&lt;-EOT\n    nodeSelector:\n      karpenter.sh/controller: 'true'\n    settings:\n      clusterName: ${module.eks.cluster_name}\n      clusterEndpoint: ${module.eks.cluster_endpoint}\n      interruptionQueue: ${module.karpenter.queue_name}\n    tolerations:\n      - key: CriticalAddonsOnly\n        operator: Exists\n      - key: karpenter.sh/controller\n        operator: Exists\n        effect: NoSchedule\n    webhook:\n      enabled: false\n    EOT\n  ]\n\n  lifecycle {\n    ignore_changes = [\n      repository_password\n    ]\n  }\n}\n</code></pre> <pre><code>---\napiVersion: karpenter.k8s.aws/v1\nkind: EC2NodeClass\nmetadata:\n  name: default\nspec:\n  amiSelectorTerms:\n    - alias: bottlerocket@latest\n  role: ex-karpenter-mng\n  subnetSelectorTerms:\n    - tags:\n        karpenter.sh/discovery: ex-karpenter-mng\n  securityGroupSelectorTerms:\n    - tags:\n        karpenter.sh/discovery: ex-karpenter-mng\n  tags:\n    karpenter.sh/discovery: ex-karpenter-mng\n---\napiVersion: karpenter.sh/v1\nkind: NodePool\nmetadata:\n  name: default\nspec:\n  template:\n    spec:\n      nodeClassRef:\n        group: karpenter.k8s.aws\n        kind: EC2NodeClass\n        name: default\n      requirements:\n        - key: \"karpenter.k8s.aws/instance-category\"\n          operator: In\n          values: [\"c\", \"m\", \"r\"]\n        - key: \"karpenter.k8s.aws/instance-cpu\"\n          operator: In\n          values: [\"4\", \"8\", \"16\", \"32\"]\n        - key: \"karpenter.k8s.aws/instance-hypervisor\"\n          operator: In\n          values: [\"nitro\"]\n        - key: \"karpenter.k8s.aws/instance-generation\"\n          operator: Gt\n          values: [\"2\"]\n  limits:\n    cpu: 1000\n  disruption:\n    consolidationPolicy: WhenEmpty\n    consolidateAfter: 30s\n</code></pre>"},{"location":"patterns/karpenter-mng/#vpc","title":"VPC","text":"<pre><code>module \"vpc\" {\n  source  = \"terraform-aws-modules/vpc/aws\"\n  version = \"~&gt; 5.0\"\n\n  name = local.name\n  cidr = local.vpc_cidr\n\n  azs             = local.azs\n  private_subnets = [for k, v in local.azs : cidrsubnet(local.vpc_cidr, 4, k)]\n  public_subnets  = [for k, v in local.azs : cidrsubnet(local.vpc_cidr, 8, k + 48)]\n\n  enable_nat_gateway = true\n  single_nat_gateway = true\n\n  public_subnet_tags = {\n    \"kubernetes.io/role/elb\" = 1\n  }\n\n  private_subnet_tags = {\n    \"kubernetes.io/role/internal-elb\" = 1\n    # Tags subnets for Karpenter auto-discovery\n    \"karpenter.sh/discovery\" = local.name\n  }\n\n  tags = local.tags\n}\n</code></pre>"},{"location":"patterns/karpenter-mng/#deploy","title":"Deploy","text":"<p>See here for the prerequisites and steps to deploy this pattern.</p>"},{"location":"patterns/karpenter-mng/#validate","title":"Validate","text":"<ol> <li> <p>Test by listing the nodes in the cluster. You should see four Fargate nodes in the cluster:</p> <pre><code>kubectl get nodes\n\nNAME                                        STATUS   ROLES    AGE     VERSION\nip-10-0-23-32.us-west-2.compute.internal    Ready    &lt;none&gt;   10m     v1.30.4-eks-a737599\nip-10-0-6-222.us-west-2.compute.internal    Ready    &lt;none&gt;   10m     v1.30.4-eks-a737599\n</code></pre> </li> <li> <p>Provision the Karpenter <code>EC2NodeClass</code> and <code>NodePool</code> resources which provide Karpenter the necessary configurations to provision EC2 resources:</p> <pre><code>kubectl apply --server-side -f karpenter.yaml\n</code></pre> </li> <li> <p>Once the Karpenter resources are in place, Karpenter will provision the necessary EC2 resources to satisfy any pending pods in the scheduler's queue. You can demonstrate this with the example deployment provided. First deploy the example deployment which has the initial number replicas set to 0:</p> <pre><code>kubectl apply --server-side -f example.yaml\n</code></pre> </li> <li> <p>When you scale the example deployment, you should see Karpenter respond by quickly provisioning EC2 resources to satisfy those pending pod requests:</p> <pre><code>kubectl scale deployment inflate --replicas=3\n</code></pre> </li> <li> <p>Listing the nodes should now show some EC2 compute that Karpenter has created for the example deployment:</p> <pre><code>kubectl get nodes\n\nNAME                                        STATUS   ROLES    AGE     VERSION\nip-10-0-23-32.us-west-2.compute.internal    Ready    &lt;none&gt;   10m     v1.30.4-eks-a737599\nip-10-0-46-239.us-west-2.compute.internal   Ready    &lt;none&gt;   20s     v1.30.1-eks-e564799 # &lt;== EC2 created by Karpenter\nip-10-0-6-222.us-west-2.compute.internal    Ready    &lt;none&gt;   10m     v1.30.4-eks-a737599\n</code></pre> </li> </ol>"},{"location":"patterns/karpenter-mng/#destroy","title":"Destroy","text":"<p>Scale down the deployment to de-provision Karpenter created resources first:</p> <pre><code>kubectl delete -f example.yaml\n</code></pre> <p>Remove the Karpenter Helm chart:</p> <pre><code>terraform destroy -target=helm_release.karpenter --auto-approve\n</code></pre> <pre><code>terraform destroy -target=\"module.eks_blueprints_addons\" -auto-approve\nterraform destroy -target=\"module.eks\" -auto-approve\nterraform destroy -auto-approve\n</code></pre> <p>See here for more details on cleaning up the resources created.</p>"},{"location":"patterns/karpenter/","title":"Karpenter on EKS Fargate","text":""},{"location":"patterns/karpenter/#karpenter-on-eks-fargate","title":"Karpenter on EKS Fargate","text":"<p>This pattern demonstrates how to provision Karpenter on a serverless cluster (serverless data plane) using Fargate Profiles.</p>"},{"location":"patterns/karpenter/#code","title":"Code","text":"<p>The areas of significance related to this pattern are highlighted in the code provided below.</p>"},{"location":"patterns/karpenter/#cluster","title":"Cluster","text":"<pre><code>################################################################################\n# Cluster\n################################################################################\n\nmodule \"eks\" {\n  source  = \"terraform-aws-modules/eks/aws\"\n  version = \"~&gt; 20.24\"\n\n  cluster_name    = local.name\n  cluster_version = \"1.30\"\n\n  # Give the Terraform identity admin access to the cluster\n  # which will allow it to deploy resources into the cluster\n  enable_cluster_creator_admin_permissions = true\n  cluster_endpoint_public_access           = true\n\n  cluster_addons = {\n    # Enable after creation to run on Karpenter managed nodes\n    # coredns                = {}\n    eks-pod-identity-agent = {}\n    kube-proxy             = {}\n    vpc-cni                = {}\n  }\n\n  vpc_id     = module.vpc.vpc_id\n  subnet_ids = module.vpc.private_subnets\n\n  # Fargate profiles use the cluster primary security group\n  # Therefore these are not used and can be skipped\n  create_cluster_security_group = false\n  create_node_security_group    = false\n\n  fargate_profiles = {\n    karpenter = {\n      selectors = [\n        { namespace = \"karpenter\" }\n      ]\n    }\n  }\n\n  tags = merge(local.tags, {\n    # NOTE - if creating multiple security groups with this module, only tag the\n    # security group that Karpenter should utilize with the following tag\n    # (i.e. - at most, only one security group should have this tag in your account)\n    \"karpenter.sh/discovery\" = local.name\n  })\n}\n\noutput \"configure_kubectl\" {\n  description = \"Configure kubectl: make sure you're logged in with the correct AWS profile and run the following command to update your kubeconfig\"\n  value       = \"aws eks --region ${local.region} update-kubeconfig --name ${module.eks.cluster_name}\"\n}\n</code></pre>"},{"location":"patterns/karpenter/#karpenter-resources","title":"Karpenter Resources","text":"<pre><code>locals {\n  namespace = \"karpenter\"\n}\n\n################################################################################\n# Controller &amp; Node IAM roles, SQS Queue, Eventbridge Rules\n################################################################################\n\nmodule \"karpenter\" {\n  source  = \"terraform-aws-modules/eks/aws//modules/karpenter\"\n  version = \"~&gt; 20.24\"\n\n  cluster_name          = module.eks.cluster_name\n  enable_v1_permissions = true\n  namespace             = local.namespace\n\n  # Name needs to match role name passed to the EC2NodeClass\n  node_iam_role_use_name_prefix = false\n  node_iam_role_name            = local.name\n\n  # EKS Fargate does not support pod identity\n  create_pod_identity_association = false\n  enable_irsa                     = true\n  irsa_oidc_provider_arn          = module.eks.oidc_provider_arn\n\n  tags = local.tags\n}\n\n################################################################################\n# Helm charts\n################################################################################\n\nresource \"helm_release\" \"karpenter\" {\n  name                = \"karpenter\"\n  namespace           = local.namespace\n  create_namespace    = true\n  repository          = \"oci://public.ecr.aws/karpenter\"\n  repository_username = data.aws_ecrpublic_authorization_token.token.user_name\n  repository_password = data.aws_ecrpublic_authorization_token.token.password\n  chart               = \"karpenter\"\n  version             = \"1.0.2\"\n  wait                = false\n\n  values = [\n    &lt;&lt;-EOT\n    dnsPolicy: Default\n    settings:\n      clusterName: ${module.eks.cluster_name}\n      clusterEndpoint: ${module.eks.cluster_endpoint}\n      interruptionQueue: ${module.karpenter.queue_name}\n    serviceAccount:\n      annotations:\n        eks.amazonaws.com/role-arn: ${module.karpenter.iam_role_arn}\n    webhook:\n      enabled: false\n    EOT\n  ]\n\n  lifecycle {\n    ignore_changes = [\n      repository_password\n    ]\n  }\n}\n</code></pre> <pre><code>---\napiVersion: karpenter.k8s.aws/v1\nkind: EC2NodeClass\nmetadata:\n  name: default\nspec:\n  amiSelectorTerms:\n    - alias: bottlerocket@latest\n  role: ex-karpenter\n  subnetSelectorTerms:\n    - tags:\n        karpenter.sh/discovery: ex-karpenter\n  securityGroupSelectorTerms:\n    - tags:\n        karpenter.sh/discovery: ex-karpenter\n  tags:\n    karpenter.sh/discovery: ex-karpenter\n---\napiVersion: karpenter.sh/v1\nkind: NodePool\nmetadata:\n  name: default\nspec:\n  template:\n    spec:\n      nodeClassRef:\n        group: karpenter.k8s.aws\n        kind: EC2NodeClass\n        name: default\n      requirements:\n        - key: \"karpenter.k8s.aws/instance-category\"\n          operator: In\n          values: [\"c\", \"m\", \"r\"]\n        - key: \"karpenter.k8s.aws/instance-cpu\"\n          operator: In\n          values: [\"4\", \"8\", \"16\", \"32\"]\n        - key: \"karpenter.k8s.aws/instance-hypervisor\"\n          operator: In\n          values: [\"nitro\"]\n        - key: \"karpenter.k8s.aws/instance-generation\"\n          operator: Gt\n          values: [\"2\"]\n  limits:\n    cpu: 1000\n  disruption:\n    consolidationPolicy: WhenEmpty\n    consolidateAfter: 30s\n</code></pre>"},{"location":"patterns/karpenter/#vpc","title":"VPC","text":"<pre><code>module \"vpc\" {\n  source  = \"terraform-aws-modules/vpc/aws\"\n  version = \"~&gt; 5.0\"\n\n  name = local.name\n  cidr = local.vpc_cidr\n\n  azs             = local.azs\n  private_subnets = [for k, v in local.azs : cidrsubnet(local.vpc_cidr, 4, k)]\n  public_subnets  = [for k, v in local.azs : cidrsubnet(local.vpc_cidr, 8, k + 48)]\n\n  enable_nat_gateway = true\n  single_nat_gateway = true\n\n  public_subnet_tags = {\n    \"kubernetes.io/role/elb\" = 1\n  }\n\n  private_subnet_tags = {\n    \"kubernetes.io/role/internal-elb\" = 1\n    # Tags subnets for Karpenter auto-discovery\n    \"karpenter.sh/discovery\" = local.name\n  }\n\n  tags = local.tags\n}\n</code></pre>"},{"location":"patterns/karpenter/#deploy","title":"Deploy","text":"<p>See here for the prerequisites and steps to deploy this pattern.</p>"},{"location":"patterns/karpenter/#validate","title":"Validate","text":"<ol> <li> <p>Test by listing the nodes in the cluster. You should see two Fargate nodes in the cluster:</p> <pre><code>kubectl get nodes\n\nNAME                                               STATUS   ROLES    AGE    VERSION\nfargate-ip-10-0-16-92.us-west-2.compute.internal   Ready    &lt;none&gt;   2m3s   v1.30.0-eks-404b9c6\nfargate-ip-10-0-8-95.us-west-2.compute.internal    Ready    &lt;none&gt;   2m3s   v1.30.0-eks-404b9c6\n</code></pre> </li> <li> <p>Provision the Karpenter <code>EC2NodeClass</code> and <code>NodePool</code> resources which provide Karpenter the necessary configurations to provision EC2 resources:</p> <pre><code>kubectl apply --server-side -f karpenter.yaml\n</code></pre> </li> <li> <p>Once the Karpenter resources are in place, Karpenter will provision the necessary EC2 resources to satisfy any pending pods in the scheduler's queue. You can demonstrate this with the example deployment provided. First deploy the example deployment which has the initial number replicas set to 0:</p> <pre><code>kubectl apply --server-side -f example.yaml\n</code></pre> </li> <li> <p>When you scale the example deployment, you should see Karpenter respond by quickly provisioning EC2 resources to satisfy those pending pod requests:</p> <pre><code>kubectl scale deployment inflate --replicas=3\n</code></pre> </li> <li> <p>Listing the nodes should now show some EC2 compute that Karpenter has created for the example deployment:</p> <pre><code>kubectl get nodes\n\nNAME                                               STATUS   ROLES    AGE    VERSION\nfargate-ip-10-0-16-92.us-west-2.compute.internal   Ready    &lt;none&gt;   2m3s   v1.30.0-eks-404b9c6\nfargate-ip-10-0-8-95.us-west-2.compute.internal    Ready    &lt;none&gt;   2m3s   v1.30.0-eks-404b9c6\nip-10-0-21-175.us-west-2.compute.internal          Ready    &lt;none&gt;   88s    v1.30.1-eks-e564799 # &lt;== EC2 created by Karpenter\n</code></pre> </li> </ol>"},{"location":"patterns/karpenter/#destroy","title":"Destroy","text":"<p>Scale down the deployment to de-provision Karpenter created resources first:</p> <pre><code>kubectl delete -f example.yaml\n</code></pre> <p>Remove the Karpenter Helm chart:</p> <pre><code>terraform destroy -target=helm_release.karpenter --auto-approve\n</code></pre> <pre><code>terraform destroy -target=\"module.eks_blueprints_addons\" -auto-approve\nterraform destroy -target=\"module.eks\" -auto-approve\nterraform destroy -auto-approve\n</code></pre> <p>See here for more details on cleaning up the resources created.</p>"},{"location":"patterns/kubecost/","title":"Kubecost","text":""},{"location":"patterns/kubecost/#kubecost-with-aws-cloud-billing-integration","title":"Kubecost with AWS Cloud Billing Integration","text":"<p>This pattern demonstrates how to install and configure Kubecost with AWS CUR report. The terraform code was created following the official Kubecost documentation for aws cloud billing integration.</p>"},{"location":"patterns/kubecost/#prerequisites","title":"Prerequisites","text":"<p>You need a valid Kubecost token. To generate one, follow the instructions here.</p>"},{"location":"patterns/kubecost/#deploy","title":"Deploy","text":"<pre><code>terraform init\nterraform apply -target=\"module.vpc\" -auto-approve\nterraform apply -target=\"module.eks\" -auto-approve\nterraform apply -auto-approve --var=\"kubecost_token=&lt;your-kubecost-token&gt;\"\n</code></pre> <p>Once all of the resources have successfully been provisioned, the following command can be used to update the <code>kubeconfig</code> on your local machine and allow you to interact with your EKS Cluster using <code>kubectl</code>.</p> <pre><code>aws eks --region &lt;REGION&gt; update-kubeconfig --name &lt;CLUSTER_NAME&gt;\n</code></pre> <p>This command will create a S3 bucket with prefix <code>kubecost-</code> and a Cost and Usage Report (CUR). Within 24h The CUR will generate a CloudFormation template file called <code>crawler-cfn.yml</code> in the S3 bucket. Once that file is generated, navigate to:</p> <pre><code>cd run-me-in-24h/\n</code></pre> <p>To download and apply the CloudFormation template, run:</p> <pre><code>terraform init\nterraform apply --auto-approve\n</code></pre>"},{"location":"patterns/kubecost/#kubecost-ui","title":"Kubecost UI","text":"<p>To access the Kubecost UI run:</p> <pre><code>echo http://$(kubectl -n kubecost get svc cost-analyzer-cost-analyzer \\\n    -o jsonpath='{.status.loadBalancer.ingress[0].hostname}'):9090/\n</code></pre> <p>and then navigate to the output URL.</p> <p>Navigate to Settings -&gt; Diagnostics -&gt; View Full Diagnostics</p> <p>Expected result:</p> <p>Note</p> <p>Spot Data Feed is included in Savings Plan, Reserved Instance, and Out-Of-Cluster.</p>"},{"location":"patterns/kubecost/#destroy","title":"Destroy","text":"<p>First destroy the CloudFormation template:</p> <pre><code>cd run-me-in-24h/\nterraform destroy --auto-approve\ncd ..\nterraform destroy -target=\"module.eks_blueprints_addon\" --var=\"kubecost_token=&lt;your-kubecost-token&gt;\" -auto-approve\nterraform destroy -target=\"module.eks_blueprints_addons\"\nterraform destroy -target=\"module.eks\" -auto-approve\nterraform destroy -auto-approve\n</code></pre>"},{"location":"patterns/multi-tenancy-with-teams/","title":"Multi-Tenancy w/ Teams","text":""},{"location":"patterns/multi-tenancy-with-teams/#multi-tenancy-w-teams","title":"Multi-Tenancy w/ Teams","text":"<p>This pattern demonstrates how to provision and configure a multi-tenancy Amazon EKS cluster with safeguards for resource consumption and namespace isolation.</p> <p>This example solution provides:</p> <ul> <li>Two development teams - <code>team-red</code> and <code>team-blue</code> - isolated to their respective namespaces</li> <li>An admin team with privileged access to the cluster (<code>team-admin</code>)</li> </ul>"},{"location":"patterns/multi-tenancy-with-teams/#deploy","title":"Deploy","text":"<p>See here for the prerequisites and steps to deploy this pattern.</p>"},{"location":"patterns/multi-tenancy-with-teams/#validate","title":"Validate","text":"<p>TODO</p> <p>Add in validation steps</p>"},{"location":"patterns/multi-tenancy-with-teams/#destroy","title":"Destroy","text":"<pre><code>terraform destroy -target=\"module.eks_blueprints_addons\" -auto-approve\nterraform destroy -target=\"module.eks\" -auto-approve\nterraform destroy -auto-approve\n</code></pre> <p>See here for more details on cleaning up the resources created.</p>"},{"location":"patterns/sso-iam-identity-center/","title":"SSO - IAM Identity Center","text":""},{"location":"patterns/sso-iam-identity-center/#iam-identity-center-single-sign-on-for-amazon-eks-cluster-with-cluster-access-manager","title":"IAM Identity Center Single Sign-On for Amazon EKS Cluster with Cluster Access Manager","text":"<p>This example demonstrates how to deploy an Amazon EKS cluster that is deployed on the AWS Cloud, integrated with IAM Identity Center (former AWS SSO) as an the Identity Provider (IdP) for Single Sign-On (SSO) authentication having Cluster Access Manager as the entry point API. The configuration to provide authorization for users is simplified using a combination of Access Entries and Kubernetes Role-based access control (RBAC).</p>"},{"location":"patterns/sso-iam-identity-center/#deploy","title":"Deploy","text":"<p>See here for the prerequisites and steps to deploy this pattern.</p> <p>This patterns uses aws Identity datasource <code>data \"aws_ssoadmin_instances\" \"this\" {}</code> to retrieve your identity center configuration. You can check that the datasource can works by using the aws cli:</p> <pre><code>aws identitystore list-instances\n</code></pre> <p>You may need to specify the appropriate AWS region in either the previous command and terraform (in main.tf) for the pattern to succeed. If the previous command is empty, you may want to properly configure AWS identity center within your account.</p>"},{"location":"patterns/sso-iam-identity-center/#validate","title":"Validate","text":"<p>After the <code>terraform</code> commands are executed successfully, check if the newly created users are active.</p> <p>To do that use the link provided in the email invite - if you added a valid email address for your users either in your Terraform code or IAM Identity Center Console - or go to the IAM Identity Center Console, in the Users dashboard on the left hand side menu, then select the user, and click on Reset password button on the upper right corner. Choose the option to Generate a one-time password and share the password with the user.</p> <p>With the active users, use one of the <code>terraform output</code> examples to configure your AWS credentials for SSO, as shown in the examples below. After you choose the SSO registration scopes, your browser windows will appear and request to login using your IAM Identity Center username and password.</p>"},{"location":"patterns/sso-iam-identity-center/#admin-user-example","title":"Admin user example","text":"<pre><code>configure_sso_admin = &lt;&lt;EOT\n  # aws configure sso --profile EKSClusterAdmin\n      SSO session name (Recommended): &lt;SESSION_NAME&gt;\n      SSO start URL [None]: https://d-1234567890.awsapps.com/start\n      SSO region [None]: us-west-2\n      SSO registration scopes [sso:account:access]:\n      Attempting to automatically open the SSO authorization page in your default browser.\n      If the browser does not open or you wish to use a different device to authorize this request, open the following URL:\n\n      https://device.sso.us-west-2.amazonaws.com/\n\n      Then enter the code:\n\n      The only AWS account available to you is: 123456789012\n      Using the account ID 123456789012\n      The only role available to you is: EKSClusterAdmin\n      Using the role name EKSClusterAdmin\n      CLI default client Region [us-west-2]: us-west-2\n      CLI default output format [json]: json\n      CLI profile name [EKSClusterAdmin-123456789012]:\n\n      To use this profile, specify the profile name using --profile, as shown:\n\n  # aws eks --region us-west-2 update-kubeconfig --name iam-identity-center --profile EKSClusterAdmin\n\nEOT\n</code></pre>"},{"location":"patterns/sso-iam-identity-center/#read-only-user-example","title":"Read-only user example","text":"<pre><code>configure_sso_user = &lt;&lt;EOT\n  # aws configure sso --profile EKSClusterUser\n      SSO session name (Recommended): &lt;SESSION_NAME&gt;\n      SSO start URL [None]: https://d-1234567890.awsapps.com/start\n      SSO region [None]: us-west-2\n      SSO registration scopes [sso:account:access]:\n      Attempting to automatically open the SSO authorization page in your default browser.\n      If the browser does not open or you wish to use a different device to authorize this request, open the following URL:\n\n      https://device.sso.us-west-2.amazonaws.com/\n\n      Then enter the code:\n\n      The only AWS account available to you is: 123456789012\n      Using the account ID 123456789012\n      The only role available to you is: EKSClusterUser\n      Using the role name EKSClusterUser\n      CLI default client Region [us-west-2]: us-west-2\n      CLI default output format [json]: json\n      CLI profile name [EKSClusterUser-123456789012]:\n\n      To use this profile, specify the profile name using --profile, as shown:\n\n  # aws eks --region us-west-2 update-kubeconfig --name iam-identity-center --profile EKSClusterUser\n\nEOT\n</code></pre> <p>With the <code>kubeconfig</code> configured, you'll be able to run <code>kubectl</code> commands in your Amazon EKS Cluster with the impersonated user. The read-only user has a <code>cluster-viewer</code> Kubernetes role bound to it's group, whereas the admin user, has the <code>admin</code> Kubernetes role bound to it's group.</p> <pre><code>kubectl get pods -A\nNAMESPACE          NAME                        READY   STATUS    RESTARTS   AGE\namazon-guardduty   aws-guardduty-agent-bl2v2   1/1     Running   0          3h54m\namazon-guardduty   aws-guardduty-agent-s2vcx   1/1     Running   0          3h54m\namazon-guardduty   aws-guardduty-agent-w8gfc   1/1     Running   0          3h54m\nkube-system        aws-node-m9hmd              1/1     Running   0          3h53m\nkube-system        aws-node-w42b8              1/1     Running   0          3h53m\nkube-system        aws-node-wm6rm              1/1     Running   0          3h53m\nkube-system        coredns-6ff9c46cd8-94jlr    1/1     Running   0          3h59m\nkube-system        coredns-6ff9c46cd8-n2mrb    1/1     Running   0          3h59m\nkube-system        kube-proxy-7fb86            1/1     Running   0          3h54m\nkube-system        kube-proxy-p4f5g            1/1     Running   0          3h54m\nkube-system        kube-proxy-q1fmc            1/1     Running   0          3h54m\n</code></pre> <p>If not revoked after the cluster creation, it's possible to use the <code>configure_kubectl</code> output to assume the Cluster creator role with <code>cluster-admin</code> access.</p> <pre><code>configure_kubectl = \"aws eks --region us-west-2 update-kubeconfig --name iam-identity-center\"\n</code></pre>"},{"location":"patterns/sso-iam-identity-center/#destroy","title":"Destroy","text":"<p>If you revoked the Cluster creator <code>cluster-admin</code> permission, you may need to re-associate the <code>AmazonEKSClusterAdminPolicy</code> access entry to run <code>terraform destroy</code>.</p> <pre><code>terraform destroy -target module.developers_team -auto-approve\nterraform destroy -target module.eks -auto-approve\nterraform destroy -auto-approve\n</code></pre>"},{"location":"patterns/sso-okta/","title":"SSO - Okta","text":""},{"location":"patterns/sso-okta/#okta-single-sign-on-for-amazon-eks-cluster","title":"Okta Single Sign-On for Amazon EKS Cluster","text":"<p>This example demonstrates how to deploy an Amazon EKS cluster that is deployed on the AWS Cloud, integrated with Okta as an the Identity Provider (IdP) for Single Sign-On (SSO) authentication. The configuration for authorization is done using Kubernetes Role-based access control (RBAC).</p>"},{"location":"patterns/sso-okta/#deploy","title":"Deploy","text":"<p>See here for the prerequisites and steps to deploy this pattern.</p>"},{"location":"patterns/sso-okta/#validate","title":"Validate","text":"<p>After the <code>terraform</code> commands are executed successfully, check if the newly created users are active.</p> <p>To do that use the link provided in the email invite if you added a valid email address for your users, or go to the Okta Admin Dashboard, select the user, and click on Set Password and Activate button.</p> <p>With the active users, use the <code>terraform output</code> example to setup your <code>kubeconfig</code> profile to authenticate through Okta.</p> <pre><code>configure_kubeconfig = &lt;&lt;EOT\n    kubectl config set-credentials oidc \\\n      --exec-api-version=client.authentication.k8s.io/v1beta1 \\\n      --exec-command=kubectl \\\n      --exec-arg=oidc-login \\\n      --exec-arg=get-token \\\n      --exec-arg=--oidc-issuer-url=https://dev-ORGID.okta.com/oauth2/1234567890abcdefghij \\\n      --exec-arg=--oidc-client-id=1234567890abcdefghij\n      --exec-arg=--oidc-extra-scope=\"email offline_access profile openid\"\n</code></pre> <p>With the <code>kubeconfig</code> configured, you'll be able to run <code>kubectl</code> commands in your Amazon EKS Cluster using the <code>--user</code> cli option to impersonate the Okta authenticated user. When <code>kubectl</code> command is issued with the <code>--user</code> option for the first time, your browser window will open and require you to authenticate.</p> <p>The read-only user has a <code>cluster-viewer</code> Kubernetes role bound to it's group, whereas the admin user, has the <code>admin</code> Kubernetes role bound to it's group.</p> <pre><code>kubectl get pods -A\nNAMESPACE          NAME                        READY   STATUS    RESTARTS   AGE\namazon-guardduty   aws-guardduty-agent-bl2v2   1/1     Running   0          3h54m\namazon-guardduty   aws-guardduty-agent-s1vcx   1/1     Running   0          3h54m\namazon-guardduty   aws-guardduty-agent-w8gfc   1/1     Running   0          3h54m\nkube-system        aws-node-m9hmd              1/1     Running   0          3h53m\nkube-system        aws-node-w42b8              1/1     Running   0          3h53m\nkube-system        aws-node-wm6rm              1/1     Running   0          3h53m\nkube-system        coredns-6ff9c46cd8-94jlr    1/1     Running   0          3h59m\nkube-system        coredns-6ff9c46cd8-nw2rb    1/1     Running   0          3h59m\nkube-system        kube-proxy-7fb86            1/1     Running   0          3h54m\nkube-system        kube-proxy-p4f5g            1/1     Running   0          3h54m\nkube-system        kube-proxy-qk2mc            1/1     Running   0          3h54m\n</code></pre> <p>You can also use the <code>configure_kubectl</code> output to assume the Cluster creator role with <code>cluster-admin</code> access.</p> <pre><code>configure_kubectl = \"aws eks --region us-west-2 update-kubeconfig --name okta\"\n</code></pre> <p>It's also possible to pre-configure your <code>kubeconfig</code> using the <code>okta_login</code> output. This will also require you to authenticate in a browser window.</p> <pre><code>okta_login = \"kubectl oidc-login setup --oidc-issuer-url=https://dev-ORGID.okta.com/oauth2/1234567890abcdefghij--oidc-client-id=1234567890abcdefghij\"\n</code></pre>"},{"location":"patterns/sso-okta/#destroy","title":"Destroy","text":"<pre><code>terraform destroy -target=\"module.eks_blueprints_addons\" -auto-approve\nterraform destroy -target=\"module.eks\" -auto-approve\nterraform destroy -auto-approve\n</code></pre> <p>See here for more details on cleaning up the resources created.</p>"},{"location":"patterns/stateful/","title":"Stateful","text":""},{"location":"patterns/stateful/#amazon-eks-cluster-for-stateful-workloads","title":"Amazon EKS Cluster for Stateful Workloads","text":""},{"location":"patterns/stateful/#features","title":"Features","text":"<p>Please note: not all of the features listed below are required for stateful workloads on EKS. We are simply grouping together a set of features that are commonly encountered when managing stateful workloads. Users are encouraged to only enable the features that are required for their workload(s) and use case(s).</p>"},{"location":"patterns/stateful/#velero","title":"velero","text":"<p>(From the project documentation) <code>velero</code> (formerly Heptio Ark) gives you tools to back up and restore your Kubernetes cluster resources and persistent volumes. You can run Velero with a public cloud platform or on-premises. Velero lets you:</p> <ul> <li>Take backups of your cluster and restore in case of loss.</li> <li>Migrate cluster resources to other clusters.</li> <li>Replicate your production cluster to development and testing clusters.</li> </ul>"},{"location":"patterns/stateful/#ebs-efs-csi-drivers","title":"EBS &amp; EFS CSI Drivers","text":"<ul> <li>A second storage class for <code>gp3</code> backed volumes has been added and made the default over the EKS default <code>gp2</code> storage class (<code>gp2</code> storage class remains in the cluster for use, but it is no longer the default storage class)</li> <li>A standard implementation of the EFS CSI driver</li> </ul>"},{"location":"patterns/stateful/#eks-managed-nodegroup-w-multiple-volumes","title":"EKS Managed Nodegroup w/ Multiple Volumes","text":"<p>An EKS managed nodegroup that utilizes multiple EBS volumes. The primary use case demonstrated in this example is a second volume that is dedicated to the <code>containerd</code> runtime to ensure the root volume is not filled up nor has its I/O exhausted to ensure the instance does not reach a degraded state. The <code>containerd</code> directories are mapped to this volume. You can read more about this recommendation in our EKS best practices guide and refer to the <code>containerd</code> documentation for more information. The update for <code>containerd</code> to use the second volume is managed through the provided user data.</p> <p>In addition, the following properties are configured on the nodegroup volumes:</p> <ul> <li>EBS encryption using a customer managed key (CMK)</li> <li>Configuring the volumes to use GP3 storage</li> </ul>"},{"location":"patterns/stateful/#eks-managed-nodegroup-w-instance-store-volumes","title":"EKS Managed Nodegroup w/ Instance Store Volume(s)","text":"<p>An EKS managed nodegroup that utilizes EC2 instances with ephemeral instance store(s). Instance stores are ideal for temporary storage of information that changes frequently, such as buffers, caches, scratch data, and other temporary content, or for data that is replicated across a fleet of instances. You can read more about instance stores in the AWS documentation; and be sure to check out the <code>Block device mapping instance store caveats</code> section as well which covers why the example has provided user data for mounting the instance store(s). The size and number of instance stores will vary based on the EC2 instance type and class.</p> <p>In addition, the following properties are configured on the nodegroup volumes:</p> <ul> <li>EBS encryption using a customer managed key (CMK)</li> <li>Configuring the volumes to use GP3 storage</li> </ul>"},{"location":"patterns/stateful/#deploy","title":"Deploy","text":"<p>See here for the prerequisites and steps to deploy this pattern.</p>"},{"location":"patterns/stateful/#validate","title":"Validate","text":"<p>The following command will update the <code>kubeconfig</code> on your local machine and allow you to interact with your EKS Cluster using <code>kubectl</code> to validate the Velero deployment.</p> <ol> <li> <p>Run <code>update-kubeconfig</code> command:</p> <pre><code>aws eks --region &lt;REGION&gt; update-kubeconfig --name &lt;CLUSTER_NAME&gt;\n</code></pre> </li> <li> <p>List the storage classes to view that <code>efs</code>, <code>gp2</code>, and <code>gp3</code> classes are present and <code>gp3</code> is the default storage class</p> <pre><code>kubectl get storageclasses\n</code></pre> <pre><code>NAME            PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE\nefs             efs.csi.aws.com         Delete          Immediate              true                   2m19s\ngp2             kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   false                  15m\ngp3 (default)   ebs.csi.aws.com         Delete          WaitForFirstConsumer   true                   2m19s\n</code></pre> </li> <li> <p>From an instance launched with instance store(s), check that the instance store has been mounted correctly. To verify, first install the <code>nvme-cli</code> tool and then use it to verify. To verify, you can access the instance using SSM Session Manager:</p> <pre><code># Install the nvme-cli tool\nsudo yum install nvme-cli -y\n\n# Show NVMe volumes attached\nsudo nvme list\n</code></pre> <pre><code># Notice the model is `EC2 NVMe Instance Storage` for the instance store\nNode             SN                   Model                                    Namespace Usage                      Format           FW Rev\n---------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- --------\n/dev/nvme0n1     vol0546d3c3b0af0bf6d Amazon Elastic Block Store               1          25.77  GB /  25.77  GB    512   B +  0 B   1.0\n/dev/nvme1n1     AWS24BBF51AF55097008 Amazon EC2 NVMe Instance Storage         1          75.00  GB /  75.00  GB    512   B +  0 B   0\n\n# Show disks, their partitions and mounts\nsudo lsblk\n\n# Output should look like below\nNAME          MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\nnvme0n1       259:0    0   24G  0 disk\n\u251c\u2500nvme0n1p1   259:2    0   24G  0 part /\n\u2514\u2500nvme0n1p128 259:3    0    1M  0 part\nnvme1n1       259:1    0 69.9G  0 disk /local1 # &lt;--- this is the instance store\n</code></pre> </li> <li> <p>From an instance launched with multiple volume(s), check that the instance store has been mounted correctly. To verify, first install the <code>nvme-cli</code> tool and then use it to verify. To verify, you can access the instance using SSM Session Manager:</p> <pre><code># Install the nvme-cli tool\nsudo yum install nvme-cli -y\n\n# Show NVMe volumes attached\nsudo nvme list\n</code></pre> <pre><code># /dev/nvme0n1 is the root volume and /dev/nvme1n1 is the second, additional volume\nNode             SN                   Model                                    Namespace Usage                      Format           FW Rev\n---------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- --------\n/dev/nvme0n1     vol0cd37dab9e4a5c184 Amazon Elastic Block Store               1          68.72  GB /  68.72  GB    512   B +  0 B   1.0\n/dev/nvme1n1     vol0ad3629c159ee869c Amazon Elastic Block Store               1          25.77  GB /  25.77  GB    512   B +  0 B   1.0\n</code></pre> </li> <li> <p>From the same instance used in step 4, check that the containerd directories are using the second <code>/dev/nvme1n1</code> volume:</p> <pre><code>df /var/lib/containerd/\n</code></pre> <pre><code># Output should look like below, which shows the directory on the\n# /dev/nvme1n1 volume and NOT on /dev/nvme0n1 (root volume)\nFilesystem     1K-blocks    Used Available Use% Mounted on\n/dev/nvme1n1    24594768 2886716  20433380  13% /var/lib/containerd\n</code></pre> <pre><code>df /run/containerd/\n</code></pre> <pre><code># Output should look like below, which shows the directory on the\n# /dev/nvme1n1 volume and NOT on /dev/nvme0n1 (root volume)\nFilesystem     1K-blocks    Used Available Use% Mounted on\n/dev/nvme1n1    24594768 2886716  20433380  13% /run/containerd\n</code></pre> </li> <li> <p>Test by listing velero resources provisioned:</p> <pre><code>kubectl get all -n velero\n\n# Output should look similar to below\nNAME                         READY   STATUS    RESTARTS   AGE\npod/velero-b4d8fd5c7-5smp6   1/1     Running   0          112s\n\nNAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE\nservice/velero   ClusterIP   172.20.217.203   &lt;none&gt;        8085/TCP   114s\n\nNAME                     READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/velero   1/1     1            1           114s\n\nNAME                               DESIRED   CURRENT   READY   AGE\nreplicaset.apps/velero-b4d8fd5c7   1         1         1       114s\n</code></pre> </li> <li> <p>Get backup location using velero CLI</p> <pre><code>velero backup-location get\n\n# Output should look similar to below\nNAME      PROVIDER   BUCKET/PREFIX             PHASE       LAST VALIDATED                  ACCESS MODE   DEFAULT\ndefault   aws        velero-ss1wm44hv1fzb32d   Available   2022-05-22 10:53:26 -0400 EDT   ReadWrite     true\n</code></pre> </li> </ol>"},{"location":"patterns/stateful/#destroy","title":"Destroy","text":"<pre><code>terraform destroy -target=\"module.eks_blueprints_addons\" -auto-approve\nterraform destroy -target=\"module.eks\" -auto-approve\nterraform destroy -auto-approve\n</code></pre> <p>See here for more details on cleaning up the resources created.</p>"},{"location":"patterns/gitops/gitops-getting-started-argocd/","title":"ArgoCD - Getting Started","text":""},{"location":"patterns/gitops/gitops-getting-started-argocd/#argocd-on-amazon-eks","title":"ArgoCD on Amazon EKS","text":"<p>This tutorial guides you through deploying an Amazon EKS cluster with addons configured via ArgoCD, employing the GitOps Bridge Pattern.</p> <p></p> <p>The GitOps Bridge Pattern enables Kubernetes administrators to utilize Infrastructure as Code (IaC) and GitOps tools for deploying Kubernetes Addons and Workloads. Addons often depend on Cloud resources that are external to the cluster. The configuration metadata for these external resources is required by the Addons' Helm charts. While IaC is used to create these cloud resources, it is not used to install the Helm charts. Instead, the IaC tool stores this metadata either within GitOps resources in the cluster or in a Git repository. The GitOps tool then extracts these metadata values and passes them to the Helm chart during the Addon installation process. This mechanism forms the bridge between IaC and GitOps, hence the term \"GitOps Bridge.\"</p> <p>Additional examples available on the GitOps Bridge Pattern:</p> <ul> <li>argocd-ingress</li> <li>aws-secrets-manager</li> <li>crossplane</li> <li>external-secrets</li> <li>multi-cluster/distributed</li> <li>multi-cluster/hub-spoke</li> <li>multi-cluster/hub-spoke-shared</li> <li>private-git</li> </ul>"},{"location":"patterns/gitops/gitops-getting-started-argocd/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have the following command line tools installed:</p> <ul> <li>git</li> <li>terraform</li> <li>kubectl</li> <li>argocd</li> </ul>"},{"location":"patterns/gitops/gitops-getting-started-argocd/#optional-fork-the-gitops-git-repositories","title":"(Optional) Fork the GitOps git repositories","text":"<p>See the appendix section Fork GitOps Repositories for more info on the terraform variables to override.</p>"},{"location":"patterns/gitops/gitops-getting-started-argocd/#deploy-the-eks-cluster","title":"Deploy the EKS Cluster","text":"<p>Initialize Terraform and deploy the EKS cluster:</p> <pre><code>terraform init\nterraform apply -target=\"module.vpc\" -auto-approve\nterraform apply -target=\"module.eks\" -auto-approve\nterraform apply -auto-approve\n</code></pre> <p>To retrieve <code>kubectl</code> config, execute the terraform output command:</p> <pre><code>terraform output -raw configure_kubectl\n</code></pre> <p>The expected output will have two lines you run in your terminal</p> <pre><code>export KUBECONFIG=\"/tmp/getting-started-gitops\"\naws eks --region us-west-2 update-kubeconfig --name getting-started-gitops\n</code></pre> <p>The first line sets the <code>KUBECONFIG</code> environment variable to a temporary file that includes the cluster name. The second line uses the <code>aws</code> CLI to populate that temporary file with the <code>kubectl</code> configuration. This approach offers the advantage of not altering your existing <code>kubectl</code> context, allowing you to work in other terminal windows without interference.</p> <p>Terraform will add GitOps Bridge Metadata to the ArgoCD secret. The annotations contain metadata for the addons' Helm charts and ArgoCD ApplicationSets.</p> <pre><code>kubectl get secret -n argocd -l argocd.argoproj.io/secret-type=cluster -o json | jq '.items[0].metadata.annotations'\n</code></pre> <p>The output looks like the following:</p> <pre><code>{\n  \"addons_repo_basepath\": \"argocd/\",\n  \"addons_repo_path\": \"bootstrap/control-plane/addons\",\n  \"addons_repo_revision\": \"main\",\n  \"addons_repo_url\": \"https://github.com/aws-samples/eks-blueprints-add-ons\",\n  \"aws_account_id\": \"0123456789\",\n  \"aws_cluster_name\": \"getting-started-gitops\",\n  \"aws_load_balancer_controller_iam_role_arn\": \"arn:aws:iam::0123456789:role/alb-controller\",\n  \"aws_load_balancer_controller_namespace\": \"kube-system\",\n  \"aws_load_balancer_controller_service_account\": \"aws-load-balancer-controller-sa\",\n  \"aws_region\": \"us-west-2\",\n  \"aws_vpc_id\": \"vpc-001d3f00151bbb731\",\n  \"cluster_name\": \"in-cluster\",\n  \"environment\": \"dev\",\n  \"workload_repo_basepath\": \"patterns/gitops/\",\n  \"workload_repo_path\": \"getting-started-argocd/k8s\",\n  \"workload_repo_revision\": \"main\",\n  \"workload_repo_url\": \"https://github.com/csantanapr/terraform-aws-eks-blueprints\"\n}\n</code></pre> <p>The labels offer a straightforward way to enable or disable an addon in ArgoCD for the cluster.</p> <pre><code>kubectl get secret -n argocd -l argocd.argoproj.io/secret-type=cluster -o json | jq '.items[0].metadata.labels' | grep -v false | jq .\n</code></pre> <p>The output looks like the following:</p> <pre><code>{\n  \"argocd.argoproj.io/secret-type\": \"cluster\",\n  \"aws_cluster_name\": \"getting-started-gitops\",\n  \"cluster_name\": \"in-cluster\",\n  \"enable_argocd\": \"true\",\n  \"enable_aws_load_balancer_controller\": \"true\",\n  \"enable_metrics_server\": \"true\",\n  \"environment\": \"dev\",\n  \"kubernetes_version\": \"1.28\"\n}\n</code></pre>"},{"location":"patterns/gitops/gitops-getting-started-argocd/#deploy-the-addons","title":"Deploy the Addons","text":"<p>Bootstrap the addons using ArgoCD:</p> <pre><code>kubectl apply --server-side -f bootstrap/addons.yaml\n</code></pre>"},{"location":"patterns/gitops/gitops-getting-started-argocd/#monitor-gitops-progress-for-addons","title":"Monitor GitOps Progress for Addons","text":"<p>Wait until all the ArgoCD applications' <code>HEALTH STATUS</code> is <code>Healthy</code>. Use <code>Ctrl+C</code> or <code>Cmd+C</code> to exit the <code>watch</code> command. ArgoCD Applications can take a couple of minutes in order to achieve the Healthy status.</p> <pre><code>kubectl get applications -n argocd -w\n</code></pre> <p>The expected output should look like the following:</p> <pre><code>NAME                                            SYNC STATUS   HEALTH STATUS\naddon-in-cluster-argo-cd                        Synced        Healthy\naddon-in-cluster-aws-load-balancer-controller   Synced        Healthy\naddon-in-cluster-metrics-server                 Synced        Healthy\ncluster-addons                                  Synced        Healthy\n</code></pre>"},{"location":"patterns/gitops/gitops-getting-started-argocd/#verify-the-addons","title":"Verify the Addons","text":"<p>Verify that the addons are ready:</p> <pre><code>kubectl get deployment -n kube-system \\\n  aws-load-balancer-controller \\\n  metrics-server\nkubectl get deploy -n argocd \\\n  argo-cd-argocd-applicationset-controller \\\n  argo-cd-argocd-repo-server \\\n  argo-cd-argocd-server\n</code></pre> <p>The expected output should look like the following:</p> <pre><code>NAME                                       READY   UP-TO-DATE   AVAILABLE   AGE\naws-load-balancer-controller               2/2     2            2           7m21s\nmetrics-server                             1/1     1            1           7m41s\nargo-cd-argocd-applicationset-controller   1/1     1            1           109m\nargo-cd-argocd-repo-server                 1/1     1            1           109m\nargo-cd-argocd-server                      1/1     1            1           109m\n</code></pre>"},{"location":"patterns/gitops/gitops-getting-started-argocd/#optional-access-argocd","title":"(Optional) Access ArgoCD","text":"<p>Access to the ArgoCD's UI is completely optional, if you want to do it, run the commands shown in the Terraform output as the example below:</p> <pre><code>terraform output -raw access_argocd\n</code></pre> <p>The expected output should contain the <code>kubectl</code> config followed by <code>kubectl</code> command to retrieve the URL, username, password to login into ArgoCD UI or CLI.</p> <pre><code>echo \"ArgoCD Username: admin\"\necho \"ArgoCD Password: $(kubectl get secrets argocd-initial-admin-secret -n argocd --template=\"{{index .data.password | base64decode}}\")\"\necho \"ArgoCD URL: https://$(kubectl get svc -n argocd argo-cd-argocd-server -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')\"\n</code></pre>"},{"location":"patterns/gitops/gitops-getting-started-argocd/#deploy-the-workloads","title":"Deploy the Workloads","text":"<p>Deploy a sample application located in k8s/game-2048.yaml using ArgoCD:</p> <pre><code>kubectl apply --server-side -f bootstrap/workloads.yaml\n</code></pre>"},{"location":"patterns/gitops/gitops-getting-started-argocd/#monitor-gitops-progress-for-workloads","title":"Monitor GitOps Progress for Workloads","text":"<p>Wait until all the ArgoCD applications' <code>HEALTH STATUS</code> is <code>Healthy</code>. Use <code>Ctrl+C</code> or <code>Cmd+C</code> to exit the <code>watch</code> command. ArgoCD Applications can take a couple of minutes in order to achieve the Healthy status.</p> <pre><code>watch kubectl get -n argocd applications workloads\n</code></pre> <p>The expected output should look like the following:</p> <pre><code>NAME        SYNC STATUS   HEALTH STATUS\nworkloads   Synced        Healthy\n</code></pre>"},{"location":"patterns/gitops/gitops-getting-started-argocd/#verify-the-application","title":"Verify the Application","text":"<p>Verify that the application configuration is present and the pod is running:</p> <pre><code>kubectl get -n game-2048 deployments,service,ep,ingress\n</code></pre> <p>The expected output should look like the following:</p> <pre><code>NAME                        READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/game-2048   1/1     1            1           7h59m\n\nNAME                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE\nservice/game-2048   ClusterIP   172.20.155.47   &lt;none&gt;        80/TCP    7h59m\n\nNAME                  ENDPOINTS       AGE\nendpoints/game-2048   10.0.13.64:80   7h59m\n\nNAME                CLASS   HOSTS   ADDRESS                              PORTS   AGE\ningress/game-2048   alb     *       k8s-&lt;&gt;.us-west-2.elb.amazonaws.com   80      7h59m\n</code></pre> <p>AWS Load Balancer can take a couple of minutes in order to be created.</p> <p>Run the following command and wait until and event for ingress <code>game-2048</code> contains <code>Successfully reconciled</code>. Use <code>Ctrl+C</code> or <code>Cmd+C</code>to exit the <code>watch</code> command.</p> <pre><code>kubectl events -n game-2048 --for ingress/game-2048 --watch\n</code></pre> <p>The expected output should look like the following:</p> <pre><code>LAST SEEN   TYPE     REASON                   OBJECT              MESSAGE\n11m         Normal   SuccessfullyReconciled   Ingress/game-2048   Successfully reconciled\n</code></pre>"},{"location":"patterns/gitops/gitops-getting-started-argocd/#access-the-application-using-aws-load-balancer","title":"Access the Application using AWS Load Balancer","text":"<p>Verify the application endpoint health using <code>wget</code>:</p> <pre><code>kubectl exec -n game-2048 deploy/game-2048 -- \\\nwget -S --spider $(kubectl get -n game-2048 ingress game-2048 -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')\n</code></pre> <p>The expected output should look like the following:</p> <pre><code>  HTTP/1.1 200 OK\n  Date: Wed, 01 Nov 2023 22:44:57 GMT\n  Content-Type: text/html\n  Content-Length: 3988\n</code></pre> <p>A success response should contain <code>HTTP/1.1 200 OK</code>.</p> <p>Retrieve the ingress URL to access the application in your local web browser.</p> <pre><code>echo \"Application URL: http://$(kubectl get -n game-2048 ingress game-2048 -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')\"\n</code></pre>"},{"location":"patterns/gitops/gitops-getting-started-argocd/#container-metrics","title":"Container Metrics","text":"<p>Check the application's CPU and memory metrics:</p> <pre><code>kubectl top pods -n game-2048\n</code></pre> <p>The expected output should look like the following:</p> <pre><code>NAME                         CPU(cores)   MEMORY(bytes)\ngame-2048-66fb78b995-h1bjv   1m           2Mi\n</code></pre> <p>Check the CPU and memory metrics for all pods for Addons and Workloads:</p> <pre><code>kubectl top pods -A\n</code></pre> <p>The expected output should look like the following:</p> <pre><code>NAMESPACE     NAME                                                        CPU(cores)   MEMORY(bytes)\nargocd        argo-cd-argocd-application-controller-0                     43m          138Mi\nargocd        argo-cd-argocd-applicationset-controller-5db688844c-79skp   1m           25Mi\nargocd        argo-cd-argocd-dex-server-cd48d7bc-x7flf                    1m           16Mi\nargocd        argo-cd-argocd-notifications-controller-7d7ccc6b9d-dg9r6    1m           17Mi\nargocd        argo-cd-argocd-redis-7f89c69877-6m2cj                       2m           3Mi\nargocd        argo-cd-argocd-repo-server-644b9b5668-m9ddg                 8m           62Mi\nargocd        argo-cd-argocd-server-57cbbd6f94-lp4wx                      2m           26Mi\ngame-2048     game-2048-66fb78b995-h1bjv                                  1m           2Mi\nkube-system   aws-load-balancer-controller-8488df87c-4nxv6                2m           26Mi\nkube-system   aws-load-balancer-controller-8488df87c-zs4p6                1m           19Mi\nkube-system   aws-node-ck6vq                                              3m           57Mi\nkube-system   aws-node-fv2sg                                              3m           56Mi\nkube-system   coredns-59754897cf-5r2xp                                    1m           13Mi\nkube-system   coredns-59754897cf-fn7jb                                    1m           13Mi\nkube-system   kube-proxy-lz2dc                                            1m           11Mi\nkube-system   kube-proxy-pd2lm                                            1m           12Mi\nkube-system   metrics-server-5b76987ff-5g1sv                              4m           17Mi\n</code></pre>"},{"location":"patterns/gitops/gitops-getting-started-argocd/#destroy-the-eks-cluster","title":"Destroy the EKS Cluster","text":"<p>To tear down all the resources and the EKS cluster, run the following command:</p> <pre><code>./destroy.sh\n</code></pre>"},{"location":"patterns/gitops/gitops-getting-started-argocd/#appendix","title":"Appendix","text":""},{"location":"patterns/gitops/gitops-getting-started-argocd/#fork-gitops-repositories","title":"Fork GitOps Repositories","text":"<p>To modify the <code>values.yaml</code> file for addons or the workload manifest files (.ie yaml), you'll need to fork two repositories: aws-samples/eks-blueprints-add-ons for addons and github.com/aws-ia/terraform-aws-eks-blueprints for workloads located in this pattern directory.</p> <p>After forking, update the following environment variables to point to your forks, replacing the default values.</p> <pre><code>export TF_VAR_gitops_addons_org=https://github.com/aws-samples\nexport TF_VAR_gitops_addons_repo=eks-blueprints-add-ons\nexport TF_VAR_gitops_addons_revision=main\n\nexport TF_VAR_gitops_workload_org=https://github.com/aws-ia\nexport TF_VAR_gitops_workload_repo=terraform-aws-eks-blueprints\nexport TF_VAR_gitops_workload_revision=main\n</code></pre>"},{"location":"patterns/gitops/gitops-multi-cluster-hub-spoke-argocd/","title":"ArgoCD - Multi-Cluster Hub & Spoke","text":""},{"location":"patterns/gitops/gitops-multi-cluster-hub-spoke-argocd/#multi-cluster-centralized-hub-spoke-topology","title":"Multi-Cluster centralized hub-spoke topology","text":"<p>This tutorial guides you through deploying an Amazon EKS cluster with addons configured via ArgoCD in a Multi-Cluster Hub-Spoke topology, employing the GitOps Bridge Pattern.</p> <p></p> <p>This example deploys ArgoCD on the Hub cluster (i.e. management/control-plane cluster). The spoke clusters are registered as remote clusters in the Hub Cluster's ArgoCD The ArgoCD on the Hub Cluster deploys addons and workloads to the spoke clusters</p> <p>Each spoke cluster gets deployed an app of apps ArgoCD Application with the name <code>workloads-${env}</code></p>"},{"location":"patterns/gitops/gitops-multi-cluster-hub-spoke-argocd/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have the following command line tools installed:</p> <ul> <li>git</li> <li>terraform</li> <li>kubectl</li> <li>argocd</li> </ul>"},{"location":"patterns/gitops/gitops-multi-cluster-hub-spoke-argocd/#optional-fork-the-gitops-git-repositories","title":"(Optional) Fork the GitOps git repositories","text":"<p>See the appendix section Fork GitOps Repositories for more info on the terraform variables to override.</p>"},{"location":"patterns/gitops/gitops-multi-cluster-hub-spoke-argocd/#deploy-the-hub-eks-cluster","title":"Deploy the Hub EKS Cluster","text":"<p>Change directory to <code>hub</code></p> <pre><code>cd hub\n</code></pre> <p>Initialize Terraform and deploy the EKS cluster:</p> <pre><code>terraform init\nterraform apply -target=\"module.vpc\" -auto-approve\nterraform apply -target=\"module.eks\" -auto-approve\nterraform apply -auto-approve\n</code></pre> <p>To retrieve <code>kubectl</code> config, execute the terraform output command:</p> <pre><code>terraform output -raw configure_kubectl\n</code></pre> <p>The expected output will have two lines you run in your terminal</p> <pre><code>export KUBECONFIG=\"/tmp/hub-spoke\"\naws eks --region us-west-2 update-kubeconfig --name getting-started-gitops --alias hub\n</code></pre> <p>The first line sets the <code>KUBECONFIG</code> environment variable to a temporary file that includes the cluster name. The second line uses the <code>aws</code> CLI to populate that temporary file with the <code>kubectl</code> configuration. This approach offers the advantage of not altering your existing <code>kubectl</code> context, allowing you to work in other terminal windows without interference.</p>"},{"location":"patterns/gitops/gitops-multi-cluster-hub-spoke-argocd/#deploy-argocd-apps-of-applicationsets-for-addons","title":"Deploy ArgoCD Apps of ApplicationSets for Addons","text":"<p>This command verifies the initial ArgoCD installation, ArgoCD will be re-configured when the addons are deployed and configured from git. <pre><code>kubectl --context hub get all -n argocd\n</code></pre> This command creates the application set manifest to deploy the addons. <pre><code>kubectl --context hub apply -n argocd -f ../hub/bootstrap/addons.yaml\n</code></pre> The application sets defined here will then deploy addons to any spoke clusters provisioned later using Terraform</p>"},{"location":"patterns/gitops/gitops-multi-cluster-hub-spoke-argocd/#monitor-gitops-progress-for-addons-on-hub-eks-cluster","title":"Monitor GitOps Progress for Addons on Hub EKS Cluster","text":"<p>Wait until all the ArgoCD applications' <code>HEALTH STATUS</code> is <code>Healthy</code>. Use <code>Ctrl+C</code> or <code>Cmd+C</code> to exit the <code>watch</code> command. ArgoCD Applications can take a couple of minutes in order to achieve the Healthy status.</p> <pre><code>kubectl --context hub get applications -n argocd -w\n</code></pre> <p>The expected output should look like the following:</p> <pre><code>NAME                                            SYNC STATUS   HEALTH STATUS\naddon-in-cluster-argo-cd                        Synced        Healthy\naddon-in-cluster-aws-load-balancer-controller   Synced        Healthy\naddon-in-cluster-metrics-server                 Synced        Healthy\ncluster-addons                                  Synced        Healthy\n</code></pre>"},{"location":"patterns/gitops/gitops-multi-cluster-hub-spoke-argocd/#optional-access-argocd","title":"(Optional) Access ArgoCD","text":"<p>Access to the ArgoCD's UI is completely optional, if you want to do it, run the commands shown in the Terraform output as the example below:</p> <pre><code>terraform output -raw access_argocd\n</code></pre> <p>The expected output should contain the <code>kubectl</code> config followed by <code>kubectl</code> command to retrieve the URL, username, password to login into ArgoCD UI or CLI.</p> <pre><code>echo \"ArgoCD Username: admin\"\necho \"ArgoCD Password: $(kubectl --context hub get secrets argocd-initial-admin-secret -n argocd --template=\"{{index .data.password | base64decode}}\")\"\necho \"ArgoCD URL: https://$(kubectl --context hub get svc -n argocd argo-cd-argocd-server -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')\"\n</code></pre>"},{"location":"patterns/gitops/gitops-multi-cluster-hub-spoke-argocd/#deploy-the-spoke-eks-cluster","title":"Deploy the Spoke EKS Cluster","text":"<p>Use the <code>deploy.sh</code> script to create terraform workspace, initialize Terraform, and deploy the EKS clusters:</p> <p>You may want to create few spoke environments to validate multi-cluster hub spoke to avoid quota limits</p> <pre><code>cd ../spokes\n./deploy.sh dev\n./deploy.sh staging\n./deploy.sh prod\n</code></pre> <p>Each environment uses a Terraform workspace</p> <p>To retrieve <code>kubectl</code> config, execute the terraform output command:</p> <pre><code>terraform workspace select dev\nterraform output -raw configure_kubectl\n</code></pre> <pre><code>terraform workspace select staging\nterraform output -raw configure_kubectl\n</code></pre> <pre><code>terraform workspace select prod\nterraform output -raw configure_kubectl\n</code></pre>"},{"location":"patterns/gitops/gitops-multi-cluster-hub-spoke-argocd/#verify-argocd-cluster-secret-for-spokes-have-the-correct-iam-role-to-be-assume-by-hub-cluster","title":"Verify ArgoCD Cluster Secret for Spokes have the correct IAM Role to be assume by Hub Cluster","text":"<pre><code>for i in dev staging prod ; do echo $i &amp;&amp; kubectl --context hub get secret -n argocd spoke-$i --template='{{index .data.config | base64decode}}' ; done\n</code></pre> <p>The output have a section <code>awsAuthConfig</code> with the <code>clusterName</code> and the <code>roleARN</code> that has write access to the spoke cluster</p> <pre><code>{\n  \"tlsClientConfig\": {\n    \"insecure\": false,\n    \"caData\" : \"LS0tL....\"\n  },\n  \"awsAuthConfig\" : {\n    \"clusterName\": \"hub-spoke-dev\",\n    \"roleARN\": \"arn:aws:iam::0123456789:role/hub-spoke-dev-argocd-spoke\"\n  }\n}\n</code></pre>"},{"location":"patterns/gitops/gitops-multi-cluster-hub-spoke-argocd/#verify-the-addons-on-spoke-clusters","title":"Verify the Addons on Spoke Clusters","text":"<p>The addons on the spoke clusters are deployed using the Application Sets created on the EKS Hub Cluster. Verify that the addons are ready:</p> <pre><code>for i in dev staging prod ; do echo $i &amp;&amp; kubectl --context $i get deployment -n kube-system ; done\n</code></pre>"},{"location":"patterns/gitops/gitops-multi-cluster-hub-spoke-argocd/#deploy-the-sample-application-to-eks-spoke-clusters","title":"Deploy the sample application to EKS Spoke Clusters","text":"<p>This command will deploy the application using kubectl to all clusters connected to the hub cluster, using the manifest files in ./hub/bootstrap/workloads.yaml. <pre><code>kubectl --context hub apply -n argocd -f ../hub/bootstrap/workloads.yaml\n</code></pre></p>"},{"location":"patterns/gitops/gitops-multi-cluster-hub-spoke-argocd/#monitor-gitops-progress-for-workloads-from-hub-cluster-run-on-hub-cluster-context","title":"Monitor GitOps Progress for Workloads from Hub Cluster (run on Hub Cluster context)","text":"<p>Watch until all the Workloads ArgoCD Applications are <code>Healthy</code></p> <pre><code>kubectl --context hub get -n argocd applications -w\n</code></pre> <p>Wait until the ArgoCD Applications <code>HEALTH STATUS</code> is <code>Healthy</code>. Crl+C to exit the <code>watch</code> command</p>"},{"location":"patterns/gitops/gitops-multi-cluster-hub-spoke-argocd/#verify-the-application","title":"Verify the Application","text":"<p>Verify that the application configuration is present and the pod is running:</p> <pre><code>for i in dev staging prod ; do echo $i &amp;&amp; kubectl --context $i get all -n workload ; done\n</code></pre>"},{"location":"patterns/gitops/gitops-multi-cluster-hub-spoke-argocd/#container-metrics","title":"Container Metrics","text":"<p>Check the application's CPU and memory metrics:</p> <pre><code>for i in dev staging prod ; do echo $i &amp;&amp; kubectl --context $i top pods -n workload ; done\n</code></pre>"},{"location":"patterns/gitops/gitops-multi-cluster-hub-spoke-argocd/#destroy-the-spoke-eks-clusters","title":"Destroy the Spoke EKS Clusters","text":"<p>To tear down all the resources and the EKS cluster, run the following command:</p> <pre><code>./destroy.sh dev\n./destroy.sh staging\n./destroy.sh prod\n</code></pre>"},{"location":"patterns/gitops/gitops-multi-cluster-hub-spoke-argocd/#destroy-the-hub-eks-clusters","title":"Destroy the Hub EKS Clusters","text":"<p>To tear down all the resources and the EKS cluster, run the following command: Destroy Hub Clusters</p> <pre><code>cd ../hub\n./destroy.sh\n</code></pre>"},{"location":"patterns/gitops/gitops-multi-cluster-hub-spoke-argocd/#appendix","title":"Appendix","text":""},{"location":"patterns/gitops/gitops-multi-cluster-hub-spoke-argocd/#fork-gitops-repositories","title":"Fork GitOps Repositories","text":"<p>To modify the <code>values.yaml</code> file or the helm chart version for addons, you'll need to fork the repository aws-samples/eks-blueprints-add-ons.</p> <p>After forking, update the following environment variables to point to your forks, replacing the default values.</p> <pre><code>export TF_VAR_gitops_addons_org=https://github.com/aws-samples\nexport TF_VAR_gitops_addons_repo=eks-blueprints-add-ons\nexport TF_VAR_gitops_addons_revision=main\n</code></pre>"},{"location":"patterns/machine-learning/aws-neuron-efa/","title":"AWS Neuron with EFA","text":""},{"location":"patterns/machine-learning/aws-neuron-efa/#eks-cluster-w-aws-neuron-devices-and-efa-for-machine-learning","title":"EKS Cluster w/ AWS Neuron Devices and EFA for Machine Learning","text":"<p>This pattern demonstrates an Amazon EKS Cluster with an EFA-enabled nodegroup that utilizes <code>trn1.32xlarge</code> instances that are used in distributed, multi-node machine learning workloads.</p> <p>The following components are demonstrated in this pattern:</p> <ul> <li>A \"default\" node group that supports addons and components that do not require AWS Neuron nor EFA devices. Any pods that do not tolerate the taints of the Neuron node group will be scheduled on instances within this node group.</li> <li>A node group of <code>trn1.32xlarge</code> instances with:<ul> <li>all x8 EFA network interfaces enabled</li> <li>provisioned within a placement group so that the instances are co-located close to one another in a single availability zone that supports the instance type</li> <li>a common taint of <code>\"aws.amazon.com/neuron:NoSchedule\"</code> to ensure only the intended applications are permitted to run on the nodes created</li> <li>two labels identifying that this nodegroup supports AWS Neuron and EFA devices; allowing pods to use node selectors with these labels</li> <li>the NVME instance store volumes are mounted in a RAID-0 array to provide a single, large, high-performance storage volume for the Neuron workloads</li> <li>kubelet and containerd are configured to utilize the RAID-0 volume, allowing kubelet to discover the additional storage as ephemeral storage that can be utilized by pods</li> </ul> </li> <li>A Helm chart deployment for the Neuron device plugin to expose and mount the Neuron devices provided by the instances to the pods that request them</li> <li>A Helm chart deployment for the EFA device plugin to expose and mount the EFA network interfaces provided by the instances to the pods that request them. Since the EFA network interfaces are only found on the instances that provide AWS Neuron devices in this pattern, we do not apply an additional taint for the EFA network interfaces to avoid over-constraining.</li> </ul>"},{"location":"patterns/machine-learning/aws-neuron-efa/#code","title":"Code","text":""},{"location":"patterns/machine-learning/aws-neuron-efa/#cluster","title":"Cluster","text":"<pre><code>################################################################################\n# Cluster\n################################################################################\n\nmodule \"eks\" {\n  source  = \"terraform-aws-modules/eks/aws\"\n  version = \"~&gt; 20.26\"\n\n  cluster_name    = local.name\n  cluster_version = \"1.31\"\n\n  # Give the Terraform identity admin access to the cluster\n  # which will allow it to deploy resources into the cluster\n  enable_cluster_creator_admin_permissions = true\n  cluster_endpoint_public_access           = true\n\n  cluster_addons = {\n    coredns                = {}\n    eks-pod-identity-agent = {}\n    kube-proxy             = {}\n    vpc-cni = {\n      most_recent = true\n    }\n  }\n\n  # Add security group rules on the node group security group to\n  # allow EFA traffic\n  enable_efa_support = true\n\n  vpc_id     = module.vpc.vpc_id\n  subnet_ids = module.vpc.private_subnets\n\n  eks_managed_node_groups = {\n    neuron-efa = {\n      # The EKS AL2023 Neuron AMI provides all of the necessary components\n      # for accelerated workloads w/ EFA\n      ami_type       = \"AL2023_x86_64_NEURON\"\n      instance_types = [\"trn1.32xlarge\"]\n\n      # Mount instance store volumes in RAID-0 for kubelet and containerd\n      # https://github.com/awslabs/amazon-eks-ami/blob/master/doc/USER_GUIDE.md#raid-0-for-kubelet-and-containerd-raid0\n      cloudinit_pre_nodeadm = [\n        {\n          content_type = \"application/node.eks.aws\"\n          content      = &lt;&lt;-EOT\n            ---\n            apiVersion: node.eks.aws/v1alpha1\n            kind: NodeConfig\n            spec:\n              instance:\n                localStorage:\n                  strategy: RAID0\n          EOT\n        }\n      ]\n\n      min_size     = 2\n      max_size     = 2\n      desired_size = 2\n\n      # This will:\n      # 1. Create a placement group to place the instances close to one another\n      # 2. Ignore subnets that reside in AZs that do not support the instance type\n      # 3. Expose all of the available EFA interfaces on the launch template\n      enable_efa_support = true\n\n      labels = {\n        \"vpc.amazonaws.com/efa.present\" = \"true\"\n        \"aws.amazon.com/neuron.present\" = \"true\"\n      }\n\n      taints = {\n        # Ensure only Neuron workloads are scheduled on this node group\n        gpu = {\n          key    = \"aws.amazon.com/neuron\"\n          value  = \"true\"\n          effect = \"NO_SCHEDULE\"\n        }\n      }\n    }\n\n    # This node group is for core addons such as CoreDNS\n    default = {\n      instance_types = [\"m5.large\"]\n\n      min_size     = 1\n      max_size     = 2\n      desired_size = 2\n    }\n  }\n\n  tags = local.tags\n}\n</code></pre>"},{"location":"patterns/machine-learning/aws-neuron-efa/#device-plugins","title":"Device Plugins","text":"<pre><code>data \"aws_ecrpublic_authorization_token\" \"token\" {\n  provider = aws.ecr\n}\n\n################################################################################\n# Helm charts\n################################################################################\n\nresource \"helm_release\" \"neuron\" {\n  name             = \"neuron\"\n  repository       = \"oci://public.ecr.aws/neuron\"\n  chart            = \"neuron-helm-chart\"\n  version          = \"1.0.0\"\n  namespace        = \"neuron\"\n  create_namespace = true\n  wait             = false\n\n  # Public ECR\n  repository_username = data.aws_ecrpublic_authorization_token.token.user_name\n  repository_password = data.aws_ecrpublic_authorization_token.token.password\n\n  values = [\n    &lt;&lt;-EOT\n      nodeSelector:\n        aws.amazon.com/neuron.present: 'true'\n      npd:\n        enabled: false\n    EOT\n  ]\n}\n\nresource \"helm_release\" \"aws_efa_device_plugin\" {\n  name       = \"aws-efa-k8s-device-plugin\"\n  repository = \"https://aws.github.io/eks-charts\"\n  chart      = \"aws-efa-k8s-device-plugin\"\n  version    = \"v0.5.5\"\n  namespace  = \"kube-system\"\n  wait       = false\n\n  values = [\n    &lt;&lt;-EOT\n      nodeSelector:\n        vpc.amazonaws.com/efa.present: 'true'\n      tolerations:\n        - key: aws.amazon.com/neuron\n          operator: Exists\n          effect: NoSchedule\n    EOT\n  ]\n}\n</code></pre>"},{"location":"patterns/machine-learning/aws-neuron-efa/#deploy","title":"Deploy","text":"<p>See here for the prerequisites and steps to deploy this pattern.</p>"},{"location":"patterns/machine-learning/aws-neuron-efa/#validate","title":"Validate","text":"<ol> <li> <p>List the nodes and their instance type:</p> <pre><code>kubectl get nodes -L node.kubernetes.io/instance-type\n</code></pre> <pre><code>NAME                                        STATUS   ROLES    AGE   VERSION               INSTANCE-TYPE\nip-10-0-12-200.us-east-2.compute.internal   Ready    &lt;none&gt;   82m   v1.31.0-eks-a737599   m5.large\nip-10-0-24-248.us-east-2.compute.internal   Ready    &lt;none&gt;   82m   v1.31.0-eks-a737599   m5.large\nip-10-0-39-213.us-east-2.compute.internal   Ready    &lt;none&gt;   75m   v1.31.0-eks-a737599   trn1.32xlarge\nip-10-0-43-172.us-east-2.compute.internal   Ready    &lt;none&gt;   75m   v1.31.0-eks-a737599   trn1.32xlarge\n</code></pre> <p>You should see two EFA-enabled (in this example <code>trn1.32xlarge</code>) nodes in the list.</p> </li> </ol>"},{"location":"patterns/machine-learning/aws-neuron-efa/#destroy","title":"Destroy","text":"<pre><code>terraform destroy -target=\"module.eks_blueprints_addons\" -auto-approve\nterraform destroy -target=\"module.eks\" -auto-approve\nterraform destroy -auto-approve\n</code></pre> <p>See here for more details on cleaning up the resources created.</p>"},{"location":"patterns/machine-learning/ml-capacity-block/","title":"ML Capacity Block Reservation (CBR)","text":""},{"location":"patterns/machine-learning/ml-capacity-block/#eks-w-ml-capacity-block-reservation-cbr","title":"EKS w/ ML Capacity Block Reservation (CBR)","text":"<p>This pattern demonstrates how to consume/utilize ML capacity block reservations (CBR) with Amazon EKS. The solution is comprised of primarily of the following components:</p> <ol> <li>The node group, either EKS managed or self-managed, that will utilize the CBR should have the subnets provided to it restricted to the availability zone where the CBR has been allocated. For example - if the CBR is allocated to <code>us-west-2b</code>, the node group should only have subnet IDs provided to it that reside in <code>us-west-2b</code>. If the subnets that reside in other AZs are provided, its possible to encounter an error such as <code>InvalidParameterException: The following supplied instance types do not exist ...</code>. It is not guaranteed that this error will always be shown, and may appear random since the underlying autoscaling group(s) will provision nodes into different AZs at random. It will only occur when the underlying autoscaling group tries to provision instances into an AZ where capacity is not allocated and there is insufficient on-demand capacity for the desired instance type.</li> <li>The launch template utilized should specify the <code>instance_market_options</code> and <code>capacity_reservation_specification</code> arguments. This is how the CBR is utilized by the node group (i.e. - tells the autoscaling group to launch instances utilizing provided capacity reservation).</li> <li>In the case of EKS managed node group(s), the <code>capacity_type</code> should be set to <code>\"CAPACITY_BLOCK\"</code>.</li> </ol> <p>Links:</p> <ul> <li>EKS - Capacity Blocks for ML</li> <li>EC2 - Capacity Blocks for ML</li> </ul>"},{"location":"patterns/machine-learning/ml-capacity-block/#code","title":"Code","text":"<pre><code>################################################################################\n# Required Input\n################################################################################\n\n# See https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/capacity-blocks-using.html\n# on how to obtain a ML capacity block reservation. Once acquired, you can provide\n# the reservation ID through this input to deploy the pattern\nvariable \"capacity_reservation_id\" {\n  description = \"The ID of the ML capacity block reservation for the node group\"\n  type        = string\n}\n\n################################################################################\n# Cluster\n################################################################################\n\nmodule \"eks\" {\n  source  = \"terraform-aws-modules/eks/aws\"\n  version = \"~&gt; 20.26\"\n\n  cluster_name    = local.name\n  cluster_version = \"1.31\"\n\n  # Give the Terraform identity admin access to the cluster\n  # which will allow it to deploy resources into the cluster\n  enable_cluster_creator_admin_permissions = true\n  cluster_endpoint_public_access           = true\n\n  cluster_addons = {\n    coredns                = {}\n    eks-pod-identity-agent = {}\n    kube-proxy             = {}\n    vpc-cni = {\n      most_recent = true\n    }\n  }\n\n  # Add security group rules on the node group security group to\n  # allow EFA traffic\n  enable_efa_support = true\n\n  vpc_id     = module.vpc.vpc_id\n  subnet_ids = module.vpc.private_subnets\n\n  eks_managed_node_groups = {\n    cbr = {\n      # The EKS AL2023 NVIDIA AMI provides all of the necessary components\n      # for accelerated workloads w/ EFA\n      ami_type       = \"AL2023_x86_64_NVIDIA\"\n      instance_types = [\"p5e.48xlarge\"]\n\n      # Mount instance store volumes in RAID-0 for kubelet and containerd\n      # https://github.com/awslabs/amazon-eks-ami/blob/master/doc/USER_GUIDE.md#raid-0-for-kubelet-and-containerd-raid0\n      cloudinit_pre_nodeadm = [\n        {\n          content_type = \"application/node.eks.aws\"\n          content      = &lt;&lt;-EOT\n            ---\n            apiVersion: node.eks.aws/v1alpha1\n            kind: NodeConfig\n            spec:\n              instance:\n                localStorage:\n                  strategy: RAID0\n          EOT\n        }\n      ]\n\n      min_size     = 2\n      max_size     = 2\n      desired_size = 2\n\n      # This will:\n      # 1. Create a placement group to place the instances close to one another\n      # 2. Ignore subnets that reside in AZs that do not support the instance type\n      # 3. Expose all of the available EFA interfaces on the launch template\n      enable_efa_support = true\n\n      labels = {\n        \"vpc.amazonaws.com/efa.present\" = \"true\"\n        \"nvidia.com/gpu.present\"        = \"true\"\n      }\n\n      taints = {\n        # Ensure only GPU workloads are scheduled on this node group\n        gpu = {\n          key    = \"nvidia.com/gpu\"\n          value  = \"true\"\n          effect = \"NO_SCHEDULE\"\n        }\n      }\n\n      # First subnet is in the \"${local.region}a\" availability zone\n      # where the capacity reservation is created\n      # TODO - Update the subnet to match the availability zone of *YOUR capacity reservation\n      subnet_ids = [element(module.vpc.private_subnets, 0)]\n\n      # ML capacity block reservation\n      capacity_type = \"CAPACITY_BLOCK\"\n      instance_market_options = {\n        market_type = \"capacity-block\"\n      }\n      capacity_reservation_specification = {\n        capacity_reservation_target = {\n          capacity_reservation_id = var.capacity_reservation_id\n        }\n      }\n    }\n    # This node group is for core addons such as CoreDNS\n    default = {\n      instance_types = [\"m5.large\"]\n\n      min_size     = 2\n      max_size     = 2\n      desired_size = 2\n    }\n  }\n\n  # Self-managed node group equivalent for ML capacity block reservation\n  # This is not required for ML CBR support with EKS managed node groups,\n  # its just showing use with both node group types. Users should select\n  # the one that works for their use case.\n  self_managed_node_groups = {\n    cbr2 = {\n      # The EKS AL2023 NVIDIA AMI provides all of the necessary components\n      # for accelerated workloads w/ EFA\n      ami_type      = \"AL2023_x86_64_NVIDIA\"\n      instance_type = \"p5e.48xlarge\"\n\n      # Mount instance store volumes in RAID-0 for kubelet and containerd\n      # https://github.com/awslabs/amazon-eks-ami/blob/master/doc/USER_GUIDE.md#raid-0-for-kubelet-and-containerd-raid0\n      cloudinit_pre_nodeadm = [\n        {\n          content_type = \"application/node.eks.aws\"\n          content      = &lt;&lt;-EOT\n            ---\n            apiVersion: node.eks.aws/v1alpha1\n            kind: NodeConfig\n            spec:\n              instance:\n                localStorage:\n                  strategy: RAID0\n              kubelet:\n                flags:\n                  - --node-labels=vpc.amazonaws.com/efa.present=true,nvidia.com/gpu.present=true\n                  - --register-with-taints=nvidia.com/gpu=true:NoSchedule\n          EOT\n        }\n      ]\n\n      min_size     = 2\n      max_size     = 2\n      desired_size = 2\n\n      # This will:\n      # 1. Create a placement group to place the instances close to one another\n      # 2. Ignore subnets that reside in AZs that do not support the instance type\n      # 3. Expose all of the available EFA interfaces on the launch template\n      enable_efa_support = true\n\n      # First subnet is in the \"${local.region}a\" availability zone\n      # where the capacity reservation is created\n      # TODO - Update the subnet to match the availability zone of *YOUR capacity reservation\n      subnet_ids = [element(module.vpc.private_subnets, 0)]\n\n      # ML capacity block reservation\n      instance_market_options = {\n        market_type = \"capacity-block\"\n      }\n      capacity_reservation_specification = {\n        capacity_reservation_target = {\n          capacity_reservation_id = var.capacity_reservation_id\n        }\n      }\n    }\n  }\n\n  tags = local.tags\n}\n</code></pre>"},{"location":"patterns/machine-learning/ml-capacity-block/#deploy","title":"Deploy","text":"<p>See here for the prerequisites and steps to deploy this pattern.</p>"},{"location":"patterns/machine-learning/ml-capacity-block/#destroy","title":"Destroy","text":"<pre><code>terraform destroy -target=\"module.eks_blueprints_addons\" -auto-approve\nterraform destroy -target=\"module.eks\" -auto-approve\nterraform destroy -auto-approve\n</code></pre> <p>See here for more details on cleaning up the resources created.</p>"},{"location":"patterns/machine-learning/ml-container-cache/","title":"ML Container Cache","text":""},{"location":"patterns/machine-learning/ml-container-cache/#eks-cluster-w-cached-ml-images","title":"EKS Cluster w/ Cached ML Images","text":"<p>This pattern demonstrates how to cache images on an EBS volume snapshot that will be used by nodes in an EKS cluster. The solution is comprised of primarily of the following components:</p> <ol> <li>An AWS Step Function implementation has been provided that demonstrates an example process for creating EBS volume snapshots that are pre-populated with the select container images. As part of this process, EBS Fast Snapshot Restore is enabled by default for the snapshots created to avoid the EBS volume  initialization time penalty. The Step Function state machine diagram is captured below for reference.</li> <li>The node group demonstrates how to mount the generated EBS volume snapshot at the <code>/var/lib/containerd</code> location in order for containerd to utilize the pre-populated images. The snapshot ID is referenced via an SSM parameter data source which was populated by the Step Function cache builder; any new images created by the cache builder will automatically update the SSM parameter used by the node group.</li> </ol> <p>The main benefit of caching, or pre-pulling, container images onto an EBS volume snapshot is faster time to start pods/containers on new nodes, especially for larger (multi-gigabyte) images that are common with machine-learning workloads. This process avoids the time and resources it takes to pull and un-pack container images from remote registries. Instead, those images are already present in the location that containerd expects, allowing for faster pod startup times.</p>"},{"location":"patterns/machine-learning/ml-container-cache/#cache-builder-state-machine","title":"Cache Builder State Machine","text":""},{"location":"patterns/machine-learning/ml-container-cache/#results","title":"Results","text":"<p>The following results use the PyTorch nvcr.io/nvidia/pytorch:24.08-py3 image which is 9.5 GB compressed and 20.4 GB decompressed on disk.</p> <p>Pod start up time duration is captured via pod events using ktime.</p>"},{"location":"patterns/machine-learning/ml-container-cache/#cached","title":"Cached","text":"<p>With the PyTorch image already present on the EBS volume, the pod starts up in less than 5 seconds:</p> <p> </p>"},{"location":"patterns/machine-learning/ml-container-cache/#uncached","title":"Uncached","text":"<p>When the PyTorch image is not present on the EBS volume, it takes roughly 6 minutes (334 seconds in the capture below) for the image to be pulled, unpacked, and the pod to start.</p> <p> </p>"},{"location":"patterns/machine-learning/ml-container-cache/#code","title":"Code","text":""},{"location":"patterns/machine-learning/ml-container-cache/#cache-builder","title":"Cache Builder","text":"<pre><code>module \"ebs_snapshot_builder\" {\n  source  = \"clowdhaus/ebs-snapshot-builder/aws\"\n  version = \"~&gt; 1.1\"\n\n  name = local.name\n\n  # Images to cache\n  public_images = [\n    \"nvcr.io/nvidia/k8s-device-plugin:v0.16.2\", # 120 MB compressed / 351 MB decompressed\n    \"nvcr.io/nvidia/pytorch:24.08-py3\",         # 9.5 GB compressed / 20.4 GB decompressed\n  ]\n\n  # AZs where EBS fast snapshot restore will be enabled\n  fsr_availability_zone_names = local.azs\n\n  vpc_id    = module.vpc.vpc_id\n  subnet_id = element(module.vpc.private_subnets, 0)\n\n  tags = local.tags\n}\n</code></pre>"},{"location":"patterns/machine-learning/ml-container-cache/#cluster","title":"Cluster","text":"<pre><code>locals {\n  dev_name = \"xvdb\"\n}\n\n# SSM parameter where the `cache-builder` stores the generated snapshot ID\n# This will be used to reference the snapshot when creating the EKS node group\ndata \"aws_ssm_parameter\" \"snapshot_id\" {\n  name = module.ebs_snapshot_builder.ssm_parameter_name\n}\n\n################################################################################\n# Cluster\n################################################################################\n\nmodule \"eks\" {\n  source  = \"terraform-aws-modules/eks/aws\"\n  version = \"~&gt; 20.24\"\n\n  cluster_name    = local.name\n  cluster_version = \"1.31\"\n\n  # Give the Terraform identity admin access to the cluster\n  # which will allow it to deploy resources into the cluster\n  enable_cluster_creator_admin_permissions = true\n  cluster_endpoint_public_access           = true\n\n  cluster_addons = {\n    coredns                = {}\n    eks-pod-identity-agent = {}\n    kube-proxy             = {}\n    vpc-cni                = {}\n  }\n\n  vpc_id     = module.vpc.vpc_id\n  subnet_ids = module.vpc.private_subnets\n\n  eks_managed_node_group_defaults = {\n    ebs_optimized = true\n  }\n\n  eks_managed_node_groups = {\n    gpu = {\n      # The EKS AL2 GPU AMI provides all of the necessary components\n      # for accelerated workloads w/ EFA\n      ami_type       = \"AL2_x86_64_GPU\"\n      instance_types = [\"g6e.xlarge\"]\n\n      min_size     = 1\n      max_size     = 1\n      desired_size = 1\n\n      pre_bootstrap_user_data = &lt;&lt;-EOT\n        # Mount the second volume for containerd persistent data\n        # This volume contains the cached images and layers\n\n        systemctl stop containerd kubelet\n\n        rm -rf /var/lib/containerd/*\n        echo '/dev/${local.dev_name} /var/lib/containerd xfs defaults 0 0' &gt;&gt; /etc/fstab\n        mount -a\n\n        systemctl restart containerd kubelet\n\n      EOT\n\n      # Mount a second volume for containerd persistent data\n      # using the snapshot that contains the cached images and layers\n      block_device_mappings = {\n        (local.dev_name) = {\n          device_name = \"/dev/${local.dev_name}\"\n          ebs = {\n            # Snapshot ID from the cache builder\n            snapshot_id = nonsensitive(data.aws_ssm_parameter.snapshot_id.value)\n            volume_size = 64\n            volume_type = \"gp3\"\n          }\n        }\n      }\n\n      labels = {\n        \"nvidia.com/gpu.present\" = \"true\"\n        \"ml-container-cache\"     = \"true\"\n      }\n\n      taints = {\n        # Ensure only GPU workloads are scheduled on this node group\n        gpu = {\n          key    = \"nvidia.com/gpu\"\n          value  = \"true\"\n          effect = \"NO_SCHEDULE\"\n        }\n      }\n    }\n\n    # This node group is for core addons such as CoreDNS\n    default = {\n      instance_types = [\"m5.large\"]\n\n      min_size     = 1\n      max_size     = 2\n      desired_size = 2\n\n      # Not required - increased to demonstrate pulling the un-cached\n      # image since the default volume size is too small for the image used\n      block_device_mappings = {\n        \"xvda\" = {\n          device_name = \"/dev/xvda\"\n          ebs = {\n            volume_size = 64\n            volume_type = \"gp3\"\n          }\n        }\n      }\n    }\n  }\n\n  tags = local.tags\n}\n</code></pre>"},{"location":"patterns/machine-learning/ml-container-cache/#deploy","title":"Deploy","text":"<p>See here for the prerequisites and steps to deploy this pattern.</p> <ol> <li> <p>First, deploy the Step Function state machine that will create the EBS volume snapshots with the cached images.</p> <pre><code>terraform init\nterraform apply -target=module.ebs_snapshot_builder -target=module.vpc --auto-approve\n</code></pre> </li> <li> <p>Once the cache builder resources have been provisioned, execute the state machine by either navigating to the state machine within the AWS console and clicking <code>Start execution</code> (with the defaults or by passing in values to override the default values), or by using the provided output from the Terraform output value <code>start_execution_command</code> to start the state machine using the awscli. For example, the output looks similar to the following:</p> <pre><code>start_execution_command = &lt;&lt;EOT\naws stepfunctions start-execution \\\n  --region us-west-2 \\\n  --state-machine-arn arn:aws:states:us-west-2:111111111111:stateMachine:cache-builder \\\n  --input \"{\\\"SnapshotDescription\\\":\\\"ML container image cache\\\",\\\"SnapshotName\\\":\\\"ml-container-cache\\\"}\"\n\nEOT\n</code></pre> </li> <li> <p>Once the state machine execution has completed successfully and created an EBS snapshot volume, provision the cluster and node group that will utilize the cached images.</p> <pre><code>terraform apply --auto-approve\n</code></pre> </li> <li> <p>Once the EKS cluster and node group have been provisioned, you can deploy the provided example pod that will use a cached image to verify the time it takes for the pod to reach a ready state.</p> <pre><code>kubectl apply --server-side -f pod-cached.yaml\n</code></pre> <p>You can contrast this with the time it takes for a pod that is not cached on a node by using the provided <code>pod-uncached.yaml</code> file. This works by simply using a pod that doesn't have a toleration for nodes that contain NVIDIA GPUs, which is where the cached images are provided in this example.</p> <pre><code>kubectl apply --server-side -f pod-uncached.yaml\n</code></pre> <p>You can also do the same steps above but using the small, utility CLI ktime which can either collect the pod events to measure the time duration to reach a ready state, or it can deploy a pod manifest and return the same:</p> <pre><code>ktime apply -f pod-cached.yaml\n-- or --\nktime apply -f pod-uncached.yaml\n</code></pre> </li> </ol>"},{"location":"patterns/machine-learning/ml-container-cache/#destroy","title":"Destroy","text":"<pre><code>terraform destroy --auto-approve\n</code></pre>"},{"location":"patterns/machine-learning/nvidia-gpu-efa/","title":"NVIDIA GPUs with EFA","text":""},{"location":"patterns/machine-learning/nvidia-gpu-efa/#eks-cluster-w-nvidia-gpus-and-efa-for-machine-learning","title":"EKS Cluster w/ NVIDIA GPUs and EFA for Machine Learning","text":"<p>This pattern demonstrates an Amazon EKS Cluster with an EFA-enabled nodegroup that utilizes <code>p5.48xlarge</code> instances with H100 NVIDIA GPUs used in distributed, multi-node machine learning.</p> <p>The following components are demonstrated in this pattern:</p> <ul> <li>A \"default\" node group that supports addons and components that do not require GPUs nor EFA devices. Any pods that do not tolerate the taints of the GPU node group will be scheduled on instances within this node group.</li> <li>A node group of <code>p5.48xlarge</code> instances with:<ul> <li>all x32 EFA network interfaces enabled</li> <li>provisioned within a placement group so that the instances are provisioned close to one another in a single availability zone that supports the instance type</li> <li>a common NVIDIA taint of <code>\"nvidia.com/gpu:NoSchedule\"</code> to ensure only the intended applications are allowed to run on the nodes created</li> <li>two labels to identify that this nodegroup supports NVIDIA GPUs and EFA devices and allow pods to use node selectors with these labels</li> <li>the NVME instance store volumes are mounted in a RAID-0 array to provide a single, large, high-performance storage volume for the GPU workloads</li> <li>kubelet and containerd are configured to utilize the RAID-0 volume, allowing kubelet to discover the additional storage as ephemeral storage that can be utilized by pods</li> </ul> </li> <li>A Helm chart deployment for the NVIDIA device plugin to expose and mount the GPUs provided by the instances to the pods that request them</li> <li>A Helm chart deployment for the EFA device plugin to expose and mount the EFA network interfaces provided by the instances to the pods that request them. Since the EFA network interfaces are only found on the instances that provide NVIDIA GPUs in this pattern, we do not apply an additional taint for the EFA network interfaces to avoid over-constraining.</li> </ul>"},{"location":"patterns/machine-learning/nvidia-gpu-efa/#code","title":"Code","text":""},{"location":"patterns/machine-learning/nvidia-gpu-efa/#cluster","title":"Cluster","text":"<pre><code>################################################################################\n# Cluster\n################################################################################\n\nmodule \"eks\" {\n  source  = \"terraform-aws-modules/eks/aws\"\n  version = \"~&gt; 20.26\"\n\n  cluster_name    = local.name\n  cluster_version = \"1.31\"\n\n  # Give the Terraform identity admin access to the cluster\n  # which will allow it to deploy resources into the cluster\n  enable_cluster_creator_admin_permissions = true\n  cluster_endpoint_public_access           = true\n\n  cluster_addons = {\n    coredns                = {}\n    eks-pod-identity-agent = {}\n    kube-proxy             = {}\n    vpc-cni = {\n      most_recent = true\n    }\n  }\n\n  # Add security group rules on the node group security group to\n  # allow EFA traffic\n  enable_efa_support = true\n\n  vpc_id     = module.vpc.vpc_id\n  subnet_ids = module.vpc.private_subnets\n\n  eks_managed_node_groups = {\n    nvidia-efa = {\n      # The EKS AL2023 NVIDIA AMI provides all of the necessary components\n      # for accelerated workloads w/ EFA\n      ami_type       = \"AL2023_x86_64_NVIDIA\"\n      instance_types = [\"p5.48xlarge\"]\n\n      # Mount instance store volumes in RAID-0 for kubelet and containerd\n      # https://github.com/awslabs/amazon-eks-ami/blob/master/doc/USER_GUIDE.md#raid-0-for-kubelet-and-containerd-raid0\n      cloudinit_pre_nodeadm = [\n        {\n          content_type = \"application/node.eks.aws\"\n          content      = &lt;&lt;-EOT\n            ---\n            apiVersion: node.eks.aws/v1alpha1\n            kind: NodeConfig\n            spec:\n              instance:\n                localStorage:\n                  strategy: RAID0\n          EOT\n        }\n      ]\n\n      min_size     = 2\n      max_size     = 2\n      desired_size = 2\n\n      # This will:\n      # 1. Create a placement group to place the instances close to one another\n      # 2. Ignore subnets that reside in AZs that do not support the instance type\n      # 3. Expose all of the available EFA interfaces on the launch template\n      enable_efa_support = true\n\n      labels = {\n        \"vpc.amazonaws.com/efa.present\" = \"true\"\n        \"nvidia.com/gpu.present\"        = \"true\"\n      }\n\n      taints = {\n        # Ensure only GPU workloads are scheduled on this node group\n        gpu = {\n          key    = \"nvidia.com/gpu\"\n          value  = \"true\"\n          effect = \"NO_SCHEDULE\"\n        }\n      }\n    }\n\n    # This node group is for core addons such as CoreDNS\n    default = {\n      instance_types = [\"m5.large\"]\n\n      min_size     = 1\n      max_size     = 2\n      desired_size = 2\n    }\n  }\n\n  tags = local.tags\n}\n</code></pre>"},{"location":"patterns/machine-learning/nvidia-gpu-efa/#device-plugins","title":"Device Plugins","text":"<pre><code>################################################################################\n# Helm charts\n################################################################################\n\nresource \"helm_release\" \"nvidia_device_plugin\" {\n  name             = \"nvidia-device-plugin\"\n  repository       = \"https://nvidia.github.io/k8s-device-plugin\"\n  chart            = \"nvidia-device-plugin\"\n  version          = \"0.16.2\"\n  namespace        = \"nvidia-device-plugin\"\n  create_namespace = true\n  wait             = false\n}\n\nresource \"helm_release\" \"aws_efa_device_plugin\" {\n  name       = \"aws-efa-k8s-device-plugin\"\n  repository = \"https://aws.github.io/eks-charts\"\n  chart      = \"aws-efa-k8s-device-plugin\"\n  version    = \"v0.5.5\"\n  namespace  = \"kube-system\"\n  wait       = false\n\n  values = [\n    &lt;&lt;-EOT\n      nodeSelector:\n        vpc.amazonaws.com/efa.present: 'true'\n      tolerations:\n        - key: nvidia.com/gpu\n          operator: Exists\n          effect: NoSchedule\n    EOT\n  ]\n}\n</code></pre>"},{"location":"patterns/machine-learning/nvidia-gpu-efa/#deploy","title":"Deploy","text":"<p>See here for the prerequisites and steps to deploy this pattern.</p>"},{"location":"patterns/machine-learning/nvidia-gpu-efa/#validate","title":"Validate","text":"<p>Note</p> <p>Desired instance type can be specified in eks.tf. Values shown below will change based on the instance type selected (i.e. - <code>p5.48xlarge</code> has 8 GPUs and 32 EFA interfaces). A list of EFA-enabled instance types is available here. If you are using an on-demand capacity reservation (ODCR) for your instance type, please uncomment the <code>capacity_reservation_specification</code> block in <code>eks.tf</code> and specify a capacity_reservation_id. Please ensure that the region and availability zone of your ODCR match the ones used in <code>main.tf</code>.</p> <ol> <li> <p>List the nodes and their instance type:</p> <pre><code>kubectl get nodes -L node.kubernetes.io/instance-type\n</code></pre> <pre><code>NAME                                        STATUS   ROLES    AGE   VERSION               INSTANCE-TYPE\nip-10-0-1-16.us-east-2.compute.internal     Ready    &lt;none&gt;   12h   v1.29.3-eks-ae9a62a   p5.48xlarge\nip-10-0-12-113.us-east-2.compute.internal   Ready    &lt;none&gt;   14h   v1.29.3-eks-ae9a62a   m5.large\nip-10-0-12-201.us-east-2.compute.internal   Ready    &lt;none&gt;   12h   v1.29.3-eks-ae9a62a   p5.48xlarge\nip-10-0-46-217.us-east-2.compute.internal   Ready    &lt;none&gt;   14h   v1.29.3-eks-ae9a62a   m5.large\n</code></pre> <p>You should see two EFA-enabled (in this example <code>p5.48xlarge</code>) nodes in the list.</p> </li> <li> <p>Deploy Kubeflow MPI Operator</p> <p>Kubeflow MPI Operator is required for running MPIJobs on EKS. We will use an MPIJob to test EFA. To deploy the MPI operator execute the following:</p> <pre><code>kubectl apply --server-side -f https://raw.githubusercontent.com/kubeflow/mpi-operator/v0.6.0/deploy/v2beta1/mpi-operator.yaml\n</code></pre> <pre><code>namespace/mpi-operator serverside-applied\ncustomresourcedefinition.apiextensions.k8s.io/mpijobs.kubeflow.org serverside-applied\nserviceaccount/mpi-operator serverside-applied\nclusterrole.rbac.authorization.k8s.io/kubeflow-mpijobs-admin serverside-applied\nclusterrole.rbac.authorization.k8s.io/kubeflow-mpijobs-edit serverside-applied\nclusterrole.rbac.authorization.k8s.io/kubeflow-mpijobs-view serverside-applied\nclusterrole.rbac.authorization.k8s.io/mpi-operator serverside-applied\nclusterrolebinding.rbac.authorization.k8s.io/mpi-operator serverside-applied\ndeployment.apps/mpi-operator serverside-applied\n</code></pre> </li> <li> <p>EFA info test</p> <p>This test prints a list of available EFA interfaces by using the <code>/opt/amazon/efa/bin/fi_info</code> utility. The script generate-efa-info-test.sh creates an MPIJob manifest file named <code>efa-info-test.yaml</code>. It assumes that there are two cluster nodes with 8 GPU's per node and 32 EFA adapters. If you are not using <code>p5.48xlarge</code> instances in your cluster, you may adjust the settings in the script prior to running it.</p> <p><code>NUM_WORKERS</code> - number of nodes you want to run the test on <code>GPU_PER_WORKER</code> - number of GPUs available on each node <code>EFA_PER_WORKER</code> - number of EFA interfaces available on each node</p> <pre><code>./generate-efa-info-test.sh\n</code></pre> <p>To start the test apply the generated manifest to the cluster:</p> <pre><code>kubectl apply --server-side -f ./efa-info-test.yaml\n</code></pre> <pre><code>mpijob.kubeflow.org/efa-info-test created\n</code></pre> <p>Observe the pods in the current namespace. You should see a launcher pod and worker pods. It is normal for the launcher pod to restart a few times until the worker pods are fully running.</p> <pre><code>watch kubectl get pods\n</code></pre> <pre><code>NAME                           READY   STATUS             RESTARTS      AGE\nefa-info-test-launcher-wm8pm   0/1     CrashLoopBackOff   1 (16s ago)   19s\nefa-info-test-worker-0         1/1     Running            0             19s\nefa-info-test-worker-1         1/1     Running            0             19s\n</code></pre> <pre><code>NAME                           READY   STATUS    RESTARTS      AGE\nefa-info-test-launcher-wm8pm   1/1     Running   2 (18s ago)   21s\nefa-info-test-worker-0         1/1     Running   0             21s\nefa-info-test-worker-1         1/1     Running   0             21s\n</code></pre> <pre><code>NAME                           READY   STATUS      RESTARTS   AGE\nefa-info-test-launcher-wm8pm   0/1     Completed   2          5m20s\n</code></pre> <p>Once the test launcher pod enters status <code>Running</code> or <code>Completed</code>, see the test logs using the command below:</p> <pre><code>kubectl logs -f $(kubectl get pods | grep launcher | cut -d ' ' -f 1)\n</code></pre> <pre><code>Warning: Permanently added 'efa-info-test-worker-1.efa-info-test.default.svc' (ED25519) to the list of known hosts.\nWarning: Permanently added 'efa-info-test-worker-0.efa-info-test.default.svc' (ED25519) to the list of known hosts.\n[1,1]&lt;stdout&gt;:provider: efa\n[1,1]&lt;stdout&gt;:    fabric: efa\n[1,1]&lt;stdout&gt;:    domain: rdmap79s0-rdm\n[1,1]&lt;stdout&gt;:    version: 120.10\n[1,1]&lt;stdout&gt;:    type: FI_EP_RDM\n[1,1]&lt;stdout&gt;:    protocol: FI_PROTO_EFA\n\n...\n\n[1,0]&lt;stdout&gt;:provider: efa\n[1,0]&lt;stdout&gt;:    fabric: efa\n[1,0]&lt;stdout&gt;:    domain: rdmap201s0-rdm\n[1,0]&lt;stdout&gt;:    version: 120.10\n[1,0]&lt;stdout&gt;:    type: FI_EP_RDM\n[1,0]&lt;stdout&gt;:    protocol: FI_PROTO_EFA\n</code></pre> <p>Finally, remove the job:</p> <pre><code>kubectl delete -f ./efa-info-test.yaml\n</code></pre> </li> <li> <p>EFA NCCL test</p> <p>The EFA NCCL test is used to measure network bandwidth by running the <code>/opt/nccl-tests/build/all_reduce_perf</code> utility. Create an MPIjob manifest by executing the script below:</p> <pre><code>./generate-efa-nccl-test.sh\n</code></pre> <p>This script creates a file named <code>efa-nccl-test.yaml</code>. Apply the manifest to start the EFA nccl test.</p> <pre><code>kubectl apply --server-side -f ./efa-nccl-test.yaml\n\n```text\nmpijob.kubeflow.org/efa-nccl-test created\n</code></pre> <p>Similarly to the EFA info test, a launcher and worker pods will be created. The launcher pod will be in CrashLoopBackoff mode until the worker pods enter Running state. As soon as the launcher pod enters Running state as well, execute the following command to see the test logs:</p> <pre><code>kubectl logs -f $(kubectl get pods | grep launcher | cut -d ' ' -f 1)\n</code></pre> <pre><code>...\n[1,0]&lt;stdout&gt;:#                                                              out-of-place                       in-place\n[1,0]&lt;stdout&gt;:#       size         count      type   redop    root     time   algbw   busbw #wrong     time   algbw   busbw #wrong\n[1,0]&lt;stdout&gt;:#        (B)    (elements)                               (us)  (GB/s)  (GB/s)            (us)  (GB/s)  (GB/s)\n[1,0]&lt;stdout&gt;:           8             2     float     sum      -1    87.96    0.00    0.00      0    78.05    0.00    0.00      0\n[1,0]&lt;stdout&gt;:          16             4     float     sum      -1    76.83    0.00    0.00      0    77.15    0.00    0.00      0\n[1,0]&lt;stdout&gt;:          32             8     float     sum      -1    77.37    0.00    0.00      0    75.38    0.00    0.00      0\n[1,0]&lt;stdout&gt;:          64            16     float     sum      -1    77.60    0.00    0.00      0    79.80    0.00    0.00      0\n[1,0]&lt;stdout&gt;:         128            32     float     sum      -1    77.20    0.00    0.00      0    77.78    0.00    0.00      0\n[1,0]&lt;stdout&gt;:         256            64     float     sum      -1    78.46    0.00    0.01      0    80.39    0.00    0.01      0\n[1,0]&lt;stdout&gt;:         512           128     float     sum      -1    77.56    0.01    0.01      0    78.00    0.01    0.01      0\n[1,0]&lt;stdout&gt;:        1024           256     float     sum      -1    76.98    0.01    0.02      0    78.52    0.01    0.02      0\n[1,0]&lt;stdout&gt;:        2048           512     float     sum      -1    77.92    0.03    0.05      0    78.64    0.03    0.05      0\n[1,0]&lt;stdout&gt;:        4096          1024     float     sum      -1    83.26    0.05    0.09      0    83.16    0.05    0.09      0\n[1,0]&lt;stdout&gt;:        8192          2048     float     sum      -1    88.46    0.09    0.17      0    86.32    0.09    0.18      0\n[1,0]&lt;stdout&gt;:       16384          4096     float     sum      -1    97.22    0.17    0.32      0    94.82    0.17    0.32      0\n[1,0]&lt;stdout&gt;:       32768          8192     float     sum      -1    98.84    0.33    0.62      0    99.85    0.33    0.62      0\n[1,0]&lt;stdout&gt;:       65536         16384     float     sum      -1    101.1    0.65    1.22      0    96.80    0.68    1.27      0\n[1,0]&lt;stdout&gt;:      131072         32768     float     sum      -1    100.5    1.30    2.44      0    99.13    1.32    2.48      0\n[1,0]&lt;stdout&gt;:      262144         65536     float     sum      -1    104.5    2.51    4.70      0    102.2    2.57    4.81      0\n[1,0]&lt;stdout&gt;:      524288        131072     float     sum      -1    108.8    4.82    9.04      0    109.8    4.78    8.96      0\n[1,0]&lt;stdout&gt;:     1048576        262144     float     sum      -1    119.1    8.81   16.51      0    121.5    8.63   16.18      0\n[1,0]&lt;stdout&gt;:     2097152        524288     float     sum      -1    145.8   14.39   26.97      0    144.7   14.49   27.17      0\n[1,0]&lt;stdout&gt;:     4194304       1048576     float     sum      -1    163.2   25.70   48.19      0    162.4   25.82   48.42      0\n[1,0]&lt;stdout&gt;:     8388608       2097152     float     sum      -1    197.9   42.38   79.46      0    197.9   42.39   79.48      0\n[1,0]&lt;stdout&gt;:    16777216       4194304     float     sum      -1    282.3   59.43  111.43      0    290.3   57.79  108.35      0\n[1,0]&lt;stdout&gt;:    33554432       8388608     float     sum      -1    442.8   75.77  142.07      0    417.4   80.39  150.73      0\n[1,0]&lt;stdout&gt;:    67108864      16777216     float     sum      -1    597.4  112.34  210.64      0    591.6  113.43  212.68      0\n[1,0]&lt;stdout&gt;:   134217728      33554432     float     sum      -1    872.0  153.92  288.60      0    872.8  153.78  288.34      0\n[1,0]&lt;stdout&gt;:   268435456      67108864     float     sum      -1   1501.2  178.81  335.27      0   1503.1  178.59  334.86      0\n[1,0]&lt;stdout&gt;:   536870912     134217728     float     sum      -1   2599.4  206.54  387.26      0   2553.0  210.29  394.29      0\n[1,0]&lt;stdout&gt;:  1073741824     268435456     float     sum      -1   4556.5  235.65  441.85      0   5274.0  203.59  381.74      0\n[1,0]&lt;stdout&gt;:  2147483648     536870912     float     sum      -1   9079.2  236.53  443.49      0   9128.1  235.26  441.11      0\n[1,0]&lt;stdout&gt;:  4294967296    1073741824     float     sum      -1    17320  247.98  464.96      0    17270  248.70  466.31      0\n[1,0]&lt;stdout&gt;:  8589934592    2147483648     float     sum      -1    33611  255.57  479.19      0    34474  249.17  467.20      0\n[1,0]&lt;stdout&gt;: 17179869184    4294967296     float     sum      -1    66988  256.46  480.87      0    66717  257.50  482.82      0\n[1,0]&lt;stdout&gt;:# Out of bounds values : 0 OK\n[1,0]&lt;stdout&gt;:# Avg bus bandwidth    : 123.343\n</code></pre> <p>Columns 9 and 13 in the output table show the in-place and out-of-place bus bandwidth calculated for the data size listed in column 2. In this case it is at maximum 444.31 and 444.41 GB/s respectively. Your actual results may be slightly different. The calculated average bus bandwidth is displayed at the end of the log. In this test run the average bus bandwidth was 79.9352 GB/s.</p> <p>Lastly, delete the MPIJob:</p> <pre><code>kubectl delete -f ./efa-nccl-test.yaml\n</code></pre> </li> </ol>"},{"location":"patterns/machine-learning/nvidia-gpu-efa/#destroy","title":"Destroy","text":"<pre><code>terraform destroy -target=\"module.eks_blueprints_addons\" -auto-approve\nterraform destroy -target=\"module.eks\" -auto-approve\nterraform destroy -auto-approve\n</code></pre> <p>See here for more details on cleaning up the resources created.</p>"},{"location":"patterns/machine-learning/targeted-odcr/","title":"Targeted On-Demand Capacity Reservation (ODCR)","text":""},{"location":"patterns/machine-learning/targeted-odcr/#eks-cluster-with-on-demand-capacity-reservations-odcr","title":"EKS Cluster with On-demand Capacity Reservations (ODCR)","text":"<p>This pattern demonstrates how to consume/utilize on-demand capacity reservations (ODCRs) with Amazon EKS. The solution is comprised of primarily 3 components:</p> <ol> <li> <p>The node group that will utilize the ODCRs should have the subnets provided to it restricted to the availability zone where the ODCR(s) capacity is allocated. For example - if the ODCR(s) are allocated to <code>us-west-2b</code>, the node group should only have subnet IDs provided to it that reside in <code>us-west-2b</code>. If the subnets that reside in other AZs are provided, its possible to encounter an error such as <code>InvalidParameterException: The following supplied instance types do not exist ...</code>. It is not guaranteed that this error will always be shown, and may appear random since the underlying autoscaling group(s) will provision nodes into different AZs at random. It will only occur when the underlying autoscaling group tries to provision instances into an AZ where capacity is not allocated and there is insufficient on-demand capacity for the desired instance type.</p> </li> <li> <p>A custom launch template is required in order to specify the <code>capacity_reservation_specification</code> arguments. This is how the ODCRs are integrated into the node group (i.e. - tells the autoscaling group to utilize the provided capacity reservation(s)).</p> <p>Info</p> <p>By default, the <code>terraform-aws-eks</code> module creates and utilizes a custom launch template with EKS managed node groups which means users just need to supply the <code>capacity_reservation_specification</code> in their node group definition.</p> </li> <li> <p>A resource group will need to be created for the capacity reservations. The resource group acts like a container, allowing for ODCRs to be added or removed as needed to adjust the available capacity. Utilizing the resource group allows for this additional capacity to be adjusted without any modification or disruption to the existing node group or launch template. As soon as the ODCR has been associated to the resource group, the node group can scale up to start utilizing that capacity.</p> </li> </ol> <p>Links:</p> <ul> <li>Tutorial: Launch On-Demand Instances using targeted Capacity Reservations</li> <li>Target a group of Amazon EC2 On-Demand Capacity Reservations</li> </ul>"},{"location":"patterns/machine-learning/targeted-odcr/#code","title":"Code","text":"<pre><code>################################################################################\n# Required Input\n################################################################################\n\nvariable \"capacity_reservation_arns\" {\n  description = \"List of on-demand capacity block reservation ARNs for the node group\"\n  type        = list(string)\n}\n\n################################################################################\n# Cluster\n################################################################################\n\nmodule \"eks\" {\n  source  = \"terraform-aws-modules/eks/aws\"\n  version = \"~&gt; 20.26\"\n\n  cluster_name    = local.name\n  cluster_version = \"1.31\"\n\n  # Give the Terraform identity admin access to the cluster\n  # which will allow it to deploy resources into the cluster\n  enable_cluster_creator_admin_permissions = true\n  cluster_endpoint_public_access           = true\n\n  cluster_addons = {\n    coredns                = {}\n    eks-pod-identity-agent = {}\n    kube-proxy             = {}\n    vpc-cni = {\n      most_recent = true\n    }\n  }\n\n  # Add security group rules on the node group security group to\n  # allow EFA traffic\n  enable_efa_support = true\n\n  vpc_id     = module.vpc.vpc_id\n  subnet_ids = module.vpc.private_subnets\n\n  eks_managed_node_groups = {\n    odcr = {\n      # The EKS AL2023 NVIDIA AMI provides all of the necessary components\n      # for accelerated workloads w/ EFA\n      ami_type       = \"AL2023_x86_64_NVIDIA\"\n      instance_types = [\"p5.48xlarge\"]\n\n      # Mount instance store volumes in RAID-0 for kubelet and containerd\n      # https://github.com/awslabs/amazon-eks-ami/blob/master/doc/USER_GUIDE.md#raid-0-for-kubelet-and-containerd-raid0\n      cloudinit_pre_nodeadm = [\n        {\n          content_type = \"application/node.eks.aws\"\n          content      = &lt;&lt;-EOT\n            ---\n            apiVersion: node.eks.aws/v1alpha1\n            kind: NodeConfig\n            spec:\n              instance:\n                localStorage:\n                  strategy: RAID0\n          EOT\n        }\n      ]\n\n      min_size     = 2\n      max_size     = 2\n      desired_size = 2\n\n      # This will:\n      # 1. Create a placement group to place the instances close to one another\n      # 2. Ignore subnets that reside in AZs that do not support the instance type\n      # 3. Expose all of the available EFA interfaces on the launch template\n      enable_efa_support = true\n\n      min_size     = 4\n      max_size     = 5\n      desired_size = 2\n\n      labels = {\n        \"vpc.amazonaws.com/efa.present\" = \"true\"\n        \"nvidia.com/gpu.present\"        = \"true\"\n      }\n\n      taints = {\n        # Ensure only GPU workloads are scheduled on this node group\n        gpu = {\n          key    = \"nvidia.com/gpu\"\n          value  = \"true\"\n          effect = \"NO_SCHEDULE\"\n        }\n      }\n\n      # First subnet is in the \"${local.region}a\" availability zone\n      # where the capacity reservation is created\n      # TODO - Update the subnet to match the availability zone of *YOUR capacity reservation\n      subnet_ids = [element(module.vpc.private_subnets, 0)]\n\n      # Targeted on-demand capacity reservation\n      capacity_reservation_specification = {\n        capacity_reservation_target = {\n          capacity_reservation_resource_group_arn = aws_resourcegroups_group.odcr.arn\n        }\n      }\n    }\n\n    # This node group is for core addons such as CoreDNS\n    default = {\n      instance_types = [\"m5.large\"]\n\n      min_size     = 1\n      max_size     = 2\n      desired_size = 2\n    }\n  }\n\n  tags = local.tags\n}\n\n################################################################################\n# Resource Group\n################################################################################\n\nresource \"aws_resourcegroups_group\" \"odcr\" {\n  name        = \"${local.name}-p5-odcr\"\n  description = \"P5 instance on-demand capacity reservations\"\n\n  configuration {\n    type = \"AWS::EC2::CapacityReservationPool\"\n  }\n\n  configuration {\n    type = \"AWS::ResourceGroups::Generic\"\n\n    parameters {\n      name   = \"allowed-resource-types\"\n      values = [\"AWS::EC2::CapacityReservation\"]\n    }\n  }\n}\n\nresource \"aws_resourcegroups_resource\" \"odcr\" {\n  count = length(var.capacity_reservation_arns)\n\n  group_arn    = aws_resourcegroups_group.odcr.arn\n  resource_arn = element(var.capacity_reservation_arns, count.index)\n}\n</code></pre>"},{"location":"patterns/machine-learning/targeted-odcr/#deploy","title":"Deploy","text":"<p>See here for the prerequisites and steps to deploy this pattern.</p>"},{"location":"patterns/machine-learning/targeted-odcr/#validate","title":"Validate","text":"<ol> <li> <p>Navigate to the EC2 console page - on the left hand side, click on <code>Capacity Reservations</code>under the <code>Instances</code> section. You should see the capacity reservation(s) that have been created similar to the screenshot below. For this example, you can see that <code>Available capacity</code> column is empty, which means that the capacity reservations have been fully utilized by the example (as expected).</p> <p> </p> </li> <li> <p>Click on one of the capacity reservation IDs to view the details of the capacity reservation. You should see the details of the capacity reservation similar to the screenshot below. For this example, you can see that <code>Available capacity</code> is <code>0 instances</code>, which means that the capacity reservation has been fully utilized by the example (as expected).</p> <p> </p> </li> </ol>"},{"location":"patterns/machine-learning/targeted-odcr/#destroy","title":"Destroy","text":"<pre><code>terraform destroy -target=\"module.eks_blueprints_addons\" -auto-approve\nterraform destroy -target=\"module.eks\" -auto-approve\nterraform destroy -auto-approve\n</code></pre> <p>See here for more details on cleaning up the resources created.</p>"},{"location":"patterns/network/aws-vpc-cni-network-policy/","title":"AWS VPC CNI Network Policy","text":""},{"location":"patterns/network/aws-vpc-cni-network-policy/#amazon-eks-cluster-w-network-policies","title":"Amazon EKS Cluster w/ Network Policies","text":"<p>This pattern demonstrates an EKS cluster that uses the native Network Policy support provided by the Amazon VPC CNI (1.14.0 or higher).</p> <ul> <li>Documentation</li> <li>Launch Blog</li> </ul>"},{"location":"patterns/network/aws-vpc-cni-network-policy/#scenario","title":"Scenario","text":"<p>This pattern deploys an Amazon EKS Cluster with Network Policies support implemented by the Amazon VPC CNI. Further it deploys a simple demo application (distributed as a Helm Chart) and some sample Network Policies to restrict the traffic between different components of the application.</p> <p>For a detailed description of the demo application and the Network Policies, please refer to the Stars demo of network policy section in the official Documentation.</p>"},{"location":"patterns/network/aws-vpc-cni-network-policy/#deploy","title":"Deploy","text":"<p>See here for the prerequisites and steps to deploy this pattern.</p>"},{"location":"patterns/network/aws-vpc-cni-network-policy/#validate","title":"Validate","text":"<ol> <li> <p>List out the pods running currently:</p> <pre><code>kubectl get pods -A\n</code></pre> <p><code>text    NAMESPACE         NAME                                       READY   STATUS    RESTARTS   AGE [...] client            client-xlf2c                               1/1     Running   0          5m19s [...] management-ui     management-ui-qrb2g                        1/1     Running   0          5m24s stars             backend-sz87q                              1/1     Running   0          5m23s stars             frontend-c1cnf                             1/1     Running   0          5m21s [...]</code></p> <p>In your output, you should see pods in the namespaces shown in the following output. The NAMES of your pods and the number of pods in the READY column are different than those in the following output. Don't continue until you see pods with similar names and they all have Running in the STATUS column.</p> </li> <li> <p>Connect to the management user interface using the EXTERNAL IP of the running service and observe the traffic flow and restrictions based on the Network Policies deployed:</p> <pre><code>kubectl get service/management-ui -n management-ui\n</code></pre> <p>Open the browser based on the URL obtained from the previous step to see the connection map and restrictions put in place by the Network Policies deployed.</p> </li> </ol>"},{"location":"patterns/network/aws-vpc-cni-network-policy/#destroy","title":"Destroy","text":"<pre><code>terraform destroy -target=\"module.eks_blueprints_addons\" -auto-approve\nterraform destroy -target=\"module.eks\" -auto-approve\nterraform destroy -auto-approve\n</code></pre> <p>See here for more details on cleaning up the resources created.</p>"},{"location":"patterns/network/client-server-communication/","title":"Amazon VPC Lattice Client-server Communication","text":""},{"location":"patterns/network/client-server-communication/#amazon-vpc-lattice-simple-client-to-server-communication","title":"Amazon VPC Lattice - Simple Client to Server Communication","text":"<p>This pattern demonstrates how to expose an EKS cluster hosted application to an internal consumer through Amazon VPC Lattice.</p>"},{"location":"patterns/network/client-server-communication/#scenario","title":"Scenario","text":"<p>With this solution we showcase how to configure Amazon VPC Lattice using the AWS Gateway API Controller in order to manage Amazon VPC Lattice resources through native K8S Gateway API objects. This pattern deploys two distinct VPCs with a client application running in one of them and a server application in the other. The server application is deployed inside an EKS cluster and made exposed to the client application through Amazon VPC Lattice which establishes connectivity between the two applications. Further we demonstrate how to configure a custom domain name for the exposed service using Amazon Route53 and the external-dns project.</p> <p></p>"},{"location":"patterns/network/client-server-communication/#deploy","title":"Deploy","text":"<p>See here for the prerequisites and steps to deploy this pattern.</p>"},{"location":"patterns/network/client-server-communication/#validate","title":"Validate","text":"<p>In order to test the connectivity between the client and server, please follow the steps outlined below:</p> <ol> <li>Login to the management console of your AWS account and navigate to the EC2 service</li> <li>Select your the EC2 Instance with the name client, click Connect, choose Session Manager and click Connect</li> <li> <p>Within the console test the connectivity to the server application by entering the following command:</p> <pre><code>$ curl -i http://server.example.com\nHTTP/1.1 200 OK\ndate: Thu, 14 Dec 2023 08:29:39 GMT\ncontent-length: 54\ncontent-type: text/plain; charset=utf-8\n\nRequesting to Pod(server-6f487b9bcd-5qm4v): server pod\n</code></pre> </li> </ol>"},{"location":"patterns/network/client-server-communication/#destroy","title":"Destroy","text":"<pre><code>terraform destroy -target=\"module.eks_blueprints_addons\" -auto-approve\nterraform destroy -target=\"module.eks\" -auto-approve\nterraform destroy -auto-approve\n</code></pre> <p>See here for more details on cleaning up the resources created.</p>"},{"location":"patterns/network/cross-cluster-pod-communication/","title":"Cross VPC and EKS clusters secure Communication","text":""},{"location":"patterns/network/cross-cluster-pod-communication/#amazon-vpc-lattice-multi-cluster-secure-communication","title":"Amazon VPC Lattice - Multi-cluster secure communication","text":"<p>This pattern showcases secure multi-cluster communication between two EKS clusters in different VPCs using VPC Lattice with IAM authorization. It illustrates service discovery and highlights how VPC Lattice facilitates communication between services in EKS clusters with overlapping CIDRs, eliminating the need for networking constructs like private NAT Gateways and Transit Gateways.</p> <p>You can also find more informations in the associated blog post</p>"},{"location":"patterns/network/cross-cluster-pod-communication/#scenario","title":"Scenario","text":"<p>With this solution we showcase how to configure Amazon VPC Lattice using the AWS Gateway API Controller in order to manage Amazon VPC Lattice resources through native Kubernetes Gateway API objects. This pattern deploys two distinct VPCs each having it's own EKS cluster, which contain an application that will be used to demonstrate cross-cluster communication.</p> <p>The cross-cluster communication will be established through Amazon VPC Lattice, using a private Amazon Route53 domain name protected by a TLS Certificate issued by Certificate Manager (ACM) and supported by an AWS Private Certificate Authority.</p> <p></p> <ol> <li>HttpRoute Configuration</li> <li>Defines service exposure through VPC Lattice Gateway API</li> <li>Specifies routing rules, paths, and backend services</li> <li>Kyverno Policy Implementation</li> <li>Injects Envoy SigV4 proxy sidecar</li> <li>Automatically signs AWS API requests with AWS credentials</li> <li>Ensures secure authentication for service-to-service communication</li> <li>AWS Private Certificate Authority (PCA)</li> <li>Issues and manages private certificates</li> <li>Validates custom domain names within VPC Lattice</li> <li>Enables TLS encryption for internal communications</li> <li>IAM Authentication Policy</li> <li>Defines fine-grained access control rules</li> <li>Specifies which principals can access which services</li> <li>Implements least-privilege security model</li> <li>ExternalDNS Integration</li> <li>Monitors Gateway API Controller's DNSEndpoint resources</li> <li>Automatically creates and updates DNS records</li> <li>Maintains service discovery through Route 53</li> <li>App1 \u2192 App2 Communication Flow</li> <li>Routes through VPC Lattice service network</li> <li>Authenticated via IAM policies</li> <li>Encrypted using TLS certificates from Private CA</li> <li>App2 \u2192 App1 Communication Flow</li> <li>Utilizes bi-directional VPC Lattice connectivity</li> <li>Follows same security and authentication patterns</li> <li>Maintains consistent service mesh principles</li> </ol>"},{"location":"patterns/network/cross-cluster-pod-communication/#deploy","title":"Deploy","text":"<p>This pattern is composed of 3 Terraform stacks, that needs to be deployed in order.</p>"},{"location":"patterns/network/cross-cluster-pod-communication/#1-create-shared-environment","title":"1. Create shared Environment","text":"<p>First, we need to deploy the shared environment:</p> <p></p> <p>Deploy terraform environment stack:</p> <pre><code>cd environment\nterraform init\nterraform apply --auto-approve\n</code></pre>"},{"location":"patterns/network/cross-cluster-pod-communication/#2-create-eks-cluster-1","title":"2. Create EKS cluster 1","text":"<p>Now we will deploy the 2 EKS clusters to match our architecture diagram</p> <p>Deploy terraform cluster1 stack:</p> <pre><code>cd ../cluster\n./deploy.sh cluster1\n</code></pre> <p>Configure Kubectl:</p> <pre><code>eval `terraform output -raw configure_kubectl`\n</code></pre>"},{"location":"patterns/network/cross-cluster-pod-communication/#3-create-eks-cluster-2","title":"3. Create EKS cluster 2","text":"<p>Deploy terraform cluster2 stack:</p> <pre><code>./deploy.sh cluster2\n</code></pre> <p>Configure Kubectl:</p> <pre><code>eval `terraform output -raw configure_kubectl`\n</code></pre>"},{"location":"patterns/network/cross-cluster-pod-communication/#validate","title":"Validate","text":"<p>In order to validate the connectivity between our services, we can use the following commands:</p> <ol> <li>From cluster1 app1, call cluster2 app2 -&gt; success</li> </ol> <pre><code>kubectl --context eks-cluster1 exec -ti -n apps deployments/demo-cluster1-v1 -c demo-cluster1-v1 -- curl demo-cluster2.example.com\n</code></pre> <pre><code>Requsting to Pod(demo-cluster2-v1-c99c7bb69-2gm5f): Hello from demo-cluster2-v1\n</code></pre> <ol> <li>From cluster1 app1, call cluster1 app1 -&gt; forbidden</li> </ol> <p>We can see the response if we call the service but don't have the authorization from VPC lattice:</p> <pre><code>kubectl --context eks-cluster1 exec -ti -n apps deployments/demo-cluster1-v1 -c demo-cluster1-v1 -- curl demo-cluster1.example.com\n</code></pre> <pre><code>AccessDeniedException: User: arn:aws:sts::12345678910:assumed-role/vpc-lattice-sigv4-client/eks-eks-cluste-demo-clust-1b575f8d-fb77-486a-8a13-af5a2a0f78ae is not authorized to perform: vpc-lattice-svcs:Invoke on resource: arn:aws:vpc-lattice:eu-west-1:12345678910:service/svc-002349360ddc5a463/ because no service-based policy allows the vpc-lattice-svcs:Invoke action\n</code></pre> <p>This is because in the VPC lattice IAMAuth Policy of Application1 we only authorize call from namespace apps from cluster2:</p> <pre><code>kubectl --context eks-cluster1 get IAMAuthPolicy -n apps demo-cluster1-iam-auth-policy  -o json | jq \".spec.policy | fromjson\"\n</code></pre> <pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"arn:aws:iam::12345678910:root\"\n      },\n      \"Action\": \"vpc-lattice-svcs:Invoke\",\n      \"Resource\": \"*\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"aws:PrincipalTag/eks-cluster-name\": \"eks-cluster2\",\n          \"aws:PrincipalTag/kubernetes-namespace\": \"apps\"\n        }\n      }\n    }\n  ]\n}\n</code></pre> <ol> <li>From cluster2 app2, call cluster1 app1 -&gt; success</li> </ol> <p>We can also validate the reverse flow, from cluster2 to cluster1:</p> <pre><code>kubectl --context eks-cluster2 exec -ti -n apps deployments/demo-cluster2-v1 -c demo-cluster2-v1 -- curl demo-cluster1.example.com\n</code></pre> <pre><code>Requsting to Pod(demo-cluster1-v1-6d7558f5b4-zk5cg): Hello from demo-cluster1-v1\n</code></pre> <ol> <li>From cluster1 app2, call cluster1 app2 -&gt; forbidden</li> </ol> <pre><code>kubectl --context eks-cluster2 exec -ti -n apps deployments/demo-cluster2-v1 -c demo-cluster2-v1 -- curl demo-cluster2.example.com\n</code></pre> <pre><code>AccessDeniedException: User: arn:aws:sts::12345678910:assumed-role/vpc-lattice-sigv4-client/eks-eks-cluste-demo-clust-a5c2432b-b84a-492f-8cbc-16f1fa5053eb is not authorized to perform: vpc-lattice-svcs:Invoke on resource: arn:aws:vpc-lattice:eu-west-1:12345678910:service/svc-00b57f32ed0a7b7c3/ because no service-based policy allows the vpc-lattice-svcs:Invoke action\n</code></pre>"},{"location":"patterns/network/cross-cluster-pod-communication/#important","title":"Important","text":"<p>In this setup, we used a Kyverno rule to inject iptables rules, and an envoy sidecar proxy into our application pod: - The iptables rule will route traffic from our application to the envoy proxy (the rule don't apply if source process gid is 0, so we provide a different gid for the application: <code>runAsGroup: 1000</code>). - The envoy proxy retrieve our Private CA certificate on startup, and install it so that it trust our VPC lattice service, through it's startup script:   <pre><code>kubectl  --context eks-cluster1 exec -it deploy/demo-cluster1-v1 -c envoy-sigv4 -n apps -- cat /usr/local/bin/launch_envoy.sh\n</code></pre></p> <p>Output:   <pre><code>#!/bin/sh\n\n# Copyright Amazon.com, Inc. or its affiliates. All Rights Reserved.\n# SPDX-License-Identifier: MIT-0\n\ncat /etc/envoy/envoy.yaml.in | envsubst \\$AWS_REGION,\\$JWT_AUDIENCE,\\$JWT_JWKS,\\$JWT_ISSUER,\\$JWKS_HOST,\\$APP_DOMAIN &gt; /etc/envoy/envoy.yaml\naws acm-pca get-certificate-authority-certificate --certificate-authority-arn $CA_ARN --region $AWS_REGION --output text &gt; /etc/pki/ca-trust/source/anchors/internal.pem\nupdate-ca-trust extract\n\ncat /etc/envoy/envoy.yaml\n/usr/local/bin/envoy --base-id 1 -l trace -c /etc/envoy/envoy.yaml\n</code></pre></p> <ul> <li>Then envoy sign the request with sigv4, and proxy it in https to the targeted service.</li> </ul> <p>You can find here the Dockerfiles used to create the envoy proxy, the iptables and the http-server images used in this demo.</p>"},{"location":"patterns/network/cross-cluster-pod-communication/#cleanup","title":"Cleanup","text":""},{"location":"patterns/network/cross-cluster-pod-communication/#1-we-start-by-deleting-the-cluster2-terraform-stack","title":"1. We start by deleting the cluster2 terraform stack.","text":"<p>Note that we need to do this in order, so that our kubernetes controllers will be able to clean external resources before deleting the controller, and kubernetes nodes.</p> <pre><code>./destroy.sh cluster2\n</code></pre> <p>If the VPC was not able to destroy, you may want to re-run the destroy command a second time</p>"},{"location":"patterns/network/cross-cluster-pod-communication/#2-we-can-then-delete-the-cluster1-terraform-stack","title":"2. We can then delete the cluster1 terraform stack.","text":"<pre><code>./destroy.sh cluster1\n</code></pre> <p>If the VPC was not able to destroy, you may want to re-run the destroy command a second time</p> <p>If the VPC lattice service network still exists, you can remove it with the following command:</p> <pre><code>SN=$(aws vpc-lattice list-service-networks --query 'items[?name==`lattice-gateway`].id' --output text)\nif [ -n \"$SN\" ]; then\n    aws vpc-lattice delete-service-network --service-network-id \"$SN\"\nfi\n</code></pre>"},{"location":"patterns/network/cross-cluster-pod-communication/#3-finally-delete-the-environment-terraform-stack","title":"3. Finally delete the environment terraform stack","text":"<pre><code>cd ../environment\nterraform destroy -auto-approve\n</code></pre>"},{"location":"patterns/network/ipv6-eks-cluster/","title":"IPv6 Networking","text":""},{"location":"patterns/network/ipv6-eks-cluster/#amazon-eks-cluster-w-ipv6-networking","title":"Amazon EKS Cluster w/ IPv6 Networking","text":"<p>This pattern demonstrates an EKS cluster that utilizes IPv6 networking.</p>"},{"location":"patterns/network/ipv6-eks-cluster/#deploy","title":"Deploy","text":"<p>See here for the prerequisites and steps to deploy this pattern.</p>"},{"location":"patterns/network/ipv6-eks-cluster/#validate","title":"Validate","text":"<ol> <li> <p>Test by listing all the pods running currently; the <code>IP</code> should be an IPv6 address.</p> <pre><code>kubectl get pods -A -o wide\n</code></pre> <pre><code># Output should look like below\nNAMESPACE     NAME                       READY   STATUS    RESTARTS   AGE     IP                                       NODE                                        NOMINATED NODE   READINESS GATES\nkube-system   aws-node-1hd2s             1/1     Running   0          3m5s    2600:1f13:6c4:a703:ecf8:3ac1:76b0:9303   ip-10-0-10-183.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\nkube-system   aws-node-2mdg             1/1     Running   0          3m21s   2600:1f13:6c4:a705:a929:f8d4:9350:1b20   ip-10-0-12-188.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\nkube-system   coredns-799c5565b4-6xrc   1/1     Running   0          10m     2600:1f13:6c4:a705:1bda::                ip-10-0-12-188.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\nkube-system   coredns-799c5565b4-fjq4q   1/1     Running   0          10m     2600:1f13:6c4:a705:1bda::1               ip-10-0-12-188.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\nkube-system   kube-proxy-58tp7           1/1     Running   0          4m25s   2600:1f13:6c4:a703:ecf8:3ac1:76b0:9303   ip-10-0-10-183.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\nkube-system   kube-proxy-3kgw           1/1     Running   0          4m25s   2600:1f13:6c4:a705:a929:f8d4:9350:1b20   ip-10-0-12-188.us-west-2.compute.internal   &lt;none&gt;           &lt;none&gt;\n</code></pre> </li> <li> <p>Test by listing all the nodes running currently; the <code>INTERNAL-IP</code> should be an IPv6 address.</p> <pre><code>kubectl nodes -A -o wide\n</code></pre> <pre><code># Output should look like below\nNAME                                        STATUS   ROLES    AGE     VERSION               INTERNAL-IP                              EXTERNAL-IP   OS-IMAGE         KERNEL-VERSION                 CONTAINER-RUNTIME\nip-10-0-10-183.us-west-2.compute.internal   Ready    &lt;none&gt;   4m57s   v1.24.7-eks-fb459a0   2600:1f13:6c4:a703:ecf8:3ac1:76b0:9303   &lt;none&gt;        Amazon Linux 2   5.4.226-129.415.amzn2.x86_64   containerd://1.6.6\nip-10-0-12-188.us-west-2.compute.internal   Ready    &lt;none&gt;   4m57s   v1.24.7-eks-fb459a0   2600:1f13:6c4:a705:a929:f8d4:9350:1b20   &lt;none&gt;        Amazon Linux 2   5.4.226-129.415.amzn2.x86_64   containerd://1.6.6\n</code></pre> </li> </ol>"},{"location":"patterns/network/ipv6-eks-cluster/#destroy","title":"Destroy","text":"<pre><code>terraform destroy -target=\"module.eks_blueprints_addons\" -auto-approve\nterraform destroy -target=\"module.eks\" -auto-approve\nterraform destroy -auto-approve\n</code></pre> <p>See here for more details on cleaning up the resources created.</p>"},{"location":"patterns/network/private-public-ingress/","title":"Private and Public Ingress","text":""},{"location":"patterns/network/private-public-ingress/#amazon-eks-private-and-public-ingress-example","title":"Amazon EKS Private and Public Ingress example","text":"<p>This example demonstrates how to provision an Amazon EKS cluster with two  ingress-nginx controllers; one to expose applications publicly and the other to expose applications internally. It also assigns security groups to the Network Load Balancers used to expose the internal and external ingress controllers.</p> <p>This solution:</p> <ul> <li>Installs an ingress-nginx controller for public traffic</li> <li>Installs an ingress-nginx controller for internal traffic</li> </ul> <p>To expose your application services via an <code>Ingress</code> resource with this solution you can set the respective <code>ingressClassName</code> as either <code>ingress-nginx-external</code> or <code>ingress-nginx-internal</code>.</p> <p>Refer to the documentation for <code>AWS Load Balancer controller</code> configuration options.</p>"},{"location":"patterns/network/private-public-ingress/#deploy","title":"Deploy","text":"<p>See here for the prerequisites and steps to deploy this pattern.</p>"},{"location":"patterns/network/private-public-ingress/#validate","title":"Validate","text":"<p>TODO</p> <p>Add in validation steps</p>"},{"location":"patterns/network/private-public-ingress/#destroy","title":"Destroy","text":"<pre><code>terraform destroy -target=\"module.eks_blueprints_addons\" -auto-approve\nterraform destroy -target=\"module.eks\" -auto-approve\nterraform destroy -auto-approve\n</code></pre> <p>See here for more details on cleaning up the resources created.</p>"},{"location":"patterns/network/privatelink-access/","title":"PrivateLink Access","text":""},{"location":"patterns/network/privatelink-access/#private-eks-cluster-access-via-aws-privatelink","title":"Private EKS cluster access via AWS PrivateLink","text":"<p>This pattern demonstrates how to access a private EKS cluster using AWS PrivateLink.</p> <p>Refer to the documentation for further details on  <code>AWS PrivateLink</code>.</p>"},{"location":"patterns/network/privatelink-access/#deploy","title":"Deploy","text":"<p>See here for the prerequisites and follow the steps below to deploy this pattern.</p> <pre><code>terraform init\nterraform apply -target=module.eventbridge -target=module.nlb --auto-approve\nterraform apply --auto-approve\n</code></pre>"},{"location":"patterns/network/privatelink-access/#test-access-to-eks-kubernetes-api-server-endpoint","title":"Test access to EKS Kubernetes API server endpoint","text":"<p>Of the output values, the value <code>ssm_test</code> is provided to aid in quickly testing the connectivity from the client EC2 instance to the private EKS cluster via AWS PrivateLink. Copy the output value, which looks like the snippet shown below (as an example) and paste it into your terminal to execute and check the connectivity. If configured correctly, the value returned should be <code>ok</code>.</p> <pre><code>COMMAND=\"curl -ks https://9A85B21811733524E3ABCDFEA8714642.gr7.us-west-2.eks.amazonaws.com/readyz\"\n\nCOMMAND_ID=$(aws ssm send-command --region us-west-2 \\\n   --document-name \"AWS-RunShellScript\" \\\n   --parameters \"commands=[$COMMAND]\" \\\n   --targets \"Key=instanceids,Values=i-0a45eff73ba408575\" \\\n   --query 'Command.CommandId' \\\n   --output text)\n\naws ssm get-command-invocation --region us-west-2 \\\n   --command-id $COMMAND_ID \\\n   --instance-id i-0a45eff73ba408575 \\\n   --query 'StandardOutputContent' \\\n   --output text\n</code></pre>"},{"location":"patterns/network/privatelink-access/#test-access-to-eks-kubernetes-api-with-kubectl","title":"Test access to EKS Kubernetes API with <code>kubectl</code>","text":"<p>Perform the following steps to access the cluster with <code>kubectl</code> from the provided Client EC2 instance.</p>"},{"location":"patterns/network/privatelink-access/#log-into-the-client-ec2-instance","title":"Log into the Client EC2 instance","text":"<p>Start a new SSM session on the Client EC2 instance using the provided <code>ssm_start_session</code> output value. It should look similar to the snippet shown below. Copy the output value and paste it into your terminal to execute. Your terminal will now be connected to the Client EC2 instance.</p> <pre><code>aws ssm start-session --region us-west-2 --target i-0280cf604085f4a44\n</code></pre>"},{"location":"patterns/network/privatelink-access/#update-kubeconfig","title":"Update Kubeconfig","text":"<p>On the Client EC2 machine, run the following command to update the local <code>~/.kube/config</code> file to enable access to the cluster:</p> <pre><code>aws eks update-kubeconfig --region us-west-2 --name privatelink-access\n</code></pre>"},{"location":"patterns/network/privatelink-access/#test-complete-access-with-kubectl","title":"Test complete access with <code>kubectl</code>","text":"<p>Test access by listing the pods running on the cluster:</p> <pre><code>kubectl get pods -A\n</code></pre> <pre><code>NAMESPACE     NAME                       READY   STATUS    RESTARTS   AGE\nkube-system   aws-node-4f8g8             1/1     Running   0          1m\nkube-system   coredns-6ff9c46cd8-59sqp   1/1     Running   0          1m\nkube-system   coredns-6ff9c46cd8-1npb   1/1     Running   0          2m\nkube-system   kube-proxy-mm2zc           1/1     Running   0          1m\n</code></pre>"},{"location":"patterns/network/privatelink-access/#destroy","title":"Destroy","text":"<pre><code>terraform destroy -target=\"module.eks_blueprints_addons\" -auto-approve\nterraform destroy -target=\"module.eks\" -auto-approve\nterraform destroy -auto-approve\n</code></pre> <p>See here for more details on cleaning up the resources created.</p>"},{"location":"patterns/network/tls-with-aws-pca-issuer/","title":"TLS w/ AWS PCA Issuer","text":""},{"location":"patterns/network/tls-with-aws-pca-issuer/#tls-with-aws-pca-issuer","title":"TLS with AWS PCA Issuer","text":"<p>This pattern demonstrates how to enable TLS with AWS PCA issuer on an Amazon EKS cluster.</p>"},{"location":"patterns/network/tls-with-aws-pca-issuer/#deploy","title":"Deploy","text":"<p>See here for the prerequisites and steps to deploy this pattern.</p>"},{"location":"patterns/network/tls-with-aws-pca-issuer/#validate","title":"Validate","text":"<ol> <li> <p>List all the pods running in <code>aws-privateca-issuer</code> and <code>cert-manager</code> Namespace.</p> <pre><code>kubectl get pods -n aws-privateca-issuer\nkubectl get pods -n cert-manager\n</code></pre> </li> <li> <p>View the <code>certificate</code> status in the <code>default</code> Namespace. It should be in <code>Ready</code> state, and be pointing to a <code>secret</code> created in the same Namespace.</p> <pre><code>kubectl get certificate -o wide\n</code></pre> <pre><code>NAME      READY   SECRET                  ISSUER                    STATUS                                          AGE\nexample   True    example-clusterissuer   tls-with-aws-pca-issuer   Certificate is up to date and has not expired   41m\n</code></pre> <pre><code>kubectl get secret example-clusterissuer\n</code></pre> <pre><code>NAME                    TYPE                DATA   AGE\nexample-clusterissuer   kubernetes.io/tls   3      43m\n</code></pre> </li> </ol>"},{"location":"patterns/network/tls-with-aws-pca-issuer/#destroy","title":"Destroy","text":"<pre><code>terraform destroy -target=\"module.eks_blueprints_addons\" -auto-approve\nterraform destroy -target=\"module.eks\" -auto-approve\nterraform destroy -auto-approve\n</code></pre> <p>See here for more details on cleaning up the resources created.</p>"},{"location":"patterns/network/wireguard-with-cilium/","title":"Wireguard /w Cilium","text":""},{"location":"patterns/network/wireguard-with-cilium/#transparent-encryption-with-cilium-and-wireguard","title":"Transparent Encryption with Cilium and Wireguard","text":"<p>This pattern demonstrates Cilium configured in CNI chaining mode with the VPC CNI and with Wireguard transparent encryption enabled on an Amazon EKS cluster.</p> <ul> <li>Cilium CNI Chaining Documentation</li> <li>Cilium Wireguard Encryption Documentation</li> </ul>"},{"location":"patterns/network/wireguard-with-cilium/#focal-points","title":"Focal Points","text":"<ul> <li><code>eks.tf</code> contains the cluster configuration and the deployment of Cilium.<ul> <li>There are no specific requirements from an EKS perspective, other than the Linux Kernel version used by the OS must be 5.10+</li> </ul> </li> <li><code>example.yaml</code> provides a sample application used to demonstrate the encrypted connectivity. This is optional and not required for the pattern.</li> </ul>"},{"location":"patterns/network/wireguard-with-cilium/#deploy","title":"Deploy","text":"<p>See here for the prerequisites and steps to deploy this pattern.</p>"},{"location":"patterns/network/wireguard-with-cilium/#validate","title":"Validate","text":"<ol> <li> <p>Deploy the example pods:</p> <pre><code>kubectl apply --server-side -f example.yaml\n</code></pre> <pre><code>pod/server created\nservice/server created\npod/client created\n</code></pre> </li> <li> <p>Get the Cilium status from one of the Cilium pods.</p> <p>Under the <code>Encryption</code> field, it should state <code>Wireguard</code> with a PubKey. <code>NodeEncryption: Disabled</code> is expected since <code>NodeEncryption</code> was not enabled via the Helm values provided.</p> <pre><code>kubectl -n kube-system exec -ti ds/cilium -- cilium status\n</code></pre> <pre><code>Defaulted container \"cilium-agent\" out of: cilium-agent, config (init), mount-cgroup (init), apply-sysctl-overwrites (init), mount-bpf-fs (init), clean-cilium-state (init), install-cni-binaries (init)\nKVStore:                 Ok   Disabled\nKubernetes:              Ok   1.28+ (v1.28.1-eks-43840fb) [linux/amd64]\nKubernetes APIs:         [\"EndpointSliceOrEndpoint\", \"cilium/v2::CiliumClusterwideNetworkPolicy\", \"cilium/v2::CiliumEndpoint\", \"cilium/v2::CiliumNetworkPolicy\", \"cilium/v2::CiliumNode\", \"cilium/v2alpha1::CiliumCIDRGroup\", \"core/v1::Namespace\", \"core/v1::Pods\", \"core/v1::Service\", \"networking.k8s.io/v1::NetworkPolicy\"]\nKubeProxyReplacement:    False   [eth0 10.0.21.109 (Direct Routing), eth1 10.0.27.0]\nHost firewall:           Disabled\nCNI Chaining:            aws-cni\nCilium:                  Ok   1.14.2 (v1.14.2-a6748946)\nNodeMonitor:             Listening for events on 2 CPUs with 64x4096 of shared memory\nCilium health daemon:    Ok\nIPAM:                    IPv4: 1/254 allocated from 10.0.0.0/24,\nIPv4 BIG TCP:            Disabled\nIPv6 BIG TCP:            Disabled\nBandwidthManager:        Disabled\nHost Routing:            Legacy\nMasquerading:            Disabled\nController Status:       24/24 healthy\nProxy Status:            No managed proxy redirect\nGlobal Identity Range:   min 256, max 65535\nHubble:                  Ok          Current/Max Flows: 410/4095 (10.01%), Flows/s: 1.59   Metrics: Disabled\nEncryption:              Wireguard   [NodeEncryption: Disabled, cilium_wg0 (Pubkey: /yuqsZyG91AzVIkZ3AIq8qjQ0gGKQd6GWcRYh4LYpko=, Port: 51871, Peers: 1)]\nCluster health:                      Probe disabled\n</code></pre> </li> <li> <p>Open a shell inside the cilium container</p> <pre><code>kubectl -n kube-system exec -ti ds/cilium -- bash\n</code></pre> </li> <li> <p>Install <code>tcpdump</code></p> <pre><code>apt-get update\napt-get install -y tcpdump\n</code></pre> </li> <li> <p>Start a packet capture on <code>cilium_wg0</code> and verify you see payload in clear text, it means the traffic is encrypted with wireguard</p> <pre><code>tcpdump -A -c 40 -i cilium_wg0 | grep \"Welcome to nginx!\"\n</code></pre> <pre><code>tcpdump: verbose output suppressed, use -v[v]... for full protocol decode\nlistening on cilium_wg0, link-type RAW (Raw IP), snapshot length 262144 bytes\n&lt;title&gt;Welcome to nginx!&lt;/title&gt;\n&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;\n40 packets captured\n40 packets received by filter\n0 packets dropped by kernel\n</code></pre> <p>Exit</p> <p>Exit the container shell by typing <code>exit</code> before continuing to next step</p> </li> <li> <p>Deploy the Cilium connectivity resources to check and evaluate connectivity:</p> <pre><code>kubectl create ns cilium-test\nkubectl apply --server-side -n cilium-test -f https://raw.githubusercontent.com/cilium/cilium/v1.14.1/examples/kubernetes/connectivity-check/connectivity-check.yaml\n</code></pre> <pre><code>deployment.apps/echo-a created\ndeployment.apps/echo-b created\ndeployment.apps/echo-b-host created\ndeployment.apps/pod-to-a created\ndeployment.apps/pod-to-external-1111 created\ndeployment.apps/pod-to-a-denied-cnp created\ndeployment.apps/pod-to-a-allowed-cnp created\ndeployment.apps/pod-to-external-fqdn-allow-google-cnp created\ndeployment.apps/pod-to-b-multi-node-clusterip created\ndeployment.apps/pod-to-b-multi-node-headless created\ndeployment.apps/host-to-b-multi-node-clusterip created\ndeployment.apps/host-to-b-multi-node-headless created\ndeployment.apps/pod-to-b-multi-node-nodeport created\ndeployment.apps/pod-to-b-intra-node-nodeport created\nservice/echo-a created\nservice/echo-b created\nservice/echo-b-headless created\nservice/echo-b-host-headless created\nciliumnetworkpolicy.cilium.io/pod-to-a-denied-cnp created\nciliumnetworkpolicy.cilium.io/pod-to-a-allowed-cnp created\nciliumnetworkpolicy.cilium.io/pod-to-external-fqdn-allow-google-cnp created\n</code></pre> </li> <li> <p>View the logs of any of the connectivity tests to view the results:</p> <pre><code>kubectl logs &lt;cilium test pod&gt; -n cilium-test\n</code></pre> <pre><code>\\{^_^}/ hi!\n\nLoading /default.json\nDone\n\nResources\nhttp://:8080/private\nhttp://:8080/public\n\nHome\nhttp://:8080\n\nType s + enter at any time to create a snapshot of the database\nWatching...\n\nGET /public 200 7.063 ms - 57\nGET /public 200 3.126 ms - 57\nGET /public 200 3.039 ms - 57\nGET /public 200 2.776 ms - 57\nGET /public 200 3.087 ms - 57\nGET /public 200 2.781 ms - 57\nGET /public 200 2.977 ms - 57\nGET /public 200 2.596 ms - 57\nGET /public 200 2.991 ms - 57\nGET /public 200 2.708 ms - 57\nGET /public 200 3.066 ms - 57\nGET /public 200 2.616 ms - 57\nGET /public 200 2.875 ms - 57\nGET /public 200 2.689 ms - 57\nGET /public 200 2.800 ms - 57\nGET /public 200 2.556 ms - 57\nGET /public 200 3.238 ms - 57\nGET /public 200 2.538 ms - 57\nGET /public 200 2.890 ms - 57\nGET /public 200 2.666 ms - 57\nGET /public 200 2.729 ms - 57\nGET /public 200 2.580 ms - 57\nGET /public 200 2.919 ms - 57\nGET /public 200 2.630 ms - 57\nGET /public 200 2.857 ms - 57\nGET /public 200 2.716 ms - 57\nGET /public 200 1.693 ms - 57\nGET /public 200 2.715 ms - 57\nGET /public 200 2.729 ms - 57\nGET /public 200 2.655 ms - 57\n</code></pre> </li> </ol>"},{"location":"patterns/network/wireguard-with-cilium/#destroy","title":"Destroy","text":"<pre><code>terraform destroy -target=\"module.eks_blueprints_addons\" -auto-approve\nterraform destroy -target=\"module.eks\" -auto-approve\nterraform destroy -auto-approve\n</code></pre> <p>See here for more details on cleaning up the resources created.</p>"},{"location":"snippets/ipv4-prefix-delegation/","title":"IPv4 Prefix Delegation","text":"<p>The configuration snippet below shows how to enable prefix delegation to increase the number of available IP addresses on the provisioned EC2 nodes.</p> <ul> <li>Documentation</li> <li>Blog post</li> </ul>"},{"location":"snippets/ipv4-prefix-delegation/#vpc-cni-configuration","title":"VPC CNI Configuration","text":"<p>In this example, the <code>vpc-cni</code> addon is configured using <code>before_compute = true</code>. This is done to ensure the <code>vpc-cni</code> is created and updated before any EC2 instances are created so that the desired settings have applied before they will be referenced. With this configuration, you will now see that nodes created will have <code>--max-pods 110</code> configured do to the use of prefix delegation being enabled on the <code>vpc-cni</code>.</p> <p>If you find that your nodes are not being created with the correct number of max pods (i.e. - for <code>m5.large</code>, if you are seeing a max pods of 29 instead of 110), most likely the <code>vpc-cni</code> was not configured before the EC2 instances.</p> <pre><code>module \"eks\" {\n  source  = \"terraform-aws-modules/eks/aws\"\n\n  # Truncated for brevity\n  ...\n\n  cluster_addons = {\n    vpc-cni = {\n      before_compute = true\n      most_recent    = true # To ensure access to the latest settings provided\n      configuration_values = jsonencode({\n        env = {\n          ENABLE_PREFIX_DELEGATION = \"true\"\n          WARM_PREFIX_TARGET       = \"1\"\n        }\n      })\n    }\n  }\n\n  ...\n}\n</code></pre> <p>When enabled, inspect one of the <code>aws-node-*</code> (AWS VPC CNI) pods to ensure prefix delegation is enabled and warm prefix target is 1:</p> <pre><code>kubectl describe ds -n kube-system aws-node | grep ENABLE_PREFIX_DELEGATION: -A 3\n</code></pre> <p>Output should look similar to below (truncated for brevity):</p> <pre><code>    ENABLE_PREFIX_DELEGATION:               true # &lt;- this should be set to true\n    WARM_ENI_TARGET:                        1\n    WARM_PREFIX_TARGET:                     1 # &lt;- this should be set to 1\n    ...\n</code></pre>"},{"location":"snippets/vpc-cni-custom-networking/","title":"VPC CNI Custom Networking","text":"<p>Custom networking addresses the IP exhaustion issue by assigning the node and Pod IPs from secondary VPC address spaces (CIDR). Custom networking support supports ENIConfig custom resource. The ENIConfig includes an alternate subnet CIDR range (carved from a secondary VPC CIDR), along with the security group(s) that the Pods will belong to. When custom networking is enabled, the VPC CNI creates secondary ENIs in the subnet defined under ENIConfig. The CNI assigns Pods an IP addresses from a CIDR range defined in a ENIConfig CRD.</p> <p>Since the primary ENI is not used by custom networking, the maximum number of Pods you can run on a node is lower. The host network Pods continue to use IP address assigned to the primary ENI. Additionally, the primary ENI is used to handle source network translation and route Pods traffic outside the node.</p> <ul> <li>Documentation</li> <li>Best Practices Guide</li> </ul>"},{"location":"snippets/vpc-cni-custom-networking/#vpc-cni-configuration","title":"VPC CNI Configuration","text":"<p>In this example, the <code>vpc-cni</code> addon is configured using <code>before_compute = true</code>. This is done to ensure the <code>vpc-cni</code> is created and updated before any EC2 instances are created so that the desired settings have applied before they will be referenced. With this configuration, you will now see that nodes created will have <code>--max-pods 110</code> configured do to the use of prefix delegation being enabled on the <code>vpc-cni</code>.</p> <p>If you find that your nodes are not being created with the correct number of max pods (i.e. - for <code>m5.large</code>, if you are seeing a max pods of 29 instead of 110), most likely the <code>vpc-cni</code> was not configured before the EC2 instances.</p>"},{"location":"snippets/vpc-cni-custom-networking/#components","title":"Components","text":"<p>To enable VPC CNI custom networking, you must configuring the following components:</p> <ol> <li> <p>Create a VPC with additional CIDR block associations. These additional CIDR blocks will be used to create subnets for the VPC CNI custom networking:</p> <pre><code>module \"vpc\" {\nsource  = \"terraform-aws-modules/vpc/aws\"\n\n# Truncated for brevity\n...\n\nsecondary_cidr_blocks = [local.secondary_vpc_cidr] # can add up to 5 total CIDR blocks\n\nazs = local.azs\nprivate_subnets = concat(\n   [for k, v in local.azs : cidrsubnet(local.vpc_cidr, 4, k)],\n   [for k, v in local.azs : cidrsubnet(local.secondary_vpc_cidr, 2, k)]\n)\n\n...\n}\n</code></pre> </li> <li> <p>Specify the VPC CNI custom networking configuration in the <code>vpc-cni</code> addon configuration:</p> <pre><code>module \"eks\" {\nsource  = \"terraform-aws-modules/eks/aws\"\n\n# Truncated for brevity\n...\n\ncluster_addons = {\n   vpc-cni = {\n      before_compute = true\n      most_recent    = true # To ensure access to the latest settings provided\n      configuration_values = jsonencode({\n      env = {\n         AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG = \"true\"\n         ENI_CONFIG_LABEL_DEF               = \"topology.kubernetes.io/zone\"\n      })\n   }\n}\n\n...\n}\n</code></pre> </li> <li> <p>Create the <code>ENIConfig</code> custom resource for each subnet that you want to deploy pods into:</p> <pre><code>resource \"kubectl_manifest\" \"eni_config\" {\nfor_each = zipmap(local.azs, slice(module.vpc.private_subnets, 3, 6))\n\nyaml_body = yamlencode({\n   apiVersion = \"crd.k8s.amazonaws.com/v1alpha1\"\n   kind       = \"ENIConfig\"\n   metadata = {\n      name = each.key\n   }\n   spec = {\n      securityGroups = [\n      module.eks.node_security_group_id,\n      ]\n      subnet = each.value\n   }\n})\n}\n</code></pre> </li> </ol> <p>Once those settings have been successfully applied, you can verify if custom networking is enabled correctly by inspecting one of the <code>aws-node-*</code> (AWS VPC CNI) pods:</p> <pre><code>kubectl describe pod aws-node-ttg4h -n kube-system\n\n# Output should look similar below (truncated for brevity)\n  Environment:\n    ADDITIONAL_ENI_TAGS:                    {}\n    AWS_VPC_CNI_NODE_PORT_SUPPORT:          true\n    AWS_VPC_ENI_MTU:                        9001\n    AWS_VPC_K8S_CNI_CONFIGURE_RPFILTER:     false\n    AWS_VPC_K8S_CNI_CUSTOM_NETWORK_CFG:     true # &lt;- this should be set to true\n    AWS_VPC_K8S_CNI_EXTERNALSNAT:           false\n    AWS_VPC_K8S_CNI_LOGLEVEL:               DEBUG\n    ...\n</code></pre>"},{"location":"v4-to-v5/addons/","title":"Migrate to EKS Blueprints Addons Module","text":"<p>Please consult the docs/v4-to-v5/example directory for reference configurations. If you find a bug, please open an issue with supporting configuration to reproduce.</p>"},{"location":"v4-to-v5/addons/#this-guide-is-under-active-development","title":"\u26a0\ufe0f This guide is under active development.","text":""},{"location":"v4-to-v5/addons/#list-of-backwards-incompatible-changes","title":"List of backwards incompatible changes","text":"<p>-</p>"},{"location":"v4-to-v5/addons/#additional-changes","title":"Additional changes","text":""},{"location":"v4-to-v5/addons/#added","title":"Added","text":"<p>-</p>"},{"location":"v4-to-v5/addons/#modified","title":"Modified","text":"<p>-</p>"},{"location":"v4-to-v5/addons/#removed","title":"Removed","text":"<p>-</p>"},{"location":"v4-to-v5/addons/#variable-and-output-changes","title":"Variable and output changes","text":"<ol> <li> <p>Removed variables:</p> <p>-</p> </li> <li> <p>Renamed variables:</p> <p>-</p> </li> <li> <p>Added variables:</p> <p>-</p> </li> <li> <p>Removed outputs:</p> <p>-</p> </li> <li> <p>Renamed outputs:</p> <p>-</p> </li> <li> <p>Added outputs:</p> <p>-</p> </li> </ol>"},{"location":"v4-to-v5/addons/#upgrade-migrations","title":"Upgrade Migrations","text":""},{"location":"v4-to-v5/addons/#before-v4x-example","title":"Before - v4.x Example","text":"<pre><code>module \"eks_blueprints_addons\" {\n  source = \"github.com/aws-ia/terraform-aws-eks-blueprints//modules/kubernetes-addons?ref=v4.32.1\"\n\n  eks_cluster_id       = module.eks.cluster_name\n  eks_cluster_endpoint = module.eks.cluster_endpoint\n  eks_oidc_provider    = module.eks.oidc_provider\n  eks_cluster_version  = module.eks.cluster_version\n\n  # TODO\n\n}\n</code></pre>"},{"location":"v4-to-v5/addons/#after-v5x-example","title":"After - v5.x Example","text":"<pre><code>module \"eks_blueprints_addons\" {\n  source  = \"aws-ia/eks-blueprints-addons/aws\"\n  version = \"~&gt; 1.0\"\n\n  cluster_name      = module.eks.cluster_name\n  cluster_endpoint  = module.eks.cluster_endpoint\n  cluster_version   = module.eks.cluster_version\n  oidc_provider_arn = module.eks.oidc_provider_arn\n\n  # TODO\n\n}\n</code></pre>"},{"location":"v4-to-v5/addons/#diff-of-before-vs-after","title":"Diff of Before vs After","text":"<pre><code>module \"eks_blueprints_addons\" {\n-  source = \"github.com/aws-ia/terraform-aws-eks-blueprints//modules/kubernetes-addons?ref=v4.32.1\"\n+  source  = \"aws-ia/eks-blueprints-addons/aws\"\n+  version = \"~&gt; 1.0\"\n\n  # TODO\n}\n</code></pre>"},{"location":"v4-to-v5/addons/#state-move-commands","title":"State Move Commands","text":"<p>In conjunction with the changes above, users can elect to move their external capacity provider(s) under this module using the following move command. Command is shown using the values from the example shown above, please update to suit your configuration names:</p> <pre><code>terraform state mv 'xxx' 'yyy'\n</code></pre>"},{"location":"v4-to-v5/cluster/","title":"Migrate to EKS Module v19.x","text":"<p>Please consult the docs/v4-to-v5/example directory for reference configurations. If you find a bug, please open an issue with supporting configuration to reproduce.</p>"},{"location":"v4-to-v5/cluster/#backwards-incompatible-changes","title":"Backwards incompatible changes","text":"<ul> <li>The cluster module provided in EKS Blueprints is being removed entirely from the project. Instead, users are encouraged to use the <code>terraform-aws-eks</code> module for creating and managing their EKS cluster in Terraform.</li> <li>The KMS module provided in EKS Blueprints has been removed. Users can leverage the KMS creation/management functionality provided by the <code>terraform-aws-eks</code> module or utilize the standalone <code>terraform-aws-kms</code> module.</li> <li>The EMR on EKS module provided in EKS Blueprints has been removed. Instead, users are encouraged to use the <code>terraform-aws-emr</code> virtual cluster sub-module for creating and managing their EMR on EKS virtual cluster in Terraform.</li> <li>The teams multi-tenancy module provided in EKS Blueprints has been removed. Instead, users are encouraged to use the <code>terraform-aws-eks-blueprints-teams</code> module for creating and managing their multi-tenancy constructions within their EKS clusters in Terraform.</li> </ul>"},{"location":"v4-to-v5/cluster/#additional-changes","title":"Additional changes","text":""},{"location":"v4-to-v5/cluster/#added","title":"Added","text":"<ul> <li>N/A</li> </ul>"},{"location":"v4-to-v5/cluster/#modified","title":"Modified","text":"<ul> <li>N/A</li> </ul>"},{"location":"v4-to-v5/cluster/#removed","title":"Removed","text":"<ul> <li>All noted above under <code>Backwards incompatible changes</code></li> </ul>"},{"location":"v4-to-v5/cluster/#variable-and-output-changes","title":"Variable and output changes","text":"<p>Since the change is to replace the EKS Blueprints cluster module with the <code>terraform-aws-eks</code> module, there aren't technically any variable or output changes other than their removal. Please consult the <code>terraform-aws-eks</code> module for its respective variables/outputs.</p> <ol> <li> <p>Removed variables:</p> <ul> <li>All</li> </ul> </li> <li> <p>Renamed variables:</p> <ul> <li>None</li> </ul> </li> <li> <p>Added variables:</p> <ul> <li>None</li> </ul> </li> <li> <p>Removed outputs:</p> <ul> <li>All</li> </ul> </li> <li> <p>Renamed outputs:</p> <ul> <li>None</li> </ul> </li> <li> <p>Added outputs:</p> <ul> <li>None</li> </ul> </li> </ol>"},{"location":"v4-to-v5/cluster/#upgrade-migrations","title":"Upgrade Migrations","text":""},{"location":"v4-to-v5/cluster/#before-v432-example","title":"Before - v4.32 Example","text":"<pre><code>module \"eks\" {\n  source = \"github.com/aws-ia/terraform-aws-eks-blueprints?ref=v4.32.1\"\n\n  cluster_name    = local.name\n  cluster_version = \"1.26\"\n\n  vpc_id                          = module.vpc.vpc_id\n  private_subnet_ids              = module.vpc.private_subnets\n  cluster_endpoint_private_access = true\n\n  map_roles = [\n    {\n      rolearn  = data.aws_caller_identity.current.arn\n      username = \"me\"\n      groups   = [\"system:masters\"]\n    },\n  ]\n\n  managed_node_groups = {\n    managed = {\n      node_group_name = \"managed\"\n      instance_types  = [\"m5.large\"]\n\n      min_size     = 1\n      max_size     = 2\n      desired_size = 1\n\n      k8s_labels = {\n        Which = \"managed\"\n      }\n    }\n  }\n\n  fargate_profiles = {\n    fargate = {\n      fargate_profile_name = \"fargate\"\n      fargate_profile_namespaces = [{\n        namespace = \"default\"\n\n        k8s_labels = {\n          Which = \"fargate\"\n        }\n      }]\n      subnet_ids = module.vpc.private_subnets\n    }\n  }\n\n  self_managed_node_groups = {\n    self_managed = {\n      node_group_name    = \"self_managed\"\n      instance_type      = \"m5.large\"\n      launch_template_os = \"amazonlinux2eks\"\n\n      min_size     = 1\n      max_size     = 2\n      desired_size = 1\n\n      k8s_labels = {\n        Which = \"self-managed\"\n      }\n    }\n  }\n\n  tags = {\n    Blueprint  = local.name\n    GithubRepo = \"github.com/aws-ia/terraform-aws-eks-blueprints\"\n  }\n}\n</code></pre>"},{"location":"v4-to-v5/cluster/#after-v50-example","title":"After - v5.0 Example","text":"<p>Any of the values that are marked with <code># Backwards compat</code> are provided to demonstrate configuration level changes to reduce the number of Terraform changes when migrating to the EKS module.</p> <pre><code>module \"eks\" {\n  source  = \"terraform-aws-modules/eks/aws\"\n  version = \"~&gt; 19.13\"\n\n  cluster_name                   = local.name\n  cluster_version                = \"1.26\"\n  cluster_endpoint_public_access = true # Backwards compat\n  cluster_enabled_log_types      = [\"api\", \"audit\", \"authenticator\", \"controllerManager\", \"scheduler\"] # Backwards compat\n\n  iam_role_name            = \"${local.name}-cluster-role\" # Backwards compat\n  iam_role_use_name_prefix = false                        # Backwards compat\n\n  kms_key_aliases = [local.name] # Backwards compat\n\n  vpc_id     = module.vpc.vpc_id\n  subnet_ids = module.vpc.private_subnets\n\n  manage_aws_auth_configmap = true\n  aws_auth_roles = [\n    {\n      rolearn  = data.aws_caller_identity.current.arn\n      username = \"me\"\n      groups   = [\"system:masters\"]\n    },\n  ]\n\n  eks_managed_node_groups = {\n    managed = {\n      iam_role_name              = \"${local.name}-managed\" # Backwards compat\n      iam_role_use_name_prefix   = false                   # Backwards compat\n      use_custom_launch_template = false                   # Backwards compat\n\n      instance_types = [\"m5.large\"]\n\n      min_size     = 1\n      max_size     = 2\n      desired_size = 1\n\n      labels = {\n        Which = \"managed\"\n      }\n    }\n  }\n\n  fargate_profiles = {\n    fargate = {\n      iam_role_name            = \"${local.name}-fargate\" # Backwards compat\n      iam_role_use_name_prefix = false                   # Backwards compat\n\n      selectors = [{\n        namespace = \"default\"\n        labels = {\n          Which = \"fargate\"\n        }\n      }]\n    }\n  }\n\n  self_managed_node_groups = {\n    self_managed = {\n      name            = \"${local.name}-self_managed\" # Backwards compat\n      use_name_prefix = false                        # Backwards compat\n\n      iam_role_name            = \"${local.name}-self_managed\" # Backwards compat\n      iam_role_use_name_prefix = false                        # Backwards compat\n\n      launch_template_name            = \"self_managed-${local.name}\" # Backwards compat\n      launch_template_use_name_prefix = false                        # Backwards compat\n\n      instance_type = \"m5.large\"\n\n      min_size     = 1\n      max_size     = 2\n      desired_size = 1\n\n      labels = {\n        Which = \"self-managed\"\n      }\n    }\n  }\n\n  tags = {\n    Blueprint  = local.name\n    GithubRepo = \"github.com/aws-ia/terraform-aws-eks-blueprints\"\n  }\n}\n</code></pre>"},{"location":"v4-to-v5/cluster/#diff-of-before-vs-after","title":"Diff of Before vs After","text":"<pre><code>module \"eks\" {\n-  source = \"github.com/aws-ia/terraform-aws-eks-blueprints?ref=v4.32.1\"\n+  source  = \"terraform-aws-modules/eks/aws\"\n+  version = \"~&gt; 19.13\"\n\n  cluster_name    = local.name\n  cluster_version = \"1.26\"\n\n  vpc_id                          = module.vpc.vpc_id\n  private_subnet_ids              = module.vpc.private_subnets\n+  cluster_endpoint_public_access  = true\n-  cluster_endpoint_private_access = true\n\n-  map_roles = [\n+  aws_auth_roles = [\n    {\n      rolearn  = data.aws_caller_identity.current.arn\n      username = \"me\"\n      groups   = [\"system:masters\"]\n    },\n  ]\n\n-  managed_node_groups = {\n+  eks_managed_node_groups = {\n    managed = {\n-      node_group_name = \"managed\"\n      instance_types  = [\"m5.large\"]\n\n      min_size     = 1\n      max_size     = 2\n      desired_size = 1\n\n-      k8s_labels = {\n+      labels = {\n        Which = \"managed\"\n      }\n    }\n  }\n\n  fargate_profiles = {\n    fargate = {\n-      fargate_profile_name = \"fargate\"\n-      fargate_profile_namespaces = [{\n+      selectors = [{\n        namespace = \"default\"\n\n-        k8s_labels = {\n+        labels = {\n          Which = \"fargate\"\n        }\n      }]\n-      subnet_ids = module.vpc.private_subnets\n    }\n  }\n\n  self_managed_node_groups = {\n    self_managed = {\n-      node_group_name    = \"self_managed\"\n      instance_type      = \"m5.large\"\n-      launch_template_os = \"amazonlinux2eks\"\n\n      min_size     = 1\n      max_size     = 2\n      desired_size = 1\n\n-      k8s_labels = {\n+      labels = {\n        Which = \"self-managed\"\n      }\n    }\n  }\n\n  tags = {\n    Blueprint  = local.name\n    GithubRepo = \"github.com/aws-ia/terraform-aws-eks-blueprints\"\n  }\n}\n</code></pre>"},{"location":"v4-to-v5/cluster/#state-move-commands","title":"State Move Commands","text":"<p>The following Terraform state move commands are provided to aid in migrating the control plane and data plane components.</p> <pre><code># This is not removing the configmap from the cluster -\n# it will be adopted by the new module\nterraform state rm 'module.eks.kubernetes_config_map.aws_auth[0]'\n\n# Cluster\nterraform state mv 'module.eks.module.aws_eks.aws_eks_cluster.this[0]' 'module.eks.aws_eks_cluster.this[0]'\n\n# Cluster IAM role\nterraform state mv 'module.eks.module.aws_eks.aws_iam_role.this[0]' 'module.eks.aws_iam_role.this[0]'\nterraform state mv 'module.eks.module.aws_eks.aws_iam_role_policy_attachment.this[\"arn:aws:iam::aws:policy/AmazonEKSClusterPolicy\"]' 'module.eks.aws_iam_role_policy_attachment.this[\"AmazonEKSClusterPolicy\"]'\nterraform state mv 'module.eks.module.aws_eks.aws_iam_role_policy_attachment.this[\"arn:aws:iam::aws:policy/AmazonEKSVPCResourceController\"]' 'module.eks.aws_iam_role_policy_attachment.this[\"AmazonEKSVPCResourceController\"]'\n\n# Cluster primary security group tags\n# Note: This will depend on the tags applied to the module - here we\n#       are demonstrating the two tags used in the configuration above\nterraform state mv 'module.eks.module.aws_eks.aws_ec2_tag.cluster_primary_security_group[\"Blueprint\"]' 'module.eks.aws_ec2_tag.cluster_primary_security_group[\"Blueprint\"]'\nterraform state mv 'module.eks.module.aws_eks.aws_ec2_tag.cluster_primary_security_group[\"GithubRepo\"]' 'module.eks.aws_ec2_tag.cluster_primary_security_group[\"GithubRepo\"]'\n\n# Cluster security group\nterraform state mv 'module.eks.module.aws_eks.aws_security_group.cluster[0]' 'module.eks.aws_security_group.cluster[0]'\n\n# Cluster security group rules\nterraform state mv 'module.eks.module.aws_eks.aws_security_group_rule.cluster[\"ingress_nodes_443\"]' 'module.eks.aws_security_group_rule.cluster[\"ingress_nodes_443\"]'\n\n# Node security group\nterraform state mv 'module.eks.module.aws_eks.aws_security_group.node[0]' 'module.eks.aws_security_group.node[0]'\n\n# Node security group rules\nterraform state mv 'module.eks.module.aws_eks.aws_security_group_rule.node[\"ingress_cluster_443\"]' 'module.eks.aws_security_group_rule.node[\"ingress_cluster_443\"]'\nterraform state mv 'module.eks.module.aws_eks.aws_security_group_rule.node[\"ingress_cluster_kubelet\"]' 'module.eks.aws_security_group_rule.node[\"ingress_cluster_kubelet\"]'\nterraform state mv 'module.eks.module.aws_eks.aws_security_group_rule.node[\"ingress_self_coredns_tcp\"]' 'module.eks.aws_security_group_rule.node[\"ingress_self_coredns_tcp\"]'\nterraform state mv 'module.eks.module.aws_eks.aws_security_group_rule.node[\"ingress_self_coredns_udp\"]' 'module.eks.aws_security_group_rule.node[\"ingress_self_coredns_udp\"]'\n\n# OIDC provider\nterraform state mv 'module.eks.module.aws_eks.aws_iam_openid_connect_provider.oidc_provider[0]' 'module.eks.aws_iam_openid_connect_provider.oidc_provider[0]'\n\n# Fargate profile(s)\n# Note: This demonstrates migrating one profile that is stored under the\n#       key `fargate` in the module definition. The same set of steps would\n#       need to be performed for each profile, changing only the key name\nterraform state mv 'module.eks.module.aws_eks_fargate_profiles[\"fargate\"].aws_eks_fargate_profile.eks_fargate' 'module.eks.module.fargate_profile[\"fargate\"].aws_eks_fargate_profile.this[0]'\nterraform state mv 'module.eks.module.aws_eks_fargate_profiles[\"fargate\"].aws_iam_role.fargate[0]' 'module.eks.module.fargate_profile[\"fargate\"].aws_iam_role.this[0]'\nterraform state mv 'module.eks.module.aws_eks_fargate_profiles[\"fargate\"].aws_iam_role_policy_attachment.fargate_pod_execution_role_policy[\"arn:aws:iam::aws:policy/AmazonEKSFargatePodExecutionRolePolicy\"]' 'module.eks.module.fargate_profile[\"fargate\"].aws_iam_role_policy_attachment.this[\"arn:aws:iam::aws:policy/AmazonEKSFargatePodExecutionRolePolicy\"]'\n\n# Managed nodegroup(s)\n# Note: This demonstrates migrating one nodegroup that is stored under the\n#       key `managed` in the module definition. The same set of steps would\n#       need to be performed for each nodegroup, changing only the key name\nterraform state mv 'module.eks.module.aws_eks_managed_node_groups[\"managed\"].aws_eks_node_group.managed_ng' 'module.eks.module.eks_managed_node_group[\"managed\"].aws_eks_node_group.this[0]'\nterraform state mv 'module.eks.module.aws_eks_managed_node_groups[\"managed\"].aws_iam_role.managed_ng[0]' 'module.eks.module.eks_managed_node_group[\"managed\"].aws_iam_role.this[0]'\nterraform state mv 'module.eks.module.aws_eks_managed_node_groups[\"managed\"].aws_iam_role_policy_attachment.managed_ng[\"arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly\"]' 'module.eks.module.eks_managed_node_group[\"managed\"].aws_iam_role_policy_attachment.this[\"arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly\"]'\nterraform state mv 'module.eks.module.aws_eks_managed_node_groups[\"managed\"].aws_iam_role_policy_attachment.managed_ng[\"arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy\"]' 'module.eks.module.eks_managed_node_group[\"managed\"].aws_iam_role_policy_attachment.this[\"arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy\"]'\nterraform state mv 'module.eks.module.aws_eks_managed_node_groups[\"managed\"].aws_iam_role_policy_attachment.managed_ng[\"arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy\"]' 'module.eks.module.eks_managed_node_group[\"managed\"].aws_iam_role_policy_attachment.this[\"arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy\"]'\n\n# Self-managed nodegroup(s)\n# Note: This demonstrates migrating one nodegroup that is stored under the\n#       key `self_managed` in the module definition. The same set of steps would\n#       need to be performed for each nodegroup, changing only the key name\nterraform state mv 'module.eks.module.aws_eks_self_managed_node_groups[\"self_managed\"].aws_autoscaling_group.self_managed_ng' 'module.eks.module.self_managed_node_group[\"self_managed\"].aws_autoscaling_group.this[0]'\nterraform state mv 'module.eks.module.aws_eks_self_managed_node_groups[\"self_managed\"].aws_iam_instance_profile.self_managed_ng[0]' 'module.eks.module.self_managed_node_group[\"self_managed\"].aws_iam_instance_profile.this[0]'\nterraform state mv 'module.eks.module.aws_eks_self_managed_node_groups[\"self_managed\"].aws_iam_role.self_managed_ng[0]' 'module.eks.module.self_managed_node_group[\"self_managed\"].aws_iam_role.this[0]'\nterraform state mv 'module.eks.module.aws_eks_self_managed_node_groups[\"self_managed\"].aws_iam_role_policy_attachment.self_managed_ng[\"arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly\"]' 'module.eks.module.self_managed_node_group[\"self_managed\"].aws_iam_role_policy_attachment.this[\"arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly\"]'\nterraform state mv 'module.eks.module.aws_eks_self_managed_node_groups[\"self_managed\"].aws_iam_role_policy_attachment.self_managed_ng[\"arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy\"]' 'module.eks.module.self_managed_node_group[\"self_managed\"].aws_iam_role_policy_attachment.this[\"arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy\"]'\nterraform state mv 'module.eks.module.aws_eks_self_managed_node_groups[\"self_managed\"].aws_iam_role_policy_attachment.self_managed_ng[\"arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy\"]' 'module.eks.module.self_managed_node_group[\"self_managed\"].aws_iam_role_policy_attachment.this[\"arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy\"]'\nterraform state mv 'module.eks.module.aws_eks_self_managed_node_groups[\"self_managed\"].module.launch_template_self_managed_ng.aws_launch_template.this[\"self-managed-node-group\"]' 'module.eks.module.self_managed_node_group[\"self_managed\"].aws_launch_template.this[0]'\n\n# Secrets KMS key\nterraform state mv ' module.eks.module.kms[0].aws_kms_key.this' 'module.eks.module.kms.aws_kms_key.this[0]'\nterraform state mv 'module.eks.module.kms[0].aws_kms_alias.this' 'module.eks.module.kms.aws_kms_alias.this[\"migration\"]'\n\n# Cloudwatch Log Group\nterraform import 'module.eks.aws_cloudwatch_log_group.this[0]' /aws/eks/migration/cluster\n</code></pre>"},{"location":"v4-to-v5/cluster/#removed-resources","title":"Removed Resources","text":"<p>The following resources will be destroyed when migrating from EKS Blueprints v4.32.1 cluster to the v19.x EKS cluster:</p> <pre><code>module.eks.module.aws_eks_managed_node_groups[\"managed\"].aws_iam_instance_profile.managed_ng[0]\n</code></pre> <ul> <li>It is not directly used and was intended to be used by Karpenter. The https://github.com/aws-ia/terraform-aws-eks-blueprints-addons module provides its own resource for creating an IAM instance profile for Karpenter</li> </ul> <pre><code>module.eks.module.aws_eks_managed_node_groups[\"managed\"].aws_iam_role_policy_attachment.managed_ng[\"arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore\"]\n</code></pre> <ul> <li>IAM policy is not required by EKS - users can re-add this policy at their discretion</li> </ul> <pre><code>module.eks.module.aws_eks_fargate_profiles[\"fargate\"].aws_iam_policy.cwlogs[0]\nmodule.eks.module.aws_eks_fargate_profiles[\"fargate\"].aws_iam_role_policy_attachment.cwlogs[0]\n</code></pre> <ul> <li>Policy is not required by EKS</li> </ul> <pre><code>module.eks.module.aws_eks_self_managed_node_groups[\"self_managed\"].aws_iam_role_policy_attachment.self_managed_ng[\"arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore\"]\n</code></pre> <ul> <li>IAM policy is not required by EKS - users can re-add this policy at their discretion</li> </ul>"},{"location":"v4-to-v5/motivation/","title":"Direction for v5 of Terraform EKS Blueprints","text":""},{"location":"v4-to-v5/motivation/#what-has-worked","title":"What Has Worked","text":"<ul> <li> <p>EKS Blueprints was started to make it easier for customers to adopt Amazon Elastic Kubernetes Service (EKS) in a shorter period of time. The project has been quite successful in this regard - hearing from customers stating that EKS Blueprints has helped them get from zero to one or more clusters running with applications in less than 1-2 weeks.</p> </li> <li> <p>EKS Blueprints has also been successful in providing working examples to users that demonstrate common architectural patterns and workload solutions. Some popular examples include:</p> </li> <li>Spark on EKS</li> <li>Karpenter on EKS Fargate</li> <li>Transparent encryption with Wireguard and Cilium</li> <li>Fully serverless cluster with EKS Fargate</li> </ul>"},{"location":"v4-to-v5/motivation/#what-has-not","title":"What Has Not","text":"<ul> <li> <p>Scaling and managing addons that are created through EKS Blueprints. With almost 1,200 projects on the CNCF roadmap, the number of various ways and methods that a project allows for deploying onto a cluster (i.e. - Datadog offers 5 different Helm charts for its service, Prometheus hosts over 30 Helm charts for its services), as well as the number of different tools used to provision addons (i.e. - Terraform, ArgoCD, FluxCD, etc.), supporting both the number of addons and their different forms has been extremely challenging for the team. In addition to managing just the sheer number of addons, supporting the different configurations that users wish to have exposed in conjunction with testing and validating those various configurations is only compounded by the number of addons and their methods of creation.</p> </li> <li> <p>Managing resources provisioned on the cluster using Terraform. Terraform is a fantastic tool for provisioning infrastructure and it is the tool of choice for many customers when it comes to creating resources in AWS. However, there are a number of downsides with Terraform when it comes to provisioning resources on a Kubernetes cluster. These include:</p> </li> <li> <p>Ordering of dependencies when relationships live outside of Terraform's HCL syntax. Terraform wants to evaluate the current state of what it controls and be able to plan a series of actions to align the current state with the desired state in one action. It does this once for each <code>terraform plan</code> or <code>terraform apply</code>, and if any issues are encountered, it simply fails and halts execution. When Terraform cannot infer the ordering of dependencies across resources (i.e. - through passing outputs of parent resources to arguments of child resources using the Terraform <code>&lt;resource&gt;.&lt;name&gt;.&lt;attribute&gt;</code> syntax), it will view this as no relationship between the resources and attempt to execute their provisioning in parallel and asynchronously. Any resources that are left waiting for a dependency will eventually timeout and fail, causing Terraform itself to timeout and fail the apply. This is where the reconciliation loop of a Kubernetes controller or operator on the cluster is better suited - continuously trying to reconcile the state over and over again as dependencies are eventually resolved. (To be clear - the issue of dependency ordering still exists, but the controller/operator will keep retrying and on each retry, some resources will succeed which will move the execution along with each cycle until everything is fully deployed. Terraform could do this if it kept re-trying, but it does not do this today)</p> </li> <li> <p>Publicly exposing access to the EKS endpoints in order to provision resources defined outside of the VPC onto the cluster. When using Terraform, the resource provisioning operation is a \"push\" model where Terraform will send requests to the EKS API Server to create resources. Coupled with the fact that the Terraform operation typically resides outside of the VPC where the cluster is running, this results in users enabling public access to the EKS endpoints to provision resources. However, the more widely accepted approach by the Kubernetes community has been the adoption of GitOps which uses a \"pull\" based model, where an operator or controller running on the cluster will pull the resource definitions from a Git repository and reconcile state from within the cluster itself. This approach is more secure as it does not require public access to the EKS endpoints and instead relies on the cluster's internal network to communicate with the EKS API Server.</p> </li> <li> <p>The nesting of multiple sub-modules in conjunction with the necessity to even require a module to be able to support an addon. When we compare and contrast the Terraform approach to addons versus the GitOps approach, the Terraform approach has a glaring disadvantage - the need to create a module that wraps the addon's Helm chart in order to provision the addon via Terraform. As opposed to the GitOps approach, where users simply consume the charts from where they are stored as needed. This creates a bottleneck on the team to review, test, and validate each new addon as well as the overhead then added for maintaining and updating those addons going forward. This also opens up more areas where breaking changes are encountered which is compounded by the fact that Terraform addons are grouped under an \"umbrella\" module which obfuscates versioning.</p> </li> <li> <p>Being able to support a combination of various tools, modules, frameworks, etc., to meet the needs of customers. The <code>terraform-aws-eks</code> was created long before EKS Blueprints, and many customers had already adopted this module for creating their clusters. In addition, Amazon has since adopted the <code>eksctl</code> as the official CLI for Amazon EKS. When EKS Blueprints was first announced, many customers raised questions asking if they needed to abandon their current clusters created through those other tools in order to adopt EKS Blueprints. The answer is no - users can and should be able to use their existing clusters while EKS Blueprints can help augment that process through its supporting modules (addons, teams, etc.). This left the team with the question - why create a Terraform module for creating an EKS cluster when the <code>terraform-aws-eks</code> already exists and the EKS Blueprints implementation already uses that module for creating the control plane and security groups?</p> </li> </ul>"},{"location":"v4-to-v5/motivation/#what-is-changing","title":"What Is Changing","text":"<p>The direction for EKS Blueprints in v5 will shift from providing an all-encompassing, monolithic \"framework\" and instead focus more on how users can organize a set of modular components to create the desired solution on Amazon EKS. This will allow customers to use the components of their choosing in a way that is more familiar to them and their organization instead of having to adopt and conform to a framework.</p> <p>With this shift in direction, the cluster definition will be removed from the project and instead examples will reference the <code>terraform-aws-eks</code> module for cluster creation. The remaining modules will be moved out to their own respective repositories as standalone projects. This leaves the EKS Blueprint project as the canonical place where users can receive guidance on how to configure their clusters to meet a desired architecture, how best to setup their clusters following well-architected practices, as well as references on the various ways that different workloads can be deployed on Amazon EKS.</p>"},{"location":"v4-to-v5/motivation/#notable-changes","title":"Notable Changes","text":"<ol> <li>EKS Blueprints will remove its Amazon EKS cluster Terraform module components (control plane, EKS managed node group, self-managed node group, and Fargate profile modules) from the project. In its place, users are encouraged to utilize the <code>terraform-aws-eks</code> module which meets or exceeds nearly all of the functionality of the EKS Blueprints v4.x cluster module. This includes the Terraform code contained at the root of the project as well as the <code>aws-eks-fargate-profiles</code>, <code>aws-eks-managed-node-groups</code>, <code>aws-eks-self-managed-node-groups</code>, and <code>launch-templates</code> modules which will all be removed from the project.</li> <li>The <code>aws-kms</code> module will be removed entirely. This was consumed in the root project module for cluster secret encryption. In its place, users can utilize the KMS key creation functionality of the <code>terraform-aws-eks</code> module or the <code>terraform-aws-kms</code> module if they wish to control the key separately from the cluster itself.</li> <li>The <code>emr-on-eks</code> module will be removed entirely; its replacement can be found in the new external module <code>terraform-aws-emr</code>.</li> <li>The <code>irsa</code> and <code>helm-addon</code> modules will be removed entirely; we have released a new external module <code>terraform-aws-eks-blueprints-addon</code> that is available on the Terraform registry that replicates/replaces the functionality of these two modules. This will now allow users, as well as partners, to create their own addons that are not natively supported by EKS Blueprints more easily and following the same process as EKS Blueprints.</li> <li>The <code>aws-eks-teams</code> module will be removed entirely; its replacement will be the new external module <code>terraform-aws-eks-blueprints-teams</code> that incorporates the changes customers have been asking for in https://github.com/aws-ia/terraform-aws-eks-blueprints/issues/842</li> <li>The integration between Terraform and ArgoCD has been removed in the initial release of v5. The team is currently investigating better patterns and solutions in conjunction with the ArgoCD and FluxCD teams that will provide a better, more integrated experience when using a GitOps based approach for cluster management. This will be released in a future version of EKS Blueprints v5 and is tracked here</li> </ol>"},{"location":"v4-to-v5/motivation/#resulting-project-structure","title":"Resulting Project Structure","text":"<p>Previously under the v4.x structure, the EKS Blueprint project was comprised of various repositories across multiple AWS organizations that looked roughly like the following:</p>"},{"location":"v4-to-v5/motivation/#v4x-structure","title":"v4.x Structure","text":"<pre><code>\u251c\u2500\u2500 aws-ia/\n|   \u251c\u2500\u2500 terraform-aws-eks-ack-addons/\n|   \u2514\u2500\u2500 terraform-aws-eks-blueprints/\n|       \u251c\u2500\u2500 aws-auth-configmap.tf\n|       \u251c\u2500\u2500 data.tf\n|       \u251c\u2500\u2500 eks-worker.tf\n|       \u251c\u2500\u2500 locals.tf\n|       \u251c\u2500\u2500 main.tf\n|       \u251c\u2500\u2500 outputs.tf\n|       \u251c\u2500\u2500 variables.tf\n|       \u251c\u2500\u2500 versions.tf\n|       \u251c\u2500\u2500 examples/\n|       \u2514\u2500\u2500 modules\n|           \u251c\u2500\u2500 aws-eks-fargate-profiles/\n|           \u251c\u2500\u2500 aws-eks-managed-node-groups/\n|           \u251c\u2500\u2500 aws-eks-self-managed-node-groups/\n|           \u251c\u2500\u2500 aws-eks-teams/\n|           \u251c\u2500\u2500 aws-kms/\n|           \u251c\u2500\u2500 emr-on-eks/\n|           \u251c\u2500\u2500 irsa/\n|           \u251c\u2500\u2500 kubernetes-addons/\n|           \u2514\u2500\u2500 launch-templates/\n\u251c\u2500\u2500 awslabs/\n|   \u251c\u2500\u2500 crossplane-on-eks/\n|   \u2514\u2500\u2500 data-on-eks/\n\u2514\u2500\u2500 aws-samples/\n    \u251c\u2500\u2500 eks-blueprints-add-ons/   # Previously shared with the CDK based EKS Blueprints project\n    \u2514\u2500\u2500 eks-blueprints-workloads/ # Previously shared with the CDK based EKS Blueprints project\n</code></pre> <p>Under th new v5.x structure, the Terraform based EKS Blueprints project will be comprised of the following repositories:</p>"},{"location":"v4-to-v5/motivation/#v5x-structure","title":"v5.x Structure","text":"<pre><code>\u251c\u2500\u2500 aws-ia/\n|   \u251c\u2500\u2500 terraform-aws-eks-ack-addons/\n|   \u251c\u2500\u2500 terraform-aws-eks-blueprints/       # Will contain only example/blueprint implementations; no modules\n|   \u251c\u2500\u2500 terraform-aws-eks-blueprints-addon  # Module for creating Terraform based addon (IRSA + Helm chart)\n|   \u251c\u2500\u2500 terraform-aws-eks-blueprints-addons # Will contain a select set of addons supported by the EKS Blueprints\n|   \u2514\u2500\u2500 terraform-aws-eks-blueprints-teams  # Was previously `aws-eks-teams/` EKS Blueprint sub-module; updated based on customer feedback\n\u2514\u2500\u2500 awslabs/\n    \u251c\u2500\u2500 crossplane-on-eks/\n    \u2514\u2500\u2500 data-on-eks/        # Data related patterns that used to be located in `terraform-aws-eks-blueprints/` are now located here\n</code></pre>"},{"location":"v4-to-v5/motivation/#what-can-users-expect","title":"What Can Users Expect","text":"<p>With these changes, the team intends to provide a better experience for users of the Terraform EKS Blueprints project as well as new and improved reference architectures. Following the v5 changes, the team intends to:</p> <ol> <li>Improved quality of the examples provided - more information on the intent of the example, why it might be useful for users, what scenarios is the pattern applicable, etc. Where applicable, architectural diagrams and supporting material will be provided to highlight the intent of the example and how its constructed.</li> <li>A more clear distinction between a blueprint and a usage reference. For example - the Karpenter on EKS Fargate blueprint should demonstrate all of the various aspects that users should be aware of and consider in order to take full advantage of this pattern (recommended practices, observability, logging, monitoring, security, day 2 operations, etc.); this is what makes it a blueprint. In contrast, a usage reference would be an example that shows how users can pass configuration values to the Karpenter provisioner. This example is less focused on the holistic architecture and more focused on how one might configure Karpenter using the implementation. The EKS Blueprints repository will focus mostly on holistic architecture and patterns, and any usage references should be saved for the repository that contains that implementation definition (i.e. - the <code>terraform-aws-eks-blueprints-addons</code> repository where the addon implementation is defined).</li> <li>Faster, and more responsive feedback. The first part of this is going to be improved documentation on how to contribute which should help clarify whether a contribution is worthy and willing to be accepted by the team before any effort is spent by the contributor. However, the goal of v5 is to focus more on the value added benefits that EKS Blueprints was created to provide as opposed to simply mass producing Helm chart wrappers (addons) and trying to keep up with that operationally intensive process.</li> <li>Lastly, more examples and blueprints that demonstrate various architectures and workloads that run on top of Amazon EKS as well as integrations into other AWS services.</li> </ol>"},{"location":"v4-to-v5/teams/","title":"Migrate to EKS Blueprints Teams Module","text":"<p>Please consult the docs/v4-to-v5/example directory for reference configurations. If you find a bug, please open an issue with supporting configuration to reproduce.</p>"},{"location":"v4-to-v5/teams/#this-guide-is-under-active-development","title":"\u26a0\ufe0f This guide is under active development.","text":""},{"location":"v4-to-v5/teams/#list-of-backwards-incompatible-changes","title":"List of backwards incompatible changes","text":"<p>-</p>"},{"location":"v4-to-v5/teams/#additional-changes","title":"Additional changes","text":""},{"location":"v4-to-v5/teams/#added","title":"Added","text":"<p>-</p>"},{"location":"v4-to-v5/teams/#modified","title":"Modified","text":"<p>-</p>"},{"location":"v4-to-v5/teams/#removed","title":"Removed","text":"<p>-</p>"},{"location":"v4-to-v5/teams/#variable-and-output-changes","title":"Variable and output changes","text":"<ol> <li> <p>Removed variables:</p> <p>-</p> </li> <li> <p>Renamed variables:</p> <p>-</p> </li> <li> <p>Added variables:</p> <p>-</p> </li> <li> <p>Removed outputs:</p> <p>-</p> </li> <li> <p>Renamed outputs:</p> <p>-</p> </li> <li> <p>Added outputs:</p> <p>-</p> </li> </ol>"},{"location":"v4-to-v5/teams/#upgrade-migrations","title":"Upgrade Migrations","text":""},{"location":"v4-to-v5/teams/#before-v4x-example","title":"Before - v4.x Example","text":"<pre><code>module \"eks_blueprints\" {\n  source = \"github.com/aws-ia/terraform-aws-eks-blueprints?ref=v4.32.1\"\n\n  # TODO\n\n}\n</code></pre>"},{"location":"v4-to-v5/teams/#after-v5x-example","title":"After - v5.x Example","text":"<pre><code>module \"eks_blueprints_teams\" {\n  source  = \"aws-ia/eks-blueprints-teams/aws\"\n  version = \"~&gt; 1.0\"\n\n  # TODO\n\n}\n</code></pre>"},{"location":"v4-to-v5/teams/#diff-of-before-vs-after","title":"Diff of Before vs After","text":"<pre><code>module \"eks_blueprints_teams\" {\n-  source = \"github.com/aws-ia/terraform-aws-eks-blueprints?ref=v4.32.1\"\n+  source  = \"aws-ia/eks-blueprints-teams/aws\"\n+  version = \"~&gt; 1.0\"\n\n  # TODO\n}\n</code></pre>"},{"location":"v4-to-v5/teams/#state-move-commands","title":"State Move Commands","text":"<p>In conjunction with the changes above, users can elect to move their external capacity provider(s) under this module using the following move command. Command is shown using the values from the example shown above, please update to suit your configuration names:</p> <pre><code>terraform state mv 'xxx' 'yyy'\n</code></pre>"},{"location":"v4-to-v5/example/","title":"Migration - v4 to v5","text":""}]}